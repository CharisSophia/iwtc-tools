{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16ccca7-3bd6-42fb-9345-a58c07280b0f",
   "metadata": {},
   "source": [
    "# IWTC Graph Indexing (v0)\n",
    "\n",
    "Builds graph artifacts from canonical index tables by translating index grammar into graph grammar.\n",
    "\n",
    "Index tables express statements in tabular form.\n",
    "Graph artifacts express the same statements as triples (subject–predicate–object).\n",
    "\n",
    "Outputs:\n",
    "- graph_nodes_v0.csv\n",
    "- graph_edges_v0.csv\n",
    "\n",
    "Artifacts are written to `working_drafts` for review before promotion to canonical indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df070d7-9e84-4ced-909a-afab540fe5b6",
   "metadata": {},
   "source": [
    "# Pre-Build: Validate environment and load canonical indexes\n",
    "\n",
    "Validates repository paths and loads canonical index tables into DataFrames.\n",
    "\n",
    "No graph artifacts are built here.\n",
    "\n",
    "You may collapse this section after it runs successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f637a-2282-4b3c-a294-da65fae45712",
   "metadata": {},
   "source": [
    "## Phase P0: Parameters\n",
    "\n",
    "Define which world repository this notebook operates on and which index version it expects.\n",
    "\n",
    "This notebook builds graph artifacts from existing canonical indexes.\n",
    "It does not modify source material.\n",
    "\n",
    "**IMPORTANT:** This notebook assumes index artifacts already exist and will fail\n",
    "if required CSV files are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862540a0-d2ef-43c7-9a85-6b57f2af2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: Parameters\n",
    "LAST_PHASE_RUN = \"P0\"\n",
    "\n",
    "# Absolute path to the world_repository.yml descriptor.\n",
    "WORLD_REPOSITORY_DESCRIPTOR = (\n",
    "    \"/Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/descriptors/world_repository.yml\"\n",
    ")\n",
    "\n",
    "# Index version to load (must match previously generated artifacts)\n",
    "INDEX_VERSION = \"V0\"\n",
    "\n",
    "# Internal run metadata (do not edit)\n",
    "from datetime import datetime\n",
    "print(f\"Notebook run initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "del datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b7688-b61f-4b66-8513-d60de5b83094",
   "metadata": {},
   "source": [
    "## Phase P1: Load and validate world descriptor\n",
    "\n",
    "This phase verifies that the world repository descriptor is readable and structurally valid.\n",
    "\n",
    "- Load the descriptor file\n",
    "- Resolve required paths\n",
    "- Confirm referenced directories and files exist\n",
    "\n",
    "If validation fails, the notebook stops with actionable error messages.\n",
    "\n",
    "No data is read, written, or scanned until this succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a6182-f431-4069-ac94-e8b9efe73fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase P1: Load and validate world repository descriptor (Graph Indexing v0)\n",
    "LAST_PHASE_RUN = \"P1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# --- Load descriptor file ---\n",
    "descriptor_path = Path(WORLD_REPOSITORY_DESCRIPTOR)\n",
    "\n",
    "if not descriptor_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"World repository descriptor file was not found.\\n\"\n",
    "        f\"Path provided:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Confirm the file exists at this location or fix WORLD_REPOSITORY_DESCRIPTOR in Phase 0\\n\"\n",
    "        \"- If you just edited Phase 0, rerun Phase 0 and then rerun this cell\\n\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    with descriptor_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        world_repo = yaml.safe_load(f)\n",
    "except Exception:\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor could not be read.\\n\"\n",
    "        \"This usually indicates a YAML formatting problem.\\n\\n\"\n",
    "        f\"File:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Paste the contents into https://www.yamllint.com/\\n\"\n",
    "        \"- Fix any reported issues, save the file, and rerun this cell\"\n",
    "    )\n",
    "\n",
    "if not isinstance(world_repo, dict):\n",
    "    raise ValueError(\n",
    "        \"World repository descriptor structure is not usable.\\n\"\n",
    "        \"The file must be a YAML mapping (top-level `name: value` entries).\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"World repository descriptor loaded successfully: {descriptor_path.name}\")\n",
    "\n",
    "# --- Extract required entries ---\n",
    "WORLD_ROOT_RAW = world_repo.get(\"world_root\")\n",
    "\n",
    "drafts_block = world_repo.get(\"working_drafts\")\n",
    "DRAFTS_RAW = drafts_block.get(\"path\") if isinstance(drafts_block, dict) else None\n",
    "\n",
    "indexes_block = world_repo.get(\"indexes\")\n",
    "INDEXES_RAW = indexes_block.get(\"path\") if isinstance(indexes_block, dict) else None\n",
    "\n",
    "vocab = world_repo.get(\"vocabulary\") or {}\n",
    "ENTITIES_RAW = vocab.get(\"entities\")\n",
    "ALIASES_RAW = vocab.get(\"aliases\")\n",
    "AUTHORS_RAW = vocab.get(\"author_aliases\")\n",
    "PC_MAP_RAW = vocab.get(\"player_character_map\")\n",
    "\n",
    "if not WORLD_ROOT_RAW:\n",
    "    errors.append(\"Missing required entry: world_root\")\n",
    "if not DRAFTS_RAW:\n",
    "    errors.append(\"Missing required entry: working_drafts.path\")\n",
    "if not INDEXES_RAW:\n",
    "    errors.append(\"Missing required entry: indexes.path\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"World repository descriptor is missing required entries:\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Edit your world_repository.yml and add/fix the missing entries\\n\"\n",
    "          \"- Save the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Published outputs (initialize up front for later phases)\n",
    "# ------------------------------------------------------------------\n",
    "WORLD_ROOT = None\n",
    "\n",
    "WORKING_DRAFTS_PATH = None\n",
    "WORKING_DRAFTS_RELPATH = None\n",
    "\n",
    "INDEXES_PATH = None\n",
    "INDEXES_RELPATH = None\n",
    "\n",
    "VOCAB_ENTITIES_PATH = None\n",
    "VOCAB_ENTITIES_RELPATH = None\n",
    "VOCAB_ALIASES_PATH = None\n",
    "VOCAB_ALIASES_RELPATH = None\n",
    "VOCAB_AUTHORS_PATH = None\n",
    "VOCAB_AUTHORS_RELPATH = None\n",
    "VOCAB_PC_MAP_PATH = None\n",
    "VOCAB_PC_MAP_RELPATH = None\n",
    "\n",
    "# --- Validate and resolve world_root ---\n",
    "WORLD_ROOT = Path(WORLD_ROOT_RAW)\n",
    "\n",
    "if str(WORLD_ROOT).startswith(\"~\"):\n",
    "    errors.append(\"world_root: '~' is not allowed. Use a full absolute path.\")\n",
    "elif not WORLD_ROOT.is_absolute():\n",
    "    errors.append(\"world_root must be an absolute path (starts with / on macOS/Linux, or C:\\\\ on Windows).\")\n",
    "elif not WORLD_ROOT.is_dir():\n",
    "    errors.append(f\"world_root must be an existing directory: {WORLD_ROOT}\")\n",
    "else:\n",
    "    WORLD_ROOT = WORLD_ROOT.resolve()\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "def _resolve_under_world_root(raw_path: str, label: str):\n",
    "    if raw_path is None or str(raw_path).strip() == \"\":\n",
    "        return None, None\n",
    "\n",
    "    p = Path(str(raw_path))\n",
    "\n",
    "    if str(p).startswith(\"~\"):\n",
    "        errors.append(f\"{label}: '~' is not allowed: {raw_path}\")\n",
    "        return None, None\n",
    "\n",
    "    if not p.is_absolute():\n",
    "        p = WORLD_ROOT / p\n",
    "    p = p.resolve()\n",
    "\n",
    "    try:\n",
    "        rel = str(p.relative_to(WORLD_ROOT))\n",
    "    except Exception:\n",
    "        rel = str(p)\n",
    "\n",
    "    return p, rel\n",
    "\n",
    "# --- Resolve and validate working_drafts path (required, directory, writable) ---\n",
    "WORKING_DRAFTS_PATH, WORKING_DRAFTS_RELPATH = _resolve_under_world_root(DRAFTS_RAW, \"working_drafts.path\")\n",
    "\n",
    "if WORKING_DRAFTS_PATH is None:\n",
    "    errors.append(\"working_drafts.path: missing or invalid.\")\n",
    "else:\n",
    "    if not WORKING_DRAFTS_PATH.exists():\n",
    "        errors.append(f\"working_drafts.path: path does not exist: {WORKING_DRAFTS_PATH}\")\n",
    "    elif not WORKING_DRAFTS_PATH.is_dir():\n",
    "        errors.append(f\"working_drafts.path: must be a directory: {WORKING_DRAFTS_PATH}\")\n",
    "    else:\n",
    "        # write probe (same behavior as raw source indexing)\n",
    "        probe = WORKING_DRAFTS_PATH / \".iwtc_tools_write_probe.tmp\"\n",
    "        try:\n",
    "            probe.write_text(\"test\", encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"working_drafts.path: not writable: {WORKING_DRAFTS_PATH} ({type(e).__name__})\")\n",
    "        finally:\n",
    "            try:\n",
    "                if probe.exists():\n",
    "                    probe.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# --- Resolve and validate indexes path (required, directory) ---\n",
    "INDEXES_PATH, INDEXES_RELPATH = _resolve_under_world_root(INDEXES_RAW, \"indexes.path\")\n",
    "\n",
    "if INDEXES_PATH is None:\n",
    "    errors.append(\"indexes.path: missing or invalid.\")\n",
    "else:\n",
    "    if not INDEXES_PATH.exists():\n",
    "        errors.append(f\"indexes.path: path does not exist: {INDEXES_PATH}\")\n",
    "    elif not INDEXES_PATH.is_dir():\n",
    "        errors.append(f\"indexes.path: must be a directory: {INDEXES_PATH}\")\n",
    "\n",
    "# --- Resolve vocabulary paths (optional; warn if missing) ---\n",
    "vocab_entries = [\n",
    "    (\"entities\", \"vocab.entities\", ENTITIES_RAW),\n",
    "    (\"aliases\", \"vocab.aliases\", ALIASES_RAW),\n",
    "    (\"author_aliases\", \"vocab.author_aliases\", AUTHORS_RAW),\n",
    "    (\"player_character_map\", \"vocab.player_character_map\", PC_MAP_RAW),\n",
    "]\n",
    "\n",
    "for key, label, raw in vocab_entries:\n",
    "    if not raw:\n",
    "        continue\n",
    "\n",
    "    p, rel = _resolve_under_world_root(raw, label)\n",
    "    if p is None:\n",
    "        continue\n",
    "\n",
    "    if p.exists() and p.is_dir():\n",
    "        warnings.append(f\"{label}: {p} must be a file (got directory). Ignoring.\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"{label}: file does not exist: {p} (name resolution may be limited).\")\n",
    "\n",
    "    if key == \"entities\":\n",
    "        VOCAB_ENTITIES_PATH, VOCAB_ENTITIES_RELPATH = p, rel\n",
    "    elif key == \"aliases\":\n",
    "        VOCAB_ALIASES_PATH, VOCAB_ALIASES_RELPATH = p, rel\n",
    "    elif key == \"author_aliases\":\n",
    "        VOCAB_AUTHORS_PATH, VOCAB_AUTHORS_RELPATH = p, rel\n",
    "    elif key == \"player_character_map\":\n",
    "        VOCAB_PC_MAP_PATH, VOCAB_PC_MAP_RELPATH = p, rel\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "print(\"Descriptor paths are usable for this notebook.\")\n",
    "print(f'world_root: {WORLD_ROOT}')\n",
    "print(f'working_drafts: {WORKING_DRAFTS_RELPATH}')\n",
    "print(f'indexes: {INDEXES_RELPATH}')\n",
    "\n",
    "print(f\"vocab.entities: {VOCAB_ENTITIES_RELPATH} (exists={VOCAB_ENTITIES_PATH.exists() if VOCAB_ENTITIES_PATH else False})\")\n",
    "print(f\"vocab.aliases: {VOCAB_ALIASES_RELPATH} (exists={VOCAB_ALIASES_PATH.exists() if VOCAB_ALIASES_PATH else False})\")\n",
    "print(f\"vocab.author_aliases: {VOCAB_AUTHORS_RELPATH} (exists={VOCAB_AUTHORS_PATH.exists() if VOCAB_AUTHORS_PATH else False})\")\n",
    "print(f\"vocab.player_character_map: {VOCAB_PC_MAP_RELPATH} (exists={VOCAB_PC_MAP_PATH.exists() if VOCAB_PC_MAP_PATH else False})\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del yaml, Path\n",
    "del descriptor_path, world_repo, drafts_block, indexes_block, vocab\n",
    "del WORLD_REPOSITORY_DESCRIPTOR\n",
    "del WORLD_ROOT_RAW, DRAFTS_RAW, INDEXES_RAW, ENTITIES_RAW, ALIASES_RAW, AUTHORS_RAW, PC_MAP_RAW\n",
    "del vocab_entries, key, label, raw, p, rel, warnings, errors, probe, f\n",
    "del _resolve_under_world_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df1569-6575-4e04-922c-52f6b5f79103",
   "metadata": {},
   "source": [
    "## Phase P2: Load index artifacts\n",
    "\n",
    "This phase verifies that the required index CSV artifacts exist and can be loaded.\n",
    "\n",
    "- Resolve expected filenames from `INDEX_VERSION`\n",
    "- Confirm they exist under `indexes.path`\n",
    "- Load them into DataFrames\n",
    "- Verify required columns are present\n",
    "\n",
    "If any artifact is missing or malformed, the notebook stops with instructions to regenerate them.\n",
    "\n",
    "No files are modified in this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbff85b-df65-485e-9ade-fd0d9bbf4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Load index artifacts (v0)\n",
    "LAST_PHASE_RUN = \"P2\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "\n",
    "# Normalize INDEX_VERSION into the on-disk suffix (your files use lowercase v0)\n",
    "# Accepts \"V0\", \"v0\", \"0\" (if you ever use that), but publishes \"v0\"\n",
    "INDEX_VERSION_SUFFIX = f\"v{str(INDEX_VERSION).lower().lstrip('v')}\"\n",
    "\n",
    "# Required artifact filenames (fixed contract for this notebook)\n",
    "required = {\n",
    "    \"entity_to_chunks\": f\"index_entity_to_chunks_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"chunk_to_entities\": f\"index_chunk_to_entities_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"player_to_chunks\": f\"index_player_to_chunks_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"source_files\": f\"index_source_files_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "}\n",
    "\n",
    "# Resolve paths and validate existence\n",
    "INDEX_FILES = {}\n",
    "for key, fname in required.items():\n",
    "    p = (INDEXES_PATH / fname).resolve()\n",
    "    INDEX_FILES[key] = p\n",
    "    if not p.exists():\n",
    "        errors.append(f\"Missing required index artifact: {fname}\\n  Expected at: {p}\")\n",
    "\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase 2 cannot proceed because required index artifacts are missing.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Rerun IWTC_Raw_Source_Indexing.ipynb to generate the v0 artifacts\\n\"\n",
    "          \"- Ensure the resulting index_*.csv files are placed under your indexes.path directory\\n\"\n",
    "          f\"- indexes.path resolved to:\\n  {INDEXES_PATH}\\n\"\n",
    "          \"- Then rerun Phase 2\"\n",
    "    )\n",
    "\n",
    "# Load CSVs (raw)\n",
    "DF_ENTITY_TO_CHUNKS = pd.read_csv(INDEX_FILES[\"entity_to_chunks\"])\n",
    "DF_CHUNK_TO_ENTITIES = pd.read_csv(INDEX_FILES[\"chunk_to_entities\"])\n",
    "DF_PLAYER_TO_CHUNKS = pd.read_csv(INDEX_FILES[\"player_to_chunks\"])\n",
    "DF_SOURCE_FILES = pd.read_csv(INDEX_FILES[\"source_files\"])\n",
    "\n",
    "# Validate required columns (presence only)\n",
    "expected_cols = {\n",
    "    \"DF_ENTITY_TO_CHUNKS\": {\"entity_id\", \"canonical\", \"chunk_ids\", \"chunk_count\", \"file_relpaths\", \"file_count\"},\n",
    "    \"DF_CHUNK_TO_ENTITIES\": {\n",
    "        \"chunk_id\", \"source_id\", \"source_type\", \"relpath\",\n",
    "        \"chunk_start_line\", \"chunk_end_line\",\n",
    "        \"entity_ids\", \"canonicals\", \"entity_count\",\n",
    "        \"matched_vocabs\", \"match_kinds\",\n",
    "    },\n",
    "    \"DF_PLAYER_TO_CHUNKS\": {\"player_entity_id\", \"canonical\", \"chunk_ids\", \"chunk_count\", \"file_relpaths\", \"file_count\"},\n",
    "    \"DF_SOURCE_FILES\": {\"source_id\", \"relpath\", \"source_type\"},\n",
    "}\n",
    "\n",
    "for df_name, cols in expected_cols.items():\n",
    "    df = globals()[df_name]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"{df_name}: missing expected columns: {missing}\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"One or more index artifacts were loaded but do not match expected v0 columns.\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Confirm you are using the v0 CSVs produced by IWTC_Raw_Source_Indexing.ipynb\\n\"\n",
    "          \"- Do not edit the CSVs manually\\n\"\n",
    "          \"- If you changed the producer notebook, re-run it to regenerate indexes and retry\"\n",
    "    )\n",
    "\n",
    "# Summary prints\n",
    "print(\"Phase 2 OK: index artifacts loaded.\")\n",
    "print(f\"indexes.path: {INDEXES_PATH}\")\n",
    "print(f\"index version: {INDEX_VERSION_SUFFIX}\")\n",
    "\n",
    "print(\"\\nLoaded tables:\")\n",
    "print(f\"- DF_ENTITY_TO_CHUNKS:   {len(DF_ENTITY_TO_CHUNKS):>8} rows, {len(DF_ENTITY_TO_CHUNKS.columns):>3} cols\")\n",
    "print(f\"- DF_CHUNK_TO_ENTITIES:  {len(DF_CHUNK_TO_ENTITIES):>8} rows, {len(DF_CHUNK_TO_ENTITIES.columns):>3} cols\")\n",
    "print(f\"- DF_PLAYER_TO_CHUNKS:   {len(DF_PLAYER_TO_CHUNKS):>8} rows, {len(DF_PLAYER_TO_CHUNKS.columns):>3} cols\")\n",
    "print(f\"- DF_SOURCE_FILES:       {len(DF_SOURCE_FILES):>8} rows, {len(DF_SOURCE_FILES.columns):>3} cols\")\n",
    "\n",
    "# Optional: quick column display (helps debugging early)\n",
    "print(\"\\nDF_ENTITY_TO_CHUNKS columns:\", list(DF_ENTITY_TO_CHUNKS.columns))\n",
    "print(\"DF_CHUNK_TO_ENTITIES columns:\", list(DF_CHUNK_TO_ENTITIES.columns))\n",
    "print(\"DF_PLAYER_TO_CHUNKS columns:\", list(DF_PLAYER_TO_CHUNKS.columns))\n",
    "print(\"DF_SOURCE_FILES columns:\", list(DF_SOURCE_FILES.columns))\n",
    "\n",
    "# cleanup locals\n",
    "del pd, Path, errors, required, key, fname, p, cols, df_name, df, missing\n",
    "del expected_cols, INDEX_VERSION_SUFFIX, INDEX_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06190a3b-7085-471e-ad88-84239299c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up INDEXES_PATH that has been loaded into dataframes\n",
    "# keep INDEXES_RELPATH for writing out final instructions\n",
    "del INDEXES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33410b0f-25cf-459c-970d-154a9c63d0d3",
   "metadata": {},
   "source": [
    "## Phase P3: Load vocabulary tables\n",
    "\n",
    "This phase loads optional vocabulary tables that enable human-readable\n",
    "resolution and display during querying.\n",
    "\n",
    "The notebook:\n",
    "\n",
    "- Loads `vocab_entities.csv`\n",
    "- Loads `vocab_aliases.csv`\n",
    "- Loads `vocab_author_aliases.csv`\n",
    "- Loads `vocab_map_player_character.csv`\n",
    "- Validates minimal required columns (presence only)\n",
    "- Publishes vocabulary dataframes for use in resolution helpers\n",
    "\n",
    "This phase does not modify index tables and does not merge data.\n",
    "It only prepares lookup tables for name resolution and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d2d0d-6e7c-460a-b651-eb0953a93719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Load vocabulary tables (human-authored CSVs; entities required)\n",
    "LAST_PHASE_RUN = \"P3\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Semantic column mappings\n",
    "# ------------------------------------------------------------------\n",
    "ENTITY_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"canonical\": [\"canonical\", \"canonical_name\", \"name\"],\n",
    "}\n",
    "ALIAS_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"alias\": [\"alias\", \"alt\", \"alternate\"],\n",
    "}\n",
    "AUTHOR_ALIAS_COLS = {\n",
    "    \"author\": [\"author\", \"discord_name\", \"handle\"],\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"ambig_char_id\": [\"ambig_char_id\", \"ambiguous_character\", \"ambig_character\"],\n",
    "}\n",
    "PC_MAP_COLS = {\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"char_entity_id\": [\"char_entity_id\", \"character_entity_id\", \"character\"],\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Use descriptor-validated vocab paths (from Phase 1)\n",
    "# ------------------------------------------------------------------\n",
    "vocab_files = [\n",
    "    (\"entities\", VOCAB_ENTITIES_PATH, ENTITY_COLS, True),\n",
    "    (\"aliases\", VOCAB_ALIASES_PATH, ALIAS_COLS, False),\n",
    "    (\"author_aliases\", VOCAB_AUTHORS_PATH, AUTHOR_ALIAS_COLS, False),\n",
    "    (\"pc_map\", VOCAB_PC_MAP_PATH, PC_MAP_COLS, False),\n",
    "]\n",
    "\n",
    "# Published outputs\n",
    "DF_VOCAB_ENTITIES = pd.DataFrame(columns=list(ENTITY_COLS.keys()))\n",
    "DF_VOCAB_ALIASES = pd.DataFrame(columns=list(ALIAS_COLS.keys()))\n",
    "DF_VOCAB_AUTHORS = pd.DataFrame(columns=list(AUTHOR_ALIAS_COLS.keys()))\n",
    "DF_VOCAB_PC_MAP = pd.DataFrame(columns=list(PC_MAP_COLS.keys()))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load + normalize (looped, inline)\n",
    "# ------------------------------------------------------------------\n",
    "for key, path_obj, col_map, required in vocab_files:\n",
    "\n",
    "    if not path_obj:\n",
    "        if required:\n",
    "            errors.append(f\"Missing required path for {key} in descriptor.\")\n",
    "        continue\n",
    "\n",
    "    p = Path(path_obj)\n",
    "\n",
    "    if required and not p.exists():\n",
    "        errors.append(f\"Missing required vocabulary file:\\n  {p}\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"Optional vocab file not found: {p}\")\n",
    "        continue\n",
    "\n",
    "    raw_df = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "\n",
    "    rename = {}\n",
    "    for semantic, options in col_map.items():\n",
    "        found = next((c for c in options if c in raw_df.columns), None)\n",
    "        if found:\n",
    "            rename[found] = semantic\n",
    "\n",
    "    if len(raw_df) > 0 and not rename:\n",
    "        warnings.append(\n",
    "            f\"[{key}] CSV has rows but none of the expected columns were found.\\n\"\n",
    "            f\"  CSV columns: {list(raw_df.columns)}\\n\"\n",
    "            f\"  Expected mapping: {col_map}\\n\"\n",
    "            f\"  File: {p}\"\n",
    "        )\n",
    "        norm_df = pd.DataFrame(columns=list(col_map.keys()))\n",
    "    else:\n",
    "        out = raw_df.rename(columns=rename)\n",
    "        keep = [k for k in col_map.keys() if k in out.columns]\n",
    "        norm_df = out[keep].copy()\n",
    "\n",
    "    if key == \"entities\":\n",
    "        DF_VOCAB_ENTITIES = norm_df\n",
    "    elif key == \"aliases\":\n",
    "        DF_VOCAB_ALIASES = norm_df\n",
    "    elif key == \"author_aliases\":\n",
    "        DF_VOCAB_AUTHORS = norm_df\n",
    "    elif key == \"pc_map\":\n",
    "        DF_VOCAB_PC_MAP = norm_df\n",
    "\n",
    "    del raw_df, rename, semantic, options, found, out, keep, norm_df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Hard validation: entities must be usable\n",
    "# ------------------------------------------------------------------\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase 3 cannot proceed.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nFix the descriptor or vocabulary files, then rerun Phase 3.\"\n",
    "    )\n",
    "\n",
    "if DF_VOCAB_ENTITIES.empty:\n",
    "    raise ValueError(\n",
    "        \"Entities vocab file loaded but no usable rows were found.\\n\"\n",
    "        \"Ensure the CSV contains entity_id and canonical columns.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build DF_VOCAB_LOOKUP (unified vocab table for remapping)\n",
    "# Columns:\n",
    "#   - vocab_id: entity_id or player_entity_id\n",
    "#   - vocab: canonical / alias / author handle\n",
    "#   - vocab_kind: \"entity\" | \"alias\" | \"author\"\n",
    "#   - vocab_norm: lowercase normalized vocab for matching\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "# Entities (canonical names)\n",
    "for _, r in DF_VOCAB_ENTITIES.iterrows():\n",
    "    vid = str(r.get(\"entity_id\", \"\")).strip()\n",
    "    v = str(r.get(\"canonical\", \"\")).strip()\n",
    "    if vid and v:\n",
    "        rows.append([vid, v, \"entity\"])\n",
    "\n",
    "# Aliases (optional)\n",
    "if DF_VOCAB_ALIASES is not None and not DF_VOCAB_ALIASES.empty:\n",
    "    for _, r in DF_VOCAB_ALIASES.iterrows():\n",
    "        vid = str(r.get(\"entity_id\", \"\")).strip()\n",
    "        v = str(r.get(\"alias\", \"\")).strip()\n",
    "        if vid and v:\n",
    "            rows.append([vid, v, \"alias\"])\n",
    "\n",
    "# Author handles (optional)\n",
    "if DF_VOCAB_AUTHORS is not None and not DF_VOCAB_AUTHORS.empty:\n",
    "    for _, r in DF_VOCAB_AUTHORS.iterrows():\n",
    "        vid = str(r.get(\"player_entity_id\", \"\")).strip()\n",
    "        v = str(r.get(\"author\", \"\")).strip()\n",
    "        if vid and v:\n",
    "            rows.append([vid, v, \"author\"])\n",
    "\n",
    "DF_VOCAB_LOOKUP = pd.DataFrame(rows, columns=[\"vocab_id\", \"vocab\", \"vocab_kind\"])\n",
    "DF_VOCAB_LOOKUP[\"vocab_norm\"] = DF_VOCAB_LOOKUP[\"vocab\"].astype(str).str.strip().str.lower()\n",
    "DF_VOCAB_LOOKUP = DF_VOCAB_LOOKUP.drop_duplicates(\n",
    "    subset=[\"vocab_id\", \"vocab_norm\", \"vocab_kind\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "del rows, r, vid, v\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Summary\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Phase 3 OK: vocabulary tables loaded.\")\n",
    "\n",
    "print(\"\\nLoaded vocab tables:\")\n",
    "print(f\"- DF_VOCAB_ENTITIES: {len(DF_VOCAB_ENTITIES):>8} rows, {len(DF_VOCAB_ENTITIES.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_ALIASES:  {len(DF_VOCAB_ALIASES):>8} rows, {len(DF_VOCAB_ALIASES.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_AUTHORS:  {len(DF_VOCAB_AUTHORS):>8} rows, {len(DF_VOCAB_AUTHORS.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_PC_MAP:   {len(DF_VOCAB_PC_MAP):>8} rows, {len(DF_VOCAB_PC_MAP.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_LOOKUP:   {len(DF_VOCAB_LOOKUP):>8} rows, {len(DF_VOCAB_LOOKUP.columns):>3} cols\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del pd, Path\n",
    "del errors, warnings, vocab_files, key, path_obj, col_map, required, p\n",
    "del ENTITY_COLS, ALIAS_COLS, AUTHOR_ALIAS_COLS, PC_MAP_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42e691-a75d-4bc0-bef6-2695e7a1c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up VOCAB path variables\n",
    "del VOCAB_ENTITIES_PATH, VOCAB_ENTITIES_RELPATH\n",
    "del VOCAB_ALIASES_PATH, VOCAB_ALIASES_RELPATH\n",
    "del VOCAB_AUTHORS_PATH, VOCAB_AUTHORS_RELPATH\n",
    "del VOCAB_PC_MAP_PATH, VOCAB_PC_MAP_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7baa54-c6fc-40ba-bbe1-a82f3a00f8de",
   "metadata": {},
   "source": [
    "# Graph Bootstrap\n",
    "\n",
    "This section translates index grammar into graph grammar and builds:\n",
    "\n",
    "- graph_nodes_v0.csv  \n",
    "- graph_edges_v0.csv  \n",
    "\n",
    "Artifacts are written to `working_drafts` for review before promotion to `indexes.path`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef964eb-10f6-4589-8946-ae00c7e62d75",
   "metadata": {},
   "source": [
    "## Phase N: Build graph nodes\n",
    "\n",
    "This phase defines **what things exist** in the graph.\n",
    "\n",
    "Index tables describe things implicitly.\n",
    "Here we make them explicit as graph nodes.\n",
    "\n",
    "Nodes represent:\n",
    "- entities (person, place, faction, etc.)\n",
    "- vocab text forms\n",
    "- chunks\n",
    "- files\n",
    "\n",
    "No relationships are created in this phase.\n",
    "Only the vocabulary of “things” the graph can talk about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc30d57-2672-44fe-bacf-d1ff3cd597fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph nodes (CSV-only; no networkx here)\n",
    "LAST_PHASE_RUN = \"N\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Nodes: start empty, append sources, then finalize\n",
    "# -------------------------------------------------------------------\n",
    "nodes = []\n",
    "\n",
    "# 1) Entity nodes (from vocab entities)\n",
    "# node_type = prefix of entity_id (before first \"_\"), e.g. \"artifact_folly\" -> \"artifact\"\n",
    "nodes.append(\n",
    "    DF_VOCAB_ENTITIES.assign(\n",
    "        node_id=lambda d: d[\"entity_id\"].astype(str),\n",
    "        node_type=lambda d: d[\"entity_id\"].astype(str).str.split(\"_\", n=1).str[0],\n",
    "        label=lambda d: d[\"canonical\"].astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Chunk nodes (from chunk index)\n",
    "nodes.append(\n",
    "    DF_CHUNK_TO_ENTITIES.assign(\n",
    "        node_id=lambda d: \"chunk_\" + d[\"chunk_id\"].astype(int).astype(str),\n",
    "        node_type=\"chunk\",\n",
    "        label=lambda d: \"chunk_\" + d[\"chunk_id\"].astype(int).astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# 3) File nodes (from source files)\n",
    "# node_type = source_type (pbp_transcripts, session_notes, etc.)\n",
    "nodes.append(\n",
    "    DF_SOURCE_FILES.assign(\n",
    "        node_id=lambda d: \"file:\" + d[\"relpath\"].astype(str),\n",
    "        node_type=lambda d: d[\"source_type\"].astype(str),\n",
    "        label=lambda d: d[\"relpath\"].astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# 4) Vocab nodes (from consolidated vocab lookup)\n",
    "# node_id = stable text-form node keyed by vocab_norm\n",
    "# label   = original vocab string (human-readable)\n",
    "nodes.append(\n",
    "    DF_VOCAB_LOOKUP.assign(\n",
    "        node_id=lambda d: \"vocab:\" + d[\"vocab_norm\"].astype(str),\n",
    "        node_type=\"vocab\",\n",
    "        label=lambda d: d[\"vocab\"].astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "# 5) Finalize\n",
    "DF_GRAPH_NODES = (\n",
    "    pd.concat(nodes, ignore_index=True)\n",
    "      .drop_duplicates(subset=[\"node_id\"])\n",
    "      .sort_values([\"node_type\", \"node_id\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sanity check (compact but useful)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Graph nodes built.\")\n",
    "print(f\"Total nodes: {len(DF_GRAPH_NODES)}\")\n",
    "print(\"\\nCounts by node_type:\")\n",
    "display(DF_GRAPH_NODES[\"node_type\"].value_counts().to_frame(\"count\"))\n",
    "\n",
    "print(\"\\nSample nodes:\")\n",
    "display(DF_GRAPH_NODES.sample(min(5, len(DF_GRAPH_NODES)), random_state=7))\n",
    "\n",
    "# cleanup locals (keep DF_GRAPH_NODES)\n",
    "del nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42798002-4c60-412e-94e1-9f46a0d507b4",
   "metadata": {},
   "source": [
    "## Phase E: Build graph edges\n",
    "\n",
    "This phase defines **how things relate** in the graph.\n",
    "\n",
    "Index tables express relationships implicitly.\n",
    "Here we translate them into explicit graph grammar:\n",
    "\n",
    "subject → predicate → object  \n",
    "(+ weight where needed)\n",
    "\n",
    "Each edge is a statement derived directly from canonical indexes.\n",
    "No interpretation or heuristics are introduced.\n",
    "\n",
    "This phase produces a single edge table ready for graph queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951abf8c-297e-46aa-a58f-f78492cf2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Phase: Graph edges (v0)\n",
    "# -------------------------------------------------------------------\n",
    "LAST_PHASE_RUN = \"E\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Relationships sourced from DF_CHUNK_TO_ENTITIES\n",
    "# -------------------------------------------------------------------\n",
    "# File contains Chunk:\n",
    "#   Source grammar (index table):\n",
    "#     relpath + chunk_id  => \"This chunk is located in this file\"\n",
    "#   Target grammar (graph edges):\n",
    "#     subject + predicate + object  => \"file:<relpath> contains chunk_<id>\"\n",
    "#\n",
    "# Chunk mentions Vocab:\n",
    "#   Source grammar (index table):\n",
    "#     chunk_id + matched_vocabs  => \"This chunk contains these text forms\"\n",
    "#   Target grammar (graph edges):\n",
    "#     subject + predicate + object  => \"chunk_<id> mentions vocab:<text>\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_CHUNK_TO_ENTITIES.loc[:, [\"chunk_id\", \"relpath\", \"matched_vocabs\", \"entity_ids\"]].iterrows():\n",
    "    chunk_node = f\"chunk_{int(r['chunk_id'])}\"\n",
    "    file_node = f\"file:{r['relpath']}\"\n",
    "\n",
    "    # file contains chunk\n",
    "    rows.append((file_node, \"contains\", chunk_node, pd.NA))\n",
    "\n",
    "    # chunk mentions vocab (pipe-delimited)\n",
    "    for v in (x.strip() for x in str(r[\"matched_vocabs\"]).split(\"|\")):\n",
    "        rows.append((chunk_node, \"mentions\", f\"vocab:{v}\", pd.NA))\n",
    "\n",
    "    # entity co-occurs with entity (within this chunk) -> \"votes\" (weight=1)\n",
    "    # chunk_id + entity_ids => \"<entity_id A> cooccurs_with <entity_id B>\" (undirected via A < B)\n",
    "    entity_ids = sorted({e.strip() for e in str(r[\"entity_ids\"]).split(\"|\") if e.strip()})\n",
    "    for i in range(len(entity_ids)):\n",
    "        for j in range(i + 1, len(entity_ids)):\n",
    "            rows.append((entity_ids[i], \"cooccurs_with\", entity_ids[j], 1))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Vocab refers_to Entity/Player\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (vocab lookup table):\n",
    "#   vocab_norm + vocab_id  => \"This text form refers to this thing\"\n",
    "#\n",
    "# Target grammar (graph edges):\n",
    "#   subject + predicate + object  => \"vocab:<vocab_norm> refers_to <vocab_id>\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_VOCAB_LOOKUP.loc[:, [\"vocab_norm\", \"vocab_id\"]].iterrows():\n",
    "\n",
    "    # Translate vocab_norm (\"shadowboy\") -> graph node id (\"vocab:shadowboy\")\n",
    "    subject = f\"vocab:{str(r['vocab_norm']).strip()}\"\n",
    "\n",
    "    # vocab_id is already the target node id (entity_id or player_entity_id)\n",
    "    object_ = str(r[\"vocab_id\"]).strip()\n",
    "\n",
    "    rows.append((subject, \"refers_to\", object_, pd.NA))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Player plays Character\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (vocab table):\n",
    "#    char_entity_id + player_entity_id  => \"Character is played by Player\"\n",
    "#\n",
    "# Target grammar (graph edges):\n",
    "#   subject + predicate + object  => \"<player_entity_id> plays <char_entity_id>\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_VOCAB_PC_MAP.loc[:, [\"player_entity_id\", \"char_entity_id\"]].iterrows():\n",
    "    subject = str(r[\"player_entity_id\"]).strip()\n",
    "    object_ = str(r[\"char_entity_id\"]).strip()\n",
    "    rows.append((subject, \"plays\", object_, pd.NA))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Entity co-occurs with Entity (within same chunk)\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (index table):\n",
    "#   chunk_id + entity_ids\n",
    "#     => \"These entities appear together in this chunk\"\n",
    "#\n",
    "# Target grammar (graph edges):\n",
    "#   subject + predicate + object\n",
    "#     => \"<entity_id_A> cooccurs_with <entity_id_B>\"\n",
    "#\n",
    "# Convention:\n",
    "#   Alphabetical ordering ensures one undirected edge per pair.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_CHUNK_TO_ENTITIES.loc[:, [\"entity_ids\"]].iterrows():\n",
    "\n",
    "    # Extract clean list of entity_ids in this chunk\n",
    "    entities = sorted(\n",
    "        e.strip()\n",
    "        for e in str(r[\"entity_ids\"]).split(\"|\")\n",
    "        if e.strip()\n",
    "    )\n",
    "\n",
    "    # Build all unordered pairs (i < j ensures no duplicates)\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            subject = entities[i]\n",
    "            object_ = entities[j]\n",
    "            rows.append((subject, \"cooccurs_with\", object_, 1))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Aggregate and build the dataframe\n",
    "# -------------------------------------------------------------------\n",
    "DF_GRAPH_EDGES = (\n",
    "    pd.DataFrame(rows, columns=[\"subject\", \"predicate\", \"object\", \"weight\"])\n",
    "      .groupby([\"subject\", \"predicate\", \"object\"], as_index=False)\n",
    "      .agg(weight=(\"weight\", lambda s: s.sum(min_count=1)))\n",
    "      .sort_values([\"predicate\", \"subject\", \"object\"], ascending=[True, True, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sanity check\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Graph edges built (v0)\")\n",
    "print(f\"Edges: {len(DF_GRAPH_EDGES)}\")\n",
    "print(\"Edge counts by predicate:\\n\")\n",
    "\n",
    "display(\n",
    "    DF_GRAPH_EDGES\n",
    "        .groupby(\"predicate\", as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"edge_count\"})\n",
    "        .sort_values(\"edge_count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(DF_GRAPH_EDGES.sample(3))\n",
    "\n",
    "# clean up locals\n",
    "del rows, r, chunk_node, file_node, v, entity_ids, i, j, subject, object_, entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88f4e7-b6ce-4247-85cb-29ce278dcc85",
   "metadata": {},
   "source": [
    "## Phase W: Write graph artifacts\n",
    "\n",
    "This phase writes the generated graph tables to `working_drafts` for review.\n",
    "\n",
    "After reviewing the CSV files:\n",
    "- If satisfied, move or copy them into `indexes.path`\n",
    "- Commit them to version control\n",
    "\n",
    "This notebook does not modify canonical data.\n",
    "It only generates reproducible artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1967c-e683-4fc8-838a-bd9dc42ca3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase: Write graph artifacts (v0) - simplified\n",
    "# Output (written to WORKING_DRAFTS_PATH only):\n",
    "#   - graph_nodes_v0.csv\n",
    "#   - graph_edges_v0.csv\n",
    "LAST_PHASE_RUN = \"W1\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate inputs (presence only)\n",
    "# ------------------------------------------------------------------\n",
    "if \"DF_GRAPH_NODES\" not in globals() or DF_GRAPH_NODES is None or DF_GRAPH_NODES.empty:\n",
    "    raise ValueError(\"DF_GRAPH_NODES is missing or empty. Build nodes first.\")\n",
    "\n",
    "if \"DF_GRAPH_EDGES\" not in globals() or DF_GRAPH_EDGES is None or DF_GRAPH_EDGES.empty:\n",
    "    raise ValueError(\"DF_GRAPH_EDGES is missing or empty. Build edges first.\")\n",
    "\n",
    "if \"WORKING_DRAFTS_PATH\" not in globals() or not WORKING_DRAFTS_PATH:\n",
    "    raise ValueError(\"WORKING_DRAFTS_PATH is missing. Rerun Phase P1.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Write artifacts to WORKING_DRAFTS_PATH (never to canonical indexes)\n",
    "# ------------------------------------------------------------------\n",
    "out_dir = Path(WORKING_DRAFTS_PATH)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_rel = globals().get(\"WORKING_DRAFTS_RELPATH\", str(out_dir)).rstrip(\"/\")\n",
    "\n",
    "DF_GRAPH_NODES.to_csv(out_dir / \"graph_nodes_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "DF_GRAPH_EDGES.to_csv(out_dir / \"graph_edges_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Graph artifacts written:\")\n",
    "print(f\" - {out_rel}/graph_nodes_v0.csv\")\n",
    "print(f\" - {out_rel}/graph_edges_v0.csv\")\n",
    "\n",
    "display(DF_GRAPH_NODES.head(5))\n",
    "display(DF_GRAPH_EDGES.head(5))\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1) Review the generated CSV files in:\")\n",
    "print(f\"   {out_rel}\")\n",
    "\n",
    "print(\"\\n2) If satisfied, move (or copy) these files into your canonical indexes directory:\")\n",
    "print(f\"   {INDEXES_RELPATH}\")\n",
    "\n",
    "print(\"\\n3) Commit the moved graph CSVs (and any descriptor updates) to version control if desired.\")\n",
    "\n",
    "print(\"\\nThis notebook does NOT modify canonical data. All artifacts were written to working_drafts.\")\n",
    "\n",
    "# cleanup locals\n",
    "del Path, out_dir, out_rel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iwtc-tools)",
   "language": "python",
   "name": "iwtc-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
