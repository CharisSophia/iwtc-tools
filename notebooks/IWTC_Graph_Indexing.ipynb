{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16ccca7-3bd6-42fb-9345-a58c07280b0f",
   "metadata": {},
   "source": [
    "# IWTC Graph Indexing\n",
    "\n",
    "This notebook builds graph-ready artifacts from the existing IWTC index tables.\n",
    "It does **not** perform query recipes. Query logic belongs in a separate graph query notebook. This notebook is structural.\n",
    "\n",
    "It does the following\n",
    "- Loads existing index artifacts (Phases P0–P3)\n",
    "- Transforms them into node/edge representations\n",
    "- Writes graph artifacts under `indexes.path`\n",
    "\n",
    "Run the Pre-load section once, then use the Graph Bootstrap section to generate or regenerate graph artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df070d7-9e84-4ced-909a-afab540fe5b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-load (Phases 0–3 from Index Query)\n",
    "\n",
    "This section loads:\n",
    "- world repository descriptor\n",
    "- index artifacts\n",
    "- vocabulary tables\n",
    "\n",
    "It must succeed before graph construction begins.\n",
    "\n",
    "You can collapse this entire section after it runs successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f637a-2282-4b3c-a294-da65fae45712",
   "metadata": {},
   "source": [
    "## Phase P0: Parameters\n",
    "\n",
    "This notebook operates on a **campaign world repository** and loads previously\n",
    "generated index artifacts for interactive querying.\n",
    "\n",
    "In this phase, you tell the notebook:\n",
    "\n",
    "- Which world repository it is operating on.\n",
    "- Which index artifact version it should expect to load.\n",
    "\n",
    "This notebook **will** generate new graph indexes.\n",
    "It does **not** modify source indexes.\n",
    "It does **not** modify canonical files.\n",
    "It does **not** alter schema.\n",
    "\n",
    "The goal is simply to answer:\n",
    "*\"What indexed world am I querying right now?\"*\n",
    "\n",
    "The code cell below contains inline comments explaining each parameter.\n",
    "\n",
    "**IMPORTANT:** This notebook assumes index artifacts already exist and will fail\n",
    "if required CSV files are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862540a0-d2ef-43c7-9a85-6b57f2af2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run initialized at: 2026-02-16 15:03\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Parameters\n",
    "LAST_PHASE_RUN = \"P0\"\n",
    "\n",
    "# Absolute path to the world_repository.yml descriptor.\n",
    "WORLD_REPOSITORY_DESCRIPTOR = (\n",
    "    \"/Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/descriptors/world_repository.yml\"\n",
    ")\n",
    "\n",
    "# Index version to load (must match previously generated artifacts)\n",
    "INDEX_VERSION = \"V0\"\n",
    "\n",
    "# Internal run metadata (do not edit)\n",
    "from datetime import datetime\n",
    "print(f\"Notebook run initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "del datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b7688-b61f-4b66-8513-d60de5b83094",
   "metadata": {},
   "source": [
    "## Phase P1: Load and validate world descriptor\n",
    "\n",
    "Before this notebook can safely read or write anything, it must be confident that it understands the **structure of the world repository**.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Loads the world repository descriptor file you provided\n",
    "- Confirms that it is readable and structurally valid\n",
    "- Extracts only the information this notebook needs\n",
    "- Verifies that referenced paths actually exist and are usable\n",
    "\n",
    "This phase answers a single question:\n",
    "\n",
    "**“Can I trust this descriptor enough to proceed?”**\n",
    "\n",
    "If the answer is *no*, the notebook will stop with clear, actionable error messages explaining what needs to be fixed in the descriptor file.  \n",
    "Nothing is modified, created, or scanned until this check succeeds.\n",
    "\n",
    "This phase does **not** interpret world lore, indexing rules, or heuristics.  \n",
    "It only establishes that the filesystem layout described by the world is coherent and usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2a6182-f431-4069-ac94-e8b9efe73fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World repository descriptor loaded successfully: world_repository.yml\n",
      "Descriptor paths are usable for this notebook.\n",
      "world_root: /Users/charissophia/obsidian/Iron Wolf Trading Company\n",
      "indexes: _meta/indexes\n",
      "vocab.entities: _meta/indexes/vocab_entities.csv (exists=True)\n",
      "vocab.aliases: _meta/indexes/vocab_aliases.csv (exists=True)\n",
      "vocab.author_aliases: _meta/indexes/vocab_author_aliases.csv (exists=True)\n",
      "vocab.player_character_map: _meta/indexes/vocab_map_player_character.csv (exists=True)\n"
     ]
    }
   ],
   "source": [
    "# Phase P1: Load and validate world repository descriptor (Index Querying v0)\n",
    "LAST_PHASE_RUN = \"P1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# --- Load descriptor file ---\n",
    "descriptor_path = Path(WORLD_REPOSITORY_DESCRIPTOR)\n",
    "\n",
    "if not descriptor_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"World repository descriptor file was not found.\\n\"\n",
    "        f\"Path provided:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Confirm the file exists at this location or fix WORLD_REPOSITORY_DESCRIPTOR in Phase 0\\n\"\n",
    "        \"- If you just edited Phase 0, rerun Phase 0 and then rerun this cell\\n\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    with descriptor_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        world_repo = yaml.safe_load(f)\n",
    "except Exception:\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor could not be read.\\n\"\n",
    "        \"This usually indicates a YAML formatting problem.\\n\\n\"\n",
    "        f\"File:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Paste the contents into https://www.yamllint.com/\\n\"\n",
    "        \"- Fix any reported issues, save the file, and rerun this cell\"\n",
    "    )\n",
    "\n",
    "if not isinstance(world_repo, dict):\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor was read, but its structure is not usable.\\n\"\n",
    "        \"The file must be a YAML mapping (top-level `name: value` entries).\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Ensure it uses clear `name: value` lines\\n\"\n",
    "        \"- Fix the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "print(f\"World repository descriptor loaded successfully: {descriptor_path.name}\")\n",
    "\n",
    "# --- Extract required entries ---\n",
    "WORLD_ROOT_RAW = world_repo.get(\"world_root\")\n",
    "\n",
    "indexes_block = world_repo.get(\"indexes\")\n",
    "INDEXES_RAW = indexes_block.get(\"path\") if isinstance(indexes_block, dict) else None\n",
    "\n",
    "vocab = world_repo.get(\"vocabulary\")\n",
    "ENTITIES_RAW = vocab.get(\"entities\") if isinstance(vocab, dict) else None\n",
    "ALIASES_RAW = vocab.get(\"aliases\") if isinstance(vocab, dict) else None\n",
    "AUTHORS_RAW = vocab.get(\"author_aliases\") if isinstance(vocab, dict) else None\n",
    "PC_MAP_RAW = vocab.get(\"player_character_map\") if isinstance(vocab, dict) else None\n",
    "\n",
    "if not WORLD_ROOT_RAW:\n",
    "    errors.append(\"Missing required entry: world_root\")\n",
    "\n",
    "if not INDEXES_RAW:\n",
    "    errors.append(\"Missing required entry: indexes.path\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"World repository descriptor is missing required entries:\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Edit your world_repository.yml and add/fix the missing entries\\n\"\n",
    "          \"- Save the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "# --- Validate and resolve world_root ---\n",
    "WORLD_ROOT = Path(WORLD_ROOT_RAW)\n",
    "\n",
    "if str(WORLD_ROOT).startswith(\"~\"):\n",
    "    errors.append(\"world_root: '~' is not allowed. Use a full absolute path.\")\n",
    "elif not WORLD_ROOT.is_absolute():\n",
    "    errors.append(\"world_root must be an absolute path (starts with / on macOS/Linux, or C:\\\\ on Windows).\")\n",
    "elif not WORLD_ROOT.is_dir():\n",
    "    errors.append(f\"world_root must be an existing directory: {WORLD_ROOT}\")\n",
    "else:\n",
    "    WORLD_ROOT = WORLD_ROOT.resolve()\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "# --- Resolve and validate indexes path ---\n",
    "INDEXES_PATH = Path(INDEXES_RAW)\n",
    "if not INDEXES_PATH.is_absolute():\n",
    "    INDEXES_PATH = WORLD_ROOT / INDEXES_PATH\n",
    "INDEXES_PATH = INDEXES_PATH.resolve()\n",
    "\n",
    "try:\n",
    "    INDEXES_RELPATH = str(INDEXES_PATH.relative_to(WORLD_ROOT))\n",
    "except Exception:\n",
    "    INDEXES_RELPATH = str(INDEXES_PATH)\n",
    "\n",
    "if not INDEXES_PATH.exists():\n",
    "    errors.append(f\"indexes: path does not exist: {INDEXES_PATH}\")\n",
    "elif not INDEXES_PATH.is_dir():\n",
    "    errors.append(f\"indexes: {INDEXES_PATH} must be a directory\")\n",
    "\n",
    "# --- Resolve vocabulary paths (optional) ---\n",
    "VOCAB_ENTITIES_PATH = None\n",
    "VOCAB_ENTITIES_RELPATH = None\n",
    "VOCAB_ALIASES_PATH = None\n",
    "VOCAB_ALIASES_RELPATH = None\n",
    "VOCAB_AUTHORS_PATH = None\n",
    "VOCAB_AUTHORS_RELPATH = None\n",
    "VOCAB_PC_MAP_PATH = None\n",
    "VOCAB_PC_MAP_RELPATH = None\n",
    "\n",
    "vocab_entries = [\n",
    "    (\"entities\", \"vocab.entities\"),\n",
    "    (\"aliases\", \"vocab.aliases\"),\n",
    "    (\"author_aliases\", \"vocab.author_aliases\"),\n",
    "    (\"player_character_map\", \"vocab.player_character_map\"),\n",
    "]\n",
    "\n",
    "for key, label in vocab_entries:\n",
    "    raw = vocab.get(key)\n",
    "    if not raw:\n",
    "        continue\n",
    "\n",
    "    p = Path(raw)\n",
    "    if not p.is_absolute():\n",
    "        p = WORLD_ROOT / p\n",
    "    p = p.resolve()\n",
    "\n",
    "    try:\n",
    "        rel = str(p.relative_to(WORLD_ROOT))\n",
    "    except Exception:\n",
    "        rel = str(p)\n",
    "\n",
    "    if p.exists() and p.is_dir():\n",
    "        warnings.append(f\"{label}: {p} must be a file (got directory). Ignoring.\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"{label}: file does not exist: {p} (name resolution may be limited).\")\n",
    "\n",
    "    if key == \"entities\":\n",
    "        VOCAB_ENTITIES_PATH = p\n",
    "        VOCAB_ENTITIES_RELPATH = rel\n",
    "    elif key == \"aliases\":\n",
    "        VOCAB_ALIASES_PATH = p\n",
    "        VOCAB_ALIASES_RELPATH = rel\n",
    "    elif key == \"author_aliases\":\n",
    "        VOCAB_AUTHORS_PATH = p\n",
    "        VOCAB_AUTHORS_RELPATH = rel\n",
    "    elif key == \"player_character_map\":\n",
    "        VOCAB_PC_MAP_PATH = p\n",
    "        VOCAB_PC_MAP_RELPATH = rel\n",
    "\n",
    "print(\"Descriptor paths are usable for this notebook.\")\n",
    "print(f\"world_root: {WORLD_ROOT}\")\n",
    "print(f\"indexes: {INDEXES_RELPATH}\")\n",
    "print(f\"vocab.entities: {VOCAB_ENTITIES_RELPATH} (exists={VOCAB_ENTITIES_PATH.exists() if VOCAB_ENTITIES_PATH else False})\")\n",
    "print(f\"vocab.aliases: {VOCAB_ALIASES_RELPATH} (exists={VOCAB_ALIASES_PATH.exists() if VOCAB_ALIASES_PATH else False})\")\n",
    "print(f\"vocab.author_aliases: {VOCAB_AUTHORS_RELPATH} (exists={VOCAB_AUTHORS_PATH.exists() if VOCAB_AUTHORS_PATH else False})\")\n",
    "print(f\"vocab.player_character_map: {VOCAB_PC_MAP_RELPATH} (exists={VOCAB_PC_MAP_PATH.exists() if VOCAB_PC_MAP_PATH else False})\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del yaml, Path\n",
    "del descriptor_path, world_repo, indexes_block, vocab\n",
    "del WORLD_REPOSITORY_DESCRIPTOR\n",
    "del WORLD_ROOT_RAW, INDEXES_RAW, ENTITIES_RAW, ALIASES_RAW, AUTHORS_RAW, PC_MAP_RAW\n",
    "del vocab_entries, key, label, raw, p, rel, errors, warnings, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df1569-6575-4e04-922c-52f6b5f79103",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase P2: Load index artifacts\n",
    "\n",
    "Before this notebook can execute any queries, it must confirm that the\n",
    "required source index artifacts already exist and can be loaded.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Constructs the expected index artifact filenames based on `INDEX_VERSION`\n",
    "- Confirms those files exist under the repository’s declared `indexes.path`\n",
    "- Loads each artifact as a raw dataframe\n",
    "- Verifies that required columns are present\n",
    "- Publishes stable dataframe variables for downstream query logic\n",
    "\n",
    "This phase answers a single question:\n",
    "\n",
    "**“Are the required source index artifacts present and structurally usable?”**\n",
    "\n",
    "If any required artifact is missing or malformed, the notebook will stop\n",
    "with clear instructions explaining how to regenerate them.\n",
    "\n",
    "No canonical files are modified.\n",
    "\n",
    "This phase only establishes the concrete, in-memory tables that the graph index bootstrap will operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbff85b-df65-485e-9ade-fd0d9bbf4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 OK: index artifacts loaded.\n",
      "indexes.path: /Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/indexes\n",
      "index version: v0\n",
      "\n",
      "Loaded tables:\n",
      "- DF_ENTITY_TO_CHUNKS:        168 rows,   6 cols\n",
      "- DF_CHUNK_TO_ENTITIES:      1139 rows,  11 cols\n",
      "- DF_PLAYER_TO_CHUNKS:          6 rows,   6 cols\n",
      "- DF_SOURCE_FILES:            130 rows,   3 cols\n",
      "\n",
      "DF_ENTITY_TO_CHUNKS columns: ['entity_id', 'canonical', 'chunk_ids', 'chunk_count', 'file_relpaths', 'file_count']\n",
      "DF_CHUNK_TO_ENTITIES columns: ['chunk_id', 'source_id', 'source_type', 'relpath', 'chunk_start_line', 'chunk_end_line', 'entity_ids', 'canonicals', 'entity_count', 'matched_vocabs', 'match_kinds']\n",
      "DF_PLAYER_TO_CHUNKS columns: ['player_entity_id', 'canonical', 'chunk_ids', 'chunk_count', 'file_relpaths', 'file_count']\n",
      "DF_SOURCE_FILES columns: ['source_id', 'relpath', 'source_type']\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Load index artifacts (v0)\n",
    "LAST_PHASE_RUN = \"P2\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "\n",
    "# Normalize INDEX_VERSION into the on-disk suffix (your files use lowercase v0)\n",
    "# Accepts \"V0\", \"v0\", \"0\" (if you ever use that), but publishes \"v0\"\n",
    "INDEX_VERSION_SUFFIX = f\"v{str(INDEX_VERSION).lower().lstrip('v')}\"\n",
    "\n",
    "# Required artifact filenames (fixed contract for this notebook)\n",
    "required = {\n",
    "    \"entity_to_chunks\": f\"index_entity_to_chunks_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"chunk_to_entities\": f\"index_chunk_to_entities_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"player_to_chunks\": f\"index_player_to_chunks_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"source_files\": f\"index_source_files_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "}\n",
    "\n",
    "# Resolve paths and validate existence\n",
    "INDEX_FILES = {}\n",
    "for key, fname in required.items():\n",
    "    p = (INDEXES_PATH / fname).resolve()\n",
    "    INDEX_FILES[key] = p\n",
    "    if not p.exists():\n",
    "        errors.append(f\"Missing required index artifact: {fname}\\n  Expected at: {p}\")\n",
    "\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase 2 cannot proceed because required index artifacts are missing.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Rerun IWTC_Raw_Source_Indexing.ipynb to generate the v0 artifacts\\n\"\n",
    "          \"- Ensure the resulting index_*.csv files are placed under your indexes.path directory\\n\"\n",
    "          f\"- indexes.path resolved to:\\n  {INDEXES_PATH}\\n\"\n",
    "          \"- Then rerun Phase 2\"\n",
    "    )\n",
    "\n",
    "# Load CSVs (raw)\n",
    "DF_ENTITY_TO_CHUNKS = pd.read_csv(INDEX_FILES[\"entity_to_chunks\"])\n",
    "DF_CHUNK_TO_ENTITIES = pd.read_csv(INDEX_FILES[\"chunk_to_entities\"])\n",
    "DF_PLAYER_TO_CHUNKS = pd.read_csv(INDEX_FILES[\"player_to_chunks\"])\n",
    "DF_SOURCE_FILES = pd.read_csv(INDEX_FILES[\"source_files\"])\n",
    "\n",
    "# Validate required columns (presence only)\n",
    "expected_cols = {\n",
    "    \"DF_ENTITY_TO_CHUNKS\": {\"entity_id\", \"canonical\", \"chunk_ids\", \"chunk_count\", \"file_relpaths\", \"file_count\"},\n",
    "    \"DF_CHUNK_TO_ENTITIES\": {\n",
    "        \"chunk_id\", \"source_id\", \"source_type\", \"relpath\",\n",
    "        \"chunk_start_line\", \"chunk_end_line\",\n",
    "        \"entity_ids\", \"canonicals\", \"entity_count\",\n",
    "        \"matched_vocabs\", \"match_kinds\",\n",
    "    },\n",
    "    \"DF_PLAYER_TO_CHUNKS\": {\"player_entity_id\", \"canonical\", \"chunk_ids\", \"chunk_count\", \"file_relpaths\", \"file_count\"},\n",
    "    \"DF_SOURCE_FILES\": {\"source_id\", \"relpath\", \"source_type\"},\n",
    "}\n",
    "\n",
    "for df_name, cols in expected_cols.items():\n",
    "    df = globals()[df_name]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"{df_name}: missing expected columns: {missing}\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"One or more index artifacts were loaded but do not match expected v0 columns.\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Confirm you are using the v0 CSVs produced by IWTC_Raw_Source_Indexing.ipynb\\n\"\n",
    "          \"- Do not edit the CSVs manually\\n\"\n",
    "          \"- If you changed the producer notebook, re-run it to regenerate indexes and retry\"\n",
    "    )\n",
    "\n",
    "# Summary prints\n",
    "print(\"Phase 2 OK: index artifacts loaded.\")\n",
    "print(f\"indexes.path: {INDEXES_PATH}\")\n",
    "print(f\"index version: {INDEX_VERSION_SUFFIX}\")\n",
    "\n",
    "print(\"\\nLoaded tables:\")\n",
    "print(f\"- DF_ENTITY_TO_CHUNKS:   {len(DF_ENTITY_TO_CHUNKS):>8} rows, {len(DF_ENTITY_TO_CHUNKS.columns):>3} cols\")\n",
    "print(f\"- DF_CHUNK_TO_ENTITIES:  {len(DF_CHUNK_TO_ENTITIES):>8} rows, {len(DF_CHUNK_TO_ENTITIES.columns):>3} cols\")\n",
    "print(f\"- DF_PLAYER_TO_CHUNKS:   {len(DF_PLAYER_TO_CHUNKS):>8} rows, {len(DF_PLAYER_TO_CHUNKS.columns):>3} cols\")\n",
    "print(f\"- DF_SOURCE_FILES:       {len(DF_SOURCE_FILES):>8} rows, {len(DF_SOURCE_FILES.columns):>3} cols\")\n",
    "\n",
    "# Optional: quick column display (helps debugging early)\n",
    "print(\"\\nDF_ENTITY_TO_CHUNKS columns:\", list(DF_ENTITY_TO_CHUNKS.columns))\n",
    "print(\"DF_CHUNK_TO_ENTITIES columns:\", list(DF_CHUNK_TO_ENTITIES.columns))\n",
    "print(\"DF_PLAYER_TO_CHUNKS columns:\", list(DF_PLAYER_TO_CHUNKS.columns))\n",
    "print(\"DF_SOURCE_FILES columns:\", list(DF_SOURCE_FILES.columns))\n",
    "\n",
    "# cleanup locals\n",
    "del pd, Path, errors, required, key, fname, p, cols, df_name, df, missing\n",
    "del expected_cols, INDEX_VERSION_SUFFIX, INDEX_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06190a3b-7085-471e-ad88-84239299c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up INDEXES path variables that have been loaded into dataframes\n",
    "del INDEXES_PATH, INDEXES_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33410b0f-25cf-459c-970d-154a9c63d0d3",
   "metadata": {},
   "source": [
    "## Phase P3: Load vocabulary tables\n",
    "\n",
    "This phase loads optional vocabulary tables that enable human-readable\n",
    "resolution and display during querying.\n",
    "\n",
    "The notebook:\n",
    "\n",
    "- Loads `vocab_entities.csv`\n",
    "- Loads `vocab_aliases.csv`\n",
    "- Loads `vocab_author_aliases.csv`\n",
    "- Loads `vocab_map_player_character.csv`\n",
    "- Validates minimal required columns (presence only)\n",
    "- Publishes vocabulary dataframes for use in resolution helpers\n",
    "\n",
    "This phase does not modify index tables and does not merge data.\n",
    "It only prepares lookup tables for name resolution and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485d2d0d-6e7c-460a-b651-eb0953a93719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3 OK: vocabulary tables loaded.\n",
      "\n",
      "Loaded vocab tables:\n",
      "- DF_VOCAB_ENTITIES:      176 rows,   2 cols\n",
      "- DF_VOCAB_ALIASES:        87 rows,   2 cols\n",
      "- DF_VOCAB_AUTHORS:         6 rows,   3 cols\n",
      "- DF_VOCAB_PC_MAP:         42 rows,   2 cols\n",
      "- DF_VOCAB_LOOKUP:        269 rows,   4 cols\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Load vocabulary tables (human-authored CSVs; entities required)\n",
    "LAST_PHASE_RUN = \"P3\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Semantic column mappings\n",
    "# ------------------------------------------------------------------\n",
    "ENTITY_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"canonical\": [\"canonical\", \"canonical_name\", \"name\"],\n",
    "}\n",
    "ALIAS_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"alias\": [\"alias\", \"alt\", \"alternate\"],\n",
    "}\n",
    "AUTHOR_ALIAS_COLS = {\n",
    "    \"author\": [\"author\", \"discord_name\", \"handle\"],\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"ambig_char_id\": [\"ambig_char_id\", \"ambiguous_character\", \"ambig_character\"],\n",
    "}\n",
    "PC_MAP_COLS = {\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"char_entity_id\": [\"char_entity_id\", \"character_entity_id\", \"character\"],\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Use descriptor-validated vocab paths (from Phase 1)\n",
    "# ------------------------------------------------------------------\n",
    "vocab_files = [\n",
    "    (\"entities\", VOCAB_ENTITIES_PATH, ENTITY_COLS, True),\n",
    "    (\"aliases\", VOCAB_ALIASES_PATH, ALIAS_COLS, False),\n",
    "    (\"author_aliases\", VOCAB_AUTHORS_PATH, AUTHOR_ALIAS_COLS, False),\n",
    "    (\"pc_map\", VOCAB_PC_MAP_PATH, PC_MAP_COLS, False),\n",
    "]\n",
    "\n",
    "# Published outputs\n",
    "DF_VOCAB_ENTITIES = pd.DataFrame(columns=list(ENTITY_COLS.keys()))\n",
    "DF_VOCAB_ALIASES = pd.DataFrame(columns=list(ALIAS_COLS.keys()))\n",
    "DF_VOCAB_AUTHORS = pd.DataFrame(columns=list(AUTHOR_ALIAS_COLS.keys()))\n",
    "DF_VOCAB_PC_MAP = pd.DataFrame(columns=list(PC_MAP_COLS.keys()))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load + normalize (looped, inline)\n",
    "# ------------------------------------------------------------------\n",
    "for key, path_obj, col_map, required in vocab_files:\n",
    "\n",
    "    if not path_obj:\n",
    "        if required:\n",
    "            errors.append(f\"Missing required path for {key} in descriptor.\")\n",
    "        continue\n",
    "\n",
    "    p = Path(path_obj)\n",
    "\n",
    "    if required and not p.exists():\n",
    "        errors.append(f\"Missing required vocabulary file:\\n  {p}\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"Optional vocab file not found: {p}\")\n",
    "        continue\n",
    "\n",
    "    raw_df = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "\n",
    "    rename = {}\n",
    "    for semantic, options in col_map.items():\n",
    "        found = next((c for c in options if c in raw_df.columns), None)\n",
    "        if found:\n",
    "            rename[found] = semantic\n",
    "\n",
    "    if len(raw_df) > 0 and not rename:\n",
    "        warnings.append(\n",
    "            f\"[{key}] CSV has rows but none of the expected columns were found.\\n\"\n",
    "            f\"  CSV columns: {list(raw_df.columns)}\\n\"\n",
    "            f\"  Expected mapping: {col_map}\\n\"\n",
    "            f\"  File: {p}\"\n",
    "        )\n",
    "        norm_df = pd.DataFrame(columns=list(col_map.keys()))\n",
    "    else:\n",
    "        out = raw_df.rename(columns=rename)\n",
    "        keep = [k for k in col_map.keys() if k in out.columns]\n",
    "        norm_df = out[keep].copy()\n",
    "\n",
    "    if key == \"entities\":\n",
    "        DF_VOCAB_ENTITIES = norm_df\n",
    "    elif key == \"aliases\":\n",
    "        DF_VOCAB_ALIASES = norm_df\n",
    "    elif key == \"author_aliases\":\n",
    "        DF_VOCAB_AUTHORS = norm_df\n",
    "    elif key == \"pc_map\":\n",
    "        DF_VOCAB_PC_MAP = norm_df\n",
    "\n",
    "    del raw_df, rename, semantic, options, found, out, keep, norm_df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Hard validation: entities must be usable\n",
    "# ------------------------------------------------------------------\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase 3 cannot proceed.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nFix the descriptor or vocabulary files, then rerun Phase 3.\"\n",
    "    )\n",
    "\n",
    "if DF_VOCAB_ENTITIES.empty:\n",
    "    raise ValueError(\n",
    "        \"Entities vocab file loaded but no usable rows were found.\\n\"\n",
    "        \"Ensure the CSV contains entity_id and canonical columns.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build DF_VOCAB_LOOKUP (unified vocab table for remapping)\n",
    "# Columns:\n",
    "#   - vocab_id: entity_id or player_entity_id\n",
    "#   - vocab: canonical / alias / author handle\n",
    "#   - vocab_kind: \"entity\" | \"alias\" | \"author\"\n",
    "#   - vocab_norm: lowercase normalized vocab for matching\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "# Entities (canonical names)\n",
    "for _, r in DF_VOCAB_ENTITIES.iterrows():\n",
    "    vid = str(r.get(\"entity_id\", \"\")).strip()\n",
    "    v = str(r.get(\"canonical\", \"\")).strip()\n",
    "    if vid and v:\n",
    "        rows.append([vid, v, \"entity\"])\n",
    "\n",
    "# Aliases (optional)\n",
    "if DF_VOCAB_ALIASES is not None and not DF_VOCAB_ALIASES.empty:\n",
    "    for _, r in DF_VOCAB_ALIASES.iterrows():\n",
    "        vid = str(r.get(\"entity_id\", \"\")).strip()\n",
    "        v = str(r.get(\"alias\", \"\")).strip()\n",
    "        if vid and v:\n",
    "            rows.append([vid, v, \"alias\"])\n",
    "\n",
    "# Author handles (optional)\n",
    "if DF_VOCAB_AUTHORS is not None and not DF_VOCAB_AUTHORS.empty:\n",
    "    for _, r in DF_VOCAB_AUTHORS.iterrows():\n",
    "        vid = str(r.get(\"player_entity_id\", \"\")).strip()\n",
    "        v = str(r.get(\"author\", \"\")).strip()\n",
    "        if vid and v:\n",
    "            rows.append([vid, v, \"author\"])\n",
    "\n",
    "DF_VOCAB_LOOKUP = pd.DataFrame(rows, columns=[\"vocab_id\", \"vocab\", \"vocab_kind\"])\n",
    "DF_VOCAB_LOOKUP[\"vocab_norm\"] = DF_VOCAB_LOOKUP[\"vocab\"].astype(str).str.strip().str.lower()\n",
    "DF_VOCAB_LOOKUP = DF_VOCAB_LOOKUP.drop_duplicates(\n",
    "    subset=[\"vocab_id\", \"vocab_norm\", \"vocab_kind\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "del rows, r, vid, v\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Summary\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Phase 3 OK: vocabulary tables loaded.\")\n",
    "\n",
    "print(\"\\nLoaded vocab tables:\")\n",
    "print(f\"- DF_VOCAB_ENTITIES: {len(DF_VOCAB_ENTITIES):>8} rows, {len(DF_VOCAB_ENTITIES.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_ALIASES:  {len(DF_VOCAB_ALIASES):>8} rows, {len(DF_VOCAB_ALIASES.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_AUTHORS:  {len(DF_VOCAB_AUTHORS):>8} rows, {len(DF_VOCAB_AUTHORS.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_PC_MAP:   {len(DF_VOCAB_PC_MAP):>8} rows, {len(DF_VOCAB_PC_MAP.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_LOOKUP:   {len(DF_VOCAB_LOOKUP):>8} rows, {len(DF_VOCAB_LOOKUP.columns):>3} cols\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del pd, Path\n",
    "del errors, warnings, vocab_files, key, path_obj, col_map, required, p\n",
    "del ENTITY_COLS, ALIAS_COLS, AUTHOR_ALIAS_COLS, PC_MAP_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae42e691-a75d-4bc0-bef6-2695e7a1c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up VOCAB path variables\n",
    "del VOCAB_ENTITIES_PATH, VOCAB_ENTITIES_RELPATH\n",
    "del VOCAB_ALIASES_PATH, VOCAB_ALIASES_RELPATH\n",
    "del VOCAB_AUTHORS_PATH, VOCAB_AUTHORS_RELPATH\n",
    "del VOCAB_PC_MAP_PATH, VOCAB_PC_MAP_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7baa54-c6fc-40ba-bbe1-a82f3a00f8de",
   "metadata": {},
   "source": [
    "# Graph Bootstrap (Build graph artifacts)\n",
    "\n",
    "This section constructs graph-ready node and edge tables using the loaded indexes.\n",
    "\n",
    "Outputs will be written under `indexes.path`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef964eb-10f6-4589-8946-ae00c7e62d75",
   "metadata": {},
   "source": [
    "## Build graph nodes (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffc30d57-2672-44fe-bacf-d1ff3cd597fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph nodes built.\n",
      "Total nodes: 1696\n",
      "\n",
      "Counts by node_type:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chunk</th>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vocab</th>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto_transcripts</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faction</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>player</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planning_notes</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pbp_transcripts</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_notes</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creat</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artifact</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count\n",
       "node_type              \n",
       "chunk              1139\n",
       "vocab               251\n",
       "auto_transcripts    105\n",
       "person              104\n",
       "place                27\n",
       "faction              14\n",
       "player               12\n",
       "planning_notes       11\n",
       "org                  10\n",
       "pbp_transcripts       9\n",
       "session_notes         5\n",
       "creat                 5\n",
       "artifact              3\n",
       "ref                   1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample nodes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>node_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>chunk_168746</td>\n",
       "      <td>chunk</td>\n",
       "      <td>chunk_168746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>place_cameth</td>\n",
       "      <td>place</td>\n",
       "      <td>Cameth Brin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>chunk_169849</td>\n",
       "      <td>chunk</td>\n",
       "      <td>chunk_169849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>chunk_169098</td>\n",
       "      <td>chunk</td>\n",
       "      <td>chunk_169098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>vocab:tolanite</td>\n",
       "      <td>vocab</td>\n",
       "      <td>Tolanite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             node_id node_type         label\n",
       "179     chunk_168746     chunk  chunk_168746\n",
       "1394    place_cameth     place   Cameth Brin\n",
       "1028    chunk_169849     chunk  chunk_169849\n",
       "458     chunk_169098     chunk  chunk_169098\n",
       "1670  vocab:tolanite     vocab      Tolanite"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build graph nodes (CSV-only; no networkx here)\n",
    "LAST_PHASE_RUN = \"nodes\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Nodes: start empty, append sources, then finalize\n",
    "# -------------------------------------------------------------------\n",
    "nodes = []\n",
    "\n",
    "# 1) Entity nodes (from vocab entities)\n",
    "# node_type = prefix of entity_id (before first \"_\"), e.g. \"artifact_folly\" -> \"artifact\"\n",
    "nodes.append(\n",
    "    DF_VOCAB_ENTITIES.assign(\n",
    "        node_id=lambda d: d[\"entity_id\"].astype(str),\n",
    "        node_type=lambda d: d[\"entity_id\"].astype(str).str.split(\"_\", n=1).str[0],\n",
    "        label=lambda d: d[\"canonical\"].astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Chunk nodes (from chunk index)\n",
    "nodes.append(\n",
    "    DF_CHUNK_TO_ENTITIES.assign(\n",
    "        node_id=lambda d: \"chunk_\" + d[\"chunk_id\"].astype(int).astype(str),\n",
    "        node_type=\"chunk\",\n",
    "        label=lambda d: \"chunk_\" + d[\"chunk_id\"].astype(int).astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# 3) File nodes (from source files)\n",
    "# node_type = source_type (pbp_transcripts, session_notes, etc.)\n",
    "nodes.append(\n",
    "    DF_SOURCE_FILES.assign(\n",
    "        node_id=lambda d: \"file:\" + d[\"relpath\"].astype(str),\n",
    "        node_type=lambda d: d[\"source_type\"].astype(str),\n",
    "        label=lambda d: d[\"relpath\"].astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# 4) Vocab nodes (from consolidated vocab lookup)\n",
    "# node_id = stable text-form node keyed by vocab_norm\n",
    "# label   = original vocab string (human-readable)\n",
    "nodes.append(\n",
    "    DF_VOCAB_LOOKUP.assign(\n",
    "        node_id=lambda d: \"vocab:\" + d[\"vocab_norm\"].astype(str),\n",
    "        node_type=\"vocab\",\n",
    "        label=lambda d: d[\"vocab\"].astype(str),\n",
    "    ).loc[:, [\"node_id\", \"node_type\", \"label\"]]\n",
    ")\n",
    "\n",
    "# 5) Finalize\n",
    "DF_GRAPH_NODES = (\n",
    "    pd.concat(nodes, ignore_index=True)\n",
    "      .drop_duplicates(subset=[\"node_id\"])\n",
    "      .sort_values([\"node_type\", \"node_id\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sanity check (compact but useful)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Graph nodes built.\")\n",
    "print(f\"Total nodes: {len(DF_GRAPH_NODES)}\")\n",
    "print(\"\\nCounts by node_type:\")\n",
    "display(DF_GRAPH_NODES[\"node_type\"].value_counts().to_frame(\"count\"))\n",
    "\n",
    "print(\"\\nSample nodes:\")\n",
    "display(DF_GRAPH_NODES.sample(min(5, len(DF_GRAPH_NODES)), random_state=7))\n",
    "\n",
    "# cleanup locals (keep DF_GRAPH_NODES)\n",
    "del nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42798002-4c60-412e-94e1-9f46a0d507b4",
   "metadata": {},
   "source": [
    "## Build graph edges (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "951abf8c-297e-46aa-a58f-f78492cf2401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph edges built (v0)\n",
      "Edges: 12691\n",
      "Edge counts by predicate:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicate</th>\n",
       "      <th>edge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cooccurs_with</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mentions</td>\n",
       "      <td>5362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contains</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>refers_to</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plays</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicate  edge_count\n",
       "0  cooccurs_with        5895\n",
       "1       mentions        5362\n",
       "2       contains        1139\n",
       "3      refers_to         253\n",
       "4          plays          42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>faction_modrons</td>\n",
       "      <td>cooccurs_with</td>\n",
       "      <td>person_melba</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>person_henry</td>\n",
       "      <td>cooccurs_with</td>\n",
       "      <td>person_tanzo</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>chunk_169782</td>\n",
       "      <td>mentions</td>\n",
       "      <td>vocab:Faeryne</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subject      predicate         object  weight\n",
       "2100   faction_modrons  cooccurs_with   person_melba     2.0\n",
       "5474      person_henry  cooccurs_with   person_tanzo    88.0\n",
       "10312     chunk_169782       mentions  vocab:Faeryne     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Phase: Graph edges (v0)\n",
    "# -------------------------------------------------------------------\n",
    "LAST_PHASE_RUN = \"E1\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Relationships sourced from DF_CHUNK_TO_ENTITIES\n",
    "# -------------------------------------------------------------------\n",
    "# File contains Chunk:\n",
    "#   Source grammar (index table):\n",
    "#     relpath + chunk_id  => \"This chunk is located in this file\"\n",
    "#   Target grammar (graph edges):\n",
    "#     subject + predicate + object  => \"file:<relpath> contains chunk_<id>\"\n",
    "#\n",
    "# Chunk mentions Vocab:\n",
    "#   Source grammar (index table):\n",
    "#     chunk_id + matched_vocabs  => \"This chunk contains these text forms\"\n",
    "#   Target grammar (graph edges):\n",
    "#     subject + predicate + object  => \"chunk_<id> mentions vocab:<text>\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_CHUNK_TO_ENTITIES.loc[:, [\"chunk_id\", \"relpath\", \"matched_vocabs\", \"entity_ids\"]].iterrows():\n",
    "    chunk_node = f\"chunk_{int(r['chunk_id'])}\"\n",
    "    file_node = f\"file:{r['relpath']}\"\n",
    "\n",
    "    # file contains chunk\n",
    "    rows.append((file_node, \"contains\", chunk_node, pd.NA))\n",
    "\n",
    "    # chunk mentions vocab (pipe-delimited)\n",
    "    for v in (x.strip() for x in str(r[\"matched_vocabs\"]).split(\"|\")):\n",
    "        rows.append((chunk_node, \"mentions\", f\"vocab:{v}\", pd.NA))\n",
    "\n",
    "    # entity co-occurs with entity (within this chunk) -> \"votes\" (weight=1)\n",
    "    # chunk_id + entity_ids => \"<entity_id A> cooccurs_with <entity_id B>\" (undirected via A < B)\n",
    "    entity_ids = sorted({e.strip() for e in str(r[\"entity_ids\"]).split(\"|\") if e.strip()})\n",
    "    for i in range(len(entity_ids)):\n",
    "        for j in range(i + 1, len(entity_ids)):\n",
    "            rows.append((entity_ids[i], \"cooccurs_with\", entity_ids[j], 1))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Vocab refers_to Entity/Player\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (vocab lookup table):\n",
    "#   vocab_norm + vocab_id  => \"This text form refers to this thing\"\n",
    "#\n",
    "# Target grammar (graph edges):\n",
    "#   subject + predicate + object  => \"vocab:<vocab_norm> refers_to <vocab_id>\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_VOCAB_LOOKUP.loc[:, [\"vocab_norm\", \"vocab_id\"]].iterrows():\n",
    "\n",
    "    # Translate vocab_norm (\"shadowboy\") -> graph node id (\"vocab:shadowboy\")\n",
    "    subject = f\"vocab:{str(r['vocab_norm']).strip()}\"\n",
    "\n",
    "    # vocab_id is already the target node id (entity_id or player_entity_id)\n",
    "    object_ = str(r[\"vocab_id\"]).strip()\n",
    "\n",
    "    rows.append((subject, \"refers_to\", object_, pd.NA))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Player plays Character\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (vocab table):\n",
    "#    char_entity_id + player_entity_id  => \"Character is played by Player\"\n",
    "#\n",
    "# Target grammar (graph edges):\n",
    "#   subject + predicate + object  => \"<player_entity_id> plays <char_entity_id>\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_VOCAB_PC_MAP.loc[:, [\"player_entity_id\", \"char_entity_id\"]].iterrows():\n",
    "    subject = str(r[\"player_entity_id\"]).strip()\n",
    "    object_ = str(r[\"char_entity_id\"]).strip()\n",
    "    rows.append((subject, \"plays\", object_, pd.NA))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Entity co-occurs with Entity (within same chunk)\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (index table):\n",
    "#   chunk_id + entity_ids\n",
    "#     => \"These entities appear together in this chunk\"\n",
    "#\n",
    "# Target grammar (graph edges):\n",
    "#   subject + predicate + object\n",
    "#     => \"<entity_id_A> cooccurs_with <entity_id_B>\"\n",
    "#\n",
    "# Convention:\n",
    "#   Alphabetical ordering ensures one undirected edge per pair.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for _, r in DF_CHUNK_TO_ENTITIES.loc[:, [\"entity_ids\"]].iterrows():\n",
    "\n",
    "    # Extract clean list of entity_ids in this chunk\n",
    "    entities = sorted(\n",
    "        e.strip()\n",
    "        for e in str(r[\"entity_ids\"]).split(\"|\")\n",
    "        if e.strip()\n",
    "    )\n",
    "\n",
    "    # Build all unordered pairs (i < j ensures no duplicates)\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            subject = entities[i]\n",
    "            object_ = entities[j]\n",
    "            rows.append((subject, \"cooccurs_with\", object_, 1))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Aggregate and build the dataframe\n",
    "# -------------------------------------------------------------------\n",
    "DF_GRAPH_EDGES = (\n",
    "    pd.DataFrame(rows, columns=[\"subject\", \"predicate\", \"object\", \"weight\"])\n",
    "      .groupby([\"subject\", \"predicate\", \"object\"], as_index=False)\n",
    "      .agg(weight=(\"weight\", lambda s: s.sum(min_count=1)))\n",
    "      .sort_values([\"predicate\", \"subject\", \"object\"], ascending=[True, True, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sanity check\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Graph edges built (v0)\")\n",
    "print(f\"Edges: {len(DF_GRAPH_EDGES)}\")\n",
    "print(\"Edge counts by predicate:\\n\")\n",
    "\n",
    "display(\n",
    "    DF_GRAPH_EDGES\n",
    "        .groupby(\"predicate\", as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"edge_count\"})\n",
    "        .sort_values(\"edge_count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(DF_GRAPH_EDGES.sample(3))\n",
    "\n",
    "# clean up locals\n",
    "del rows, r, v, i, j, chunk_node, file_node, subject, object_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88f4e7-b6ce-4247-85cb-29ce278dcc85",
   "metadata": {},
   "source": [
    "## Phase 7: Write artifacts + summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0d1967c-e683-4fc8-838a-bd9dc42ca3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del DF_GRAPH_EDGES_COOCCURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090eab0d-ecce-4a5a-9f7d-3ca6b8b00aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
