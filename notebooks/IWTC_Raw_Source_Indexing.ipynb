{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c029da0e-6f8e-4eb4-a044-0a9a8ff10a5f",
   "metadata": {},
   "source": [
    "# IWTC Raw Source Indexing\n",
    "\n",
    "This notebook executes the raw source indexing workflow defined in:\n",
    "\n",
    "- `docs/raw_source_indexing_design.md`\n",
    "\n",
    "It is intended for hands-on execution and experimentation. Conceptual scope, responsibilities, and workflow design are defined in the linked design document.\n",
    "\n",
    "This notebook operates on a single world repository.\n",
    "\n",
    "A minimal example of `world_repository.yml` is provided in this repository\n",
    "under:\n",
    "\n",
    "- `data/config_examples/world_repository.yml`\n",
    "\n",
    "You may copy and adapt that example for your own world repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2058b0-9c50-411e-bdfd-eb5f63600c06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 0: Parameters\n",
    "\n",
    "This notebook operates on a **campaign world repository** and produces draft, machine-generated indexes for human review.\n",
    "\n",
    "In this phase, you tell the notebook **which world it is operating on** and **how broad this run should be**.\n",
    "\n",
    "At a high level:\n",
    "- You point the notebook at a world descriptor file that explains how your world’s files are organized.\n",
    "- You can optionally restrict this run to specific files or folders if you are working on a subset of material.\n",
    "- You choose whether to review discovered files interactively or process everything automatically.\n",
    "\n",
    "You do **not** need to understand internal data structures or file parsing to set these parameters.  \n",
    "The goal is simply to answer: *“What world am I indexing, and how much of it do I want to work on right now?”*\n",
    "\n",
    "The code cell below contains inline comments explaining each parameter in concrete terms.\n",
    "\n",
    "**IMPORTANT:** Resulting indexes are, by design, not auditable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b467c2b-04d7-46e3-8a00-e0bd37f493b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run initialized at: 2026-02-10 17:13\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Parameters\n",
    "LAST_PHASE_RUN = \"0\"\n",
    "\n",
    "# Absolute path to the world_repository.yml descriptor.\n",
    "WORLD_REPOSITORY_DESCRIPTOR = (\n",
    "    \"/Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/descriptors/world_repository.yml\"\n",
    ")\n",
    "\n",
    "# Optional override: use these paths instead of descriptor sources for this run.\n",
    "# Examples:\n",
    "#   OVERRIDE_PATHS = \"/Users/you/path/to/file_or_dir\"\n",
    "#   OVERRIDE_PATHS = [\"/Users/you/path/a\", \"/Users/you/path/b\"]\n",
    "OVERRIDE_PATHS = None\n",
    "\n",
    "# Selection behavior:\n",
    "#   \"PROMPT\" -> list candidates and prompt for selection\n",
    "#   \"ALL\"    -> select all candidates\n",
    "SOURCE_MODE = \"ALL\"\n",
    "\n",
    "# Internal run metadata (do not edit)\n",
    "from datetime import datetime\n",
    "print(f\"Notebook run initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "del datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d6408-b9bd-4cf0-a5b3-1cf8c41d2aa1",
   "metadata": {},
   "source": [
    "## Phase 1: Load and validate world descriptor\n",
    "\n",
    "Before this notebook can safely read or write anything, it must be confident that it understands the **structure of the world repository**.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Loads the world repository descriptor file you provided\n",
    "- Confirms that it is readable and structurally valid\n",
    "- Extracts only the information this notebook needs\n",
    "- Verifies that referenced paths actually exist and are usable\n",
    "\n",
    "This phase answers a single question:\n",
    "\n",
    "**“Can I trust this descriptor enough to proceed?”**\n",
    "\n",
    "If the answer is *no*, the notebook will stop with clear, actionable error messages explaining what needs to be fixed in the descriptor file.  \n",
    "Nothing is modified, created, or scanned until this check succeeds.\n",
    "\n",
    "This phase does **not** interpret world lore, indexing rules, or heuristics.  \n",
    "It only establishes that the filesystem layout described by the world is coherent and usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b7319b2-ed33-4c54-bf08-d4c17137ed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World repository descriptor loaded successfully: world_repository.yml\n"
     ]
    }
   ],
   "source": [
    "# Phase 1a: Load and parse world repository descriptor\n",
    "LAST_PHASE_RUN = \"1a\"\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Locate descriptor file\n",
    "descriptor_path = Path(WORLD_REPOSITORY_DESCRIPTOR)\n",
    "\n",
    "if not descriptor_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"World repository descriptor file was not found.\\n\"\n",
    "        f\"Path provided:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Confirm the file exists at this location or fix the Parameters cell\\n\"\n",
    "        \"- If you just edited the Parameters cell, rerun Phase 0 and then rerun this cell\\n\"\n",
    "    )\n",
    "\n",
    "# Read and parse YAML\n",
    "try:\n",
    "    with descriptor_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        world_repo = yaml.safe_load(f)\n",
    "except Exception:\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor could not be read.\\n\"\n",
    "        \"This usually indicates a YAML formatting problem.\\n\\n\"\n",
    "        f\"File:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Paste the contents into https://www.yamllint.com/\\n\"\n",
    "        \"- Fix any reported issues, save the file, and rerun this cell\"\n",
    "    )\n",
    "\n",
    "# Validate basic structure\n",
    "if not isinstance(world_repo, dict):\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor was read, but its structure is not usable.\\n\"\n",
    "        \"The file must be a YAML mapping (top-level `name: value` entries).\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Ensure it uses clear `name: value` lines\\n\"\n",
    "        \"- Fix the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "print(f\"World repository descriptor loaded successfully: {descriptor_path.name}\")\n",
    "\n",
    "# cleanup: remove local variables\n",
    "del f, yaml, descriptor_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c501c1bd-20ad-48ad-b99a-9e845794408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1b OK: required entries present and parameters are coherent.\n",
      "SOURCE_ORIGIN: descriptor\n",
      "PATHS_RAW entries: 15\n"
     ]
    }
   ],
   "source": [
    "# Phase 1b: Extract only the information this notebook needs (presence + run intent)\n",
    "LAST_PHASE_RUN = \"1b\"\n",
    "\n",
    "errors = []\n",
    "\n",
    "# ---- OVERRIDE_PATHS normalization (shape only) ----\n",
    "override_list = None\n",
    "\n",
    "if OVERRIDE_PATHS:\n",
    "    if isinstance(OVERRIDE_PATHS, str):\n",
    "        override_list = [OVERRIDE_PATHS]\n",
    "    elif isinstance(OVERRIDE_PATHS, list):\n",
    "        bad = [x for x in OVERRIDE_PATHS if not isinstance(x, str) or not x]\n",
    "        if bad:\n",
    "            errors.append(\"OVERRIDE_PATHS must be a path string or a list of non-empty path strings.\")\n",
    "        else:\n",
    "            override_list = OVERRIDE_PATHS\n",
    "        del bad\n",
    "    else:\n",
    "        errors.append(\"OVERRIDE_PATHS must be None, a path string, or a list of path strings.\")\n",
    "\n",
    "SOURCE_ORIGIN = \"override_paths\" if override_list else \"descriptor\"\n",
    "\n",
    "# ---- extract descriptor blocks (presence only) ----\n",
    "WORLD_ROOT_RAW = world_repo.get(\"world_root\")\n",
    "\n",
    "read_paths = None\n",
    "sources = world_repo.get(\"sources\")\n",
    "if isinstance(sources, dict):\n",
    "    read_paths = sources.get(\"read_paths\")\n",
    "\n",
    "drafts = world_repo.get(\"working_drafts\")\n",
    "DRAFTS_RAW = drafts.get(\"path\") if isinstance(drafts, dict) else None\n",
    "\n",
    "indexes = world_repo.get(\"indexes\")  # optional\n",
    "INDEXES_RAW = indexes.get(\"path\") if isinstance(indexes, dict) else None\n",
    "\n",
    "vocab = world_repo.get(\"vocabulary\")\n",
    "ENTITIES_RAW = vocab.get(\"entities\") if isinstance(vocab, dict) else None\n",
    "ALIASES_RAW = vocab.get(\"aliases\") if isinstance(vocab, dict) else None\n",
    "AUTHORS_RAW = vocab.get(\"author_aliases\") if isinstance(vocab, dict) else None  # optional\n",
    "PC_MAP_RAW = vocab.get(\"player_character_map\") if isinstance(vocab, dict) else None  # optional\n",
    "\n",
    "# ---- required entries ----\n",
    "if not WORLD_ROOT_RAW:\n",
    "    errors.append(\"Missing required entry: world_root\")\n",
    "\n",
    "if not DRAFTS_RAW:\n",
    "    errors.append(\"Missing required entry: working_drafts.path\")\n",
    "\n",
    "# indexes optional (may not exist yet)\n",
    "# but if declared it must have a path\n",
    "if INDEXES_RAW is not None:\n",
    "    errors.append(\"indexes is declared but missing indexes.path\")\n",
    "\n",
    "# vocab files optional (may not exist on first run)\n",
    "# but paths must be present if declared\n",
    "if vocab is not None:\n",
    "    if ENTITIES_RAW is None:\n",
    "        errors.append(\"vocabulary.entities is declared but missing a path\")\n",
    "    if ALIASES_RAW is None:\n",
    "        errors.append(\"vocabulary.aliases is declared but missing a path\")\n",
    "\n",
    "# sources.read_paths required only when not overriding\n",
    "if SOURCE_ORIGIN == \"descriptor\":\n",
    "    if read_paths is None:\n",
    "        errors.append(\"Missing required entry: sources.read_paths\")\n",
    "    elif not isinstance(read_paths, list):\n",
    "        errors.append(\"sources.read_paths must be a YAML list\")\n",
    "else:\n",
    "    # override mode: read_paths optional, but if present must be a list\n",
    "    if read_paths is not None and not isinstance(read_paths, list):\n",
    "        errors.append(\"sources.read_paths must be a YAML list\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"World repository + parameters are missing required entries or have invalid values:\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Fix OVERRIDE_PATHS in Phase 0 (if set)\\n\"\n",
    "          \"- Edit your world_repository.yml\\n\"\n",
    "          \"- Save and rerun Phase 1a, then rerun this cell\\n\"\n",
    "          \"\\nNote: This check only confirms entries and shapes. Filesystem usability is validated in Phase 1c.\"\n",
    "    )\n",
    "\n",
    "# ---- build PATHS_RAW (flat list for Phase 1c validation) ----\n",
    "PATHS_RAW = [\n",
    "    {\"tag\": \"world_root\", \"raw\": WORLD_ROOT_RAW},\n",
    "    {\"tag\": \"drafts\",     \"raw\": DRAFTS_RAW},\n",
    "]\n",
    "\n",
    "if INDEXES_RAW:\n",
    "    PATHS_RAW.append({\"tag\": \"indexes\", \"raw\": INDEXES_RAW})\n",
    "\n",
    "if ENTITIES_RAW:\n",
    "    PATHS_RAW.append({\"tag\": \"entities\", \"raw\": ENTITIES_RAW})\n",
    "\n",
    "if ALIASES_RAW:\n",
    "    PATHS_RAW.append({\"tag\": \"aliases\", \"raw\": ALIASES_RAW})\n",
    "\n",
    "if AUTHORS_RAW:\n",
    "    PATHS_RAW.append({\"tag\": \"authors\", \"raw\": AUTHORS_RAW})\n",
    "\n",
    "if PC_MAP_RAW:\n",
    "    PATHS_RAW.append({\"tag\": \"pc_map\", \"raw\": PC_MAP_RAW})\n",
    "\n",
    "# run source paths\n",
    "if SOURCE_ORIGIN == \"override_paths\":\n",
    "    for p in override_list:\n",
    "        PATHS_RAW.append({\"tag\": \"src\", \"raw\": p})\n",
    "else:\n",
    "    for entry in read_paths:\n",
    "        if isinstance(entry, str):\n",
    "            PATHS_RAW.append({\"tag\": \"src\", \"raw\": entry})\n",
    "        elif isinstance(entry, dict):\n",
    "            p = entry.get(\"path\")\n",
    "            if not p:\n",
    "                raise ValueError(\"sources.read_paths contains a mapping entry missing 'path'.\")\n",
    "            PATHS_RAW.append({\"tag\": \"src\", \"raw\": p})\n",
    "            del p\n",
    "        else:\n",
    "            raise ValueError(\"sources.read_paths entries must be a path string or a {path,type} mapping.\")\n",
    "\n",
    "# source types (descriptor-declared typing only)\n",
    "if isinstance(read_paths, list):\n",
    "    for entry in read_paths:\n",
    "        if isinstance(entry, dict):\n",
    "            p = entry.get(\"path\")\n",
    "            if not p:\n",
    "                raise ValueError(\"sources.read_paths contains a mapping entry missing 'path'.\")\n",
    "            PATHS_RAW.append({\n",
    "                \"tag\": \"src_type\",\n",
    "                \"raw\": p,\n",
    "                \"source_type\": entry.get(\"type\") or \"unknown\",\n",
    "            })\n",
    "            del p\n",
    "\n",
    "print(\"Phase 1b OK: required entries present and parameters are coherent.\")\n",
    "print(f\"SOURCE_ORIGIN: {SOURCE_ORIGIN}\")\n",
    "print(f\"PATHS_RAW entries: {len(PATHS_RAW)}\")\n",
    "\n",
    "# ---- clean up locals ----\n",
    "del errors, override_list, sources, drafts, indexes, vocab, read_paths, WORLD_ROOT_RAW, entry, world_repo\n",
    "del DRAFTS_RAW, INDEXES_RAW, ENTITIES_RAW, ALIASES_RAW, AUTHORS_RAW, PC_MAP_RAW, OVERRIDE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b8db18-1c50-4eaa-ae5c-665210324ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor paths are usable for this notebook.\n",
      "world_root: /Users/charissophia/obsidian/Iron Wolf Trading Company\n",
      "working_drafts: _local/machine_wip\n",
      "indexes: none\n",
      "vocab.entities: _meta/indexes/vocab_entities.csv (exists=True)\n",
      "vocab.aliases: _meta/indexes/vocab_aliases.csv (exists=True)\n",
      "vocab.authors: _meta/indexes/vocab_author_aliases.csv (exists=True)\n",
      "vocab.pc_map: _meta/indexes/vocab_map_player_character.csv (exists=True)\n",
      "typed source paths: 4\n",
      "run source paths: 5\n"
     ]
    }
   ],
   "source": [
    "# Phase 1c: Validate paths and publish contract variables (simple, no RUN_SOURCE_DIRS)\n",
    "LAST_PHASE_RUN = \"1c\"\n",
    "\n",
    "errors = []\n",
    "\n",
    "# Published outputs (initialized up front)\n",
    "WORLD_ROOT = None\n",
    "WORKING_DRAFTS_PATH = None\n",
    "WORKING_DRAFTS_RELPATH = None\n",
    "\n",
    "INDEXES_PATH = None\n",
    "INDEXES_RELPATH = None\n",
    "\n",
    "VOCAB_ENTITIES_PATH = None\n",
    "VOCAB_ENTITIES_RELPATH = None\n",
    "VOCAB_ALIASES_PATH = None\n",
    "VOCAB_ALIASES_RELPATH = None\n",
    "VOCAB_AUTHORS_PATH = None\n",
    "VOCAB_AUTHORS_RELPATH = None\n",
    "VOCAB_PC_MAP_PATH = None\n",
    "VOCAB_PC_MAP_RELPATH = None\n",
    "\n",
    "SOURCE_TYPES_MAP = {}   # resolved Path (file or dir) -> source_type\n",
    "RUN_SOURCE_PATHS = []   # list[Path] (files or dirs)\n",
    "\n",
    "# world_root (1b guarantees presence/uniqueness)\n",
    "WORLD_ROOT = Path(next(x[\"raw\"] for x in PATHS_RAW if x.get(\"tag\") == \"world_root\"))\n",
    "\n",
    "if str(WORLD_ROOT).startswith(\"~\"):\n",
    "    errors.append(\"world_root: '~' is not allowed. Use a full absolute path.\")\n",
    "elif not WORLD_ROOT.is_absolute():\n",
    "    errors.append(\"world_root must be an absolute path (starts with / on macOS/Linux, or C:\\\\ on Windows).\")\n",
    "elif not WORLD_ROOT.is_dir():\n",
    "    errors.append(f\"world_root must be an existing directory: {WORLD_ROOT}\")\n",
    "else:\n",
    "    WORLD_ROOT = WORLD_ROOT.resolve()\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "for item in PATHS_RAW:\n",
    "    tag = item.get(\"tag\")\n",
    "\n",
    "    if tag != \"world_root\" and tag in (\"drafts\", \"src\", \"src_type\", \"indexes\", \"entities\", \"aliases\", \"authors\", \"pc_map\"):\n",
    "        raw = item.get(\"raw\")\n",
    "        \n",
    "        if not raw:\n",
    "            errors.append(f\"{tag}: missing path value.\")\n",
    "        \n",
    "        else:\n",
    "            p = Path(raw)\n",
    "            rel = None\n",
    "        \n",
    "            if str(p).startswith(\"~\"):\n",
    "                errors.append(f\"{tag}: '~' is not allowed: {raw}\")\n",
    "            else:\n",
    "                if not p.is_absolute():\n",
    "                    p = WORLD_ROOT / p\n",
    "                p = p.resolve()\n",
    "\n",
    "                try:\n",
    "                    rel = str(p.relative_to(WORLD_ROOT))\n",
    "                except Exception:\n",
    "                    rel = str(p)\n",
    "\n",
    "            # Required existence\n",
    "            if tag in (\"drafts\", \"src\") and not p.exists():\n",
    "                errors.append(f\"{tag}: path does not exist: {p}\")\n",
    "\n",
    "            # If it's a directory but tag requires file\n",
    "            if p.exists() and p.is_dir() and tag in (\"entities\", \"aliases\", \"authors\", \"pc_map\"):\n",
    "                errors.append(f\"{tag}: {p} must be a file\")\n",
    "\n",
    "            # If it's a file but tag requires directory\n",
    "            if p.exists() and p.is_file() and tag in (\"drafts\", \"indexes\", \"src_type\"):\n",
    "                errors.append(f\"{tag}: {p} must be a directory\")\n",
    "\n",
    "            # Publish / collect\n",
    "            if tag == \"drafts\" and p.exists() and p.is_dir():\n",
    "                WORKING_DRAFTS_PATH = p\n",
    "                WORKING_DRAFTS_RELPATH = rel\n",
    "\n",
    "            elif tag == \"indexes\" and p.exists() and p.is_dir():\n",
    "                INDEXES_PATH = p\n",
    "                INDEXES_RELPATH = rel\n",
    "\n",
    "            elif tag == \"src\" and p.exists():\n",
    "                RUN_SOURCE_PATHS.append(p)\n",
    "\n",
    "            elif tag == \"src_type\" and p.exists():\n",
    "                SOURCE_TYPES_MAP[p] = item.get(\"source_type\") or \"unknown\"\n",
    "\n",
    "            elif tag == \"entities\" and (not p.exists() or p.is_file()):\n",
    "                VOCAB_ENTITIES_PATH = p\n",
    "                VOCAB_ENTITIES_RELPATH = rel\n",
    "            \n",
    "            elif tag == \"aliases\" and (not p.exists() or p.is_file()):\n",
    "                VOCAB_ALIASES_PATH = p\n",
    "                VOCAB_ALIASES_RELPATH = rel\n",
    "            \n",
    "            elif tag == \"authors\" and (not p.exists() or p.is_file()):\n",
    "                VOCAB_AUTHORS_PATH = p\n",
    "                VOCAB_AUTHORS_RELPATH = rel\n",
    "            \n",
    "            elif tag == \"pc_map\" and (not p.exists() or p.is_file()):\n",
    "                VOCAB_PC_MAP_PATH = p\n",
    "                VOCAB_PC_MAP_RELPATH = rel\n",
    "\n",
    "if WORKING_DRAFTS_PATH is None:\n",
    "    errors.append(\"drafts: required working drafts directory was not validated (missing or invalid).\")\n",
    "\n",
    "if len(RUN_SOURCE_PATHS) == 0:\n",
    "    errors.append(\"src: no valid source paths were provided for this run.\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"Descriptor path validation failed:\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + f\"\\n\\nFix entries in: {Path(WORLD_REPOSITORY_DESCRIPTOR).name}\\n\"\n",
    "          \"Then rerun Phase 1a, Phase 1b, and this cell.\"\n",
    "    )\n",
    "\n",
    "# drafts write probe\n",
    "probe = WORKING_DRAFTS_PATH / \".iwtc_tools_write_probe.tmp\"\n",
    "try:\n",
    "    probe.write_text(\"test\", encoding=\"utf-8\")\n",
    "finally:\n",
    "    if probe.exists():\n",
    "        probe.unlink()\n",
    "\n",
    "print(\"Descriptor paths are usable for this notebook.\")\n",
    "print(f\"world_root: {WORLD_ROOT}\")\n",
    "print(f\"working_drafts: {WORKING_DRAFTS_RELPATH}\")\n",
    "print(f\"indexes: {INDEXES_RELPATH if INDEXES_RELPATH else 'none'}\")\n",
    "\n",
    "print(f\"vocab.entities: {VOCAB_ENTITIES_RELPATH} (exists={VOCAB_ENTITIES_PATH.exists() if VOCAB_ENTITIES_PATH else False})\")\n",
    "print(f\"vocab.aliases: {VOCAB_ALIASES_RELPATH} (exists={VOCAB_ALIASES_PATH.exists() if VOCAB_ALIASES_PATH else False})\")\n",
    "print(f\"vocab.authors: {VOCAB_AUTHORS_RELPATH} (exists={VOCAB_AUTHORS_PATH.exists() if VOCAB_AUTHORS_PATH else False})\")\n",
    "print(f\"vocab.pc_map: {VOCAB_PC_MAP_RELPATH} (exists={VOCAB_PC_MAP_PATH.exists() if VOCAB_PC_MAP_PATH else False})\")\n",
    "print(f\"typed source paths: {len(SOURCE_TYPES_MAP)}\")\n",
    "print(f\"run source paths: {len(RUN_SOURCE_PATHS)}\")\n",
    "\n",
    "# clean up locals\n",
    "del errors, item, tag, raw, p, probe, PATHS_RAW\n",
    "del Path, rel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7758c00-3037-44b5-98a4-2dd984ca62a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 2: Discover source files\n",
    "\n",
    "Before this notebook can index or analyze anything, it must determine **which files are available to work with**.\n",
    "\n",
    "In this phase, the notebook:\n",
    "- Determines which source locations to use (either override paths you provided, or the repository’s declared sources)\n",
    "- Recursively scans those locations for supported file types\n",
    "- Groups discovered files by directory for human-readable review\n",
    "- Establishes a stable ordering\n",
    "- Associates each file with its declared source type (if available)\n",
    "\n",
    "This phase answers a single question:\n",
    "\n",
    "**“What source files are available for processing right now?”**\n",
    "\n",
    "If no supported files are found, the notebook will stop and explain why.  \n",
    "Nothing is read, modified, or written during discovery.\n",
    "\n",
    "Depending on your configuration:\n",
    "- If SOURCE_MODE is \"PROMPT\", you will be prompted to choose which files to process\n",
    "- Otherwise, all discovered files will be selected without prompting\n",
    "\n",
    "This phase does **not** read file contents, interpret text, or apply chunking rules.  \n",
    "It only establishes the complete, concrete list of candidate files that later phases may operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dce55e5-1749-4380-8780-af7fb9e4f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate files by source_type:\n",
      "   105  auto_transcripts\n",
      "    11  planning_notes\n",
      "     9  pbp_transcripts\n",
      "     5  session_notes\n",
      "\n",
      "Phase 2 OK: selected 130 source files (of 130 candidates).\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Expand RUN_SOURCE_PATHS into concrete files + resolve source_type\n",
    "LAST_PHASE_RUN = \"2\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Output\n",
    "SOURCE_FILES = []\n",
    "\n",
    "# Eligible extensions (must match Phase 3 readers)\n",
    "ALLOWED_EXTS = {\".md\", \".txt\", \".docx\", \".pdf\"}\n",
    "\n",
    "# 2a) Expand dirs/files -> candidate file paths\n",
    "candidates = []\n",
    "\n",
    "for p in RUN_SOURCE_PATHS:\n",
    "    if p.is_file():\n",
    "        if (p.suffix.lower() in ALLOWED_EXTS) and (not p.name.startswith(\".\")):\n",
    "            candidates.append(p)\n",
    "\n",
    "    if p.is_dir():\n",
    "        for f in p.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                if (f.suffix.lower() in ALLOWED_EXTS) and (not f.name.startswith(\".\")):\n",
    "                    candidates.append(f)\n",
    "\n",
    "# De-dupe + stable order\n",
    "candidates = sorted(set(candidates), key=lambda x: x.as_posix().lower())\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    raise ValueError(\"Phase 2: No eligible source files found under RUN_SOURCE_PATHS.\")\n",
    "\n",
    "# 2b) Resolve source_type (exact path match, else nearest parent match, else unknown)\n",
    "typed = []\n",
    "for f in candidates:\n",
    "    st = next(\n",
    "        (SOURCE_TYPES_MAP[p] for p in ([f] + list(f.parents)) if p in SOURCE_TYPES_MAP),\n",
    "        \"unknown\",\n",
    "    )\n",
    "    typed.append((f, st))\n",
    "\n",
    "# quick counts by type\n",
    "counts = {}\n",
    "for _, st in typed:\n",
    "    counts[st] = counts.get(st, 0) + 1\n",
    "\n",
    "print(\"Candidate files by source_type:\")\n",
    "for k in sorted(counts, key=lambda x: (-counts[x], x)):\n",
    "    print(f\"{counts[k]:>6}  {k}\")\n",
    "del counts, k\n",
    "\n",
    "# 2c) Selection by SOURCE_MODE\n",
    "selected = []\n",
    "\n",
    "if SOURCE_MODE == \"ALL\":\n",
    "    selected = typed\n",
    "\n",
    "else:\n",
    "    print(\"\\nCandidate files:\")\n",
    "    for i, (f, st) in enumerate(typed):\n",
    "        try:\n",
    "            show = f.relative_to(WORLD_ROOT)\n",
    "        except Exception:\n",
    "            show = f\n",
    "        print(f\"{i:>4}  {st:<18}  {show}\")\n",
    "        del show\n",
    "\n",
    "    raw = input(\"\\nSelect by index (e.g., 0,2,5-8) or 'all': \").strip().lower()\n",
    "\n",
    "    if raw == \"all\":\n",
    "        selected = typed\n",
    "    else:\n",
    "        idxs = set()\n",
    "        parts = [x.strip() for x in raw.split(\",\") if x.strip()]\n",
    "\n",
    "        for part in parts:\n",
    "            if \"-\" in part:\n",
    "                a, b = [x.strip() for x in part.split(\"-\", 1)]\n",
    "                if not (a.isdigit() and b.isdigit()):\n",
    "                    raise ValueError(f\"Bad range: '{part}'\")\n",
    "                for j in range(int(a), int(b) + 1):\n",
    "                    idxs.add(j)\n",
    "                del a, b\n",
    "            else:\n",
    "                if not part.isdigit():\n",
    "                    raise ValueError(f\"Bad index: '{part}'\")\n",
    "                idxs.add(int(part))\n",
    "\n",
    "        bad = [j for j in sorted(idxs) if j < 0 or j >= len(typed)]\n",
    "        if bad:\n",
    "            raise ValueError(f\"Invalid selection indexes: {bad}\")\n",
    "\n",
    "        selected = [typed[j] for j in sorted(idxs)]\n",
    "\n",
    "        del idxs, parts, part, bad, j\n",
    "\n",
    "    del raw\n",
    "\n",
    "# 2d) Build SOURCE_FILES records (include relpath)\n",
    "for i, (f, st) in enumerate(selected, start=1):\n",
    "    try:\n",
    "        rel = str(f.resolve().relative_to(WORLD_ROOT))\n",
    "    except Exception:\n",
    "        rel = str(f)\n",
    "\n",
    "    SOURCE_FILES.append({\n",
    "        \"source_id\": f\"src_{i:06d}\",\n",
    "        \"path\": f,\n",
    "        \"relpath\": rel,\n",
    "        \"source_type\": st,\n",
    "        \"ext\": f.suffix.lower(),\n",
    "    })\n",
    "\n",
    "print(f\"\\nPhase 2 OK: selected {len(SOURCE_FILES)} source files (of {len(candidates)} candidates).\")\n",
    "\n",
    "# clean up locals\n",
    "del ALLOWED_EXTS, candidates, typed, selected, p, f, st, i, rel\n",
    "del Path\n",
    "# clean up \n",
    "del RUN_SOURCE_PATHS, SOURCE_TYPES_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c19c6-b8b8-4f43-9234-bdca07394c1e",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Phase 3: Normalize selected inputs\n",
    "\n",
    "In this phase, the notebook converts the **selected source files** into a consistent, machine-usable form.\n",
    "\n",
    "Different file formats (Markdown, plain text, Word documents) store text differently.  \n",
    "Before any indexing, chunking, or analysis can occur, those differences must be removed.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Opens each selected file using a format-appropriate reader\n",
    "- Extracts raw textual content without interpretation\n",
    "- Preserves line order exactly as it appears in the source file\n",
    "- Represents each file as an ordered sequence of text lines\n",
    "- Records minimal metadata needed to trace each line back to its source file\n",
    "\n",
    "This phase performs the task:\n",
    "\n",
    "**“Create a uniform, trustworthy representation of the selected sources.”**\n",
    "\n",
    "This phase performs **no chunking, interpretation, or transformation** of content.\n",
    "Text is preserved exactly as read (including blank lines and formatting), and files are never modified on disk.\n",
    "\n",
    "The output of this phase is a normalized in-memory representation of each selected file, suitable for later chunking and indexing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ad5429-a91e-4601-b8cf-352f88c0e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sources: 130\n",
      " - [0] txt: _local/auto_transcripts/caravan tales session 1.txt  [auto_transcripts]  (3473 lines)\n",
      " - [1] txt: _local/auto_transcripts/caravan tales session 2.txt  [auto_transcripts]  (2827 lines)\n",
      " - [2] txt: _local/auto_transcripts/iwtc session 000.txt  [auto_transcripts]  (2599 lines)\n",
      " - [3] txt: _local/auto_transcripts/iwtc session 001.5.txt  [auto_transcripts]  (1561 lines)\n",
      " - [4] txt: _local/auto_transcripts/iwtc session 001.txt  [auto_transcripts]  (4192 lines)\n",
      " - [5] txt: _local/auto_transcripts/iwtc session 002.txt  [auto_transcripts]  (2543 lines)\n",
      " - [6] txt: _local/auto_transcripts/iwtc session 003.txt  [auto_transcripts]  (4853 lines)\n",
      " - [7] txt: _local/auto_transcripts/iwtc session 007.txt  [auto_transcripts]  (2033 lines)\n",
      " - [8] txt: _local/auto_transcripts/iwtc session 008.txt  [auto_transcripts]  (2341 lines)\n",
      " - [9] txt: _local/auto_transcripts/iwtc session 010.txt  [auto_transcripts]  (3363 lines)\n",
      " - [10] txt: _local/auto_transcripts/iwtc session 011.txt  [auto_transcripts]  (2687 lines)\n",
      " - [11] txt: _local/auto_transcripts/iwtc session 012.txt  [auto_transcripts]  (3317 lines)\n",
      " - [12] txt: _local/auto_transcripts/iwtc session 013.txt  [auto_transcripts]  (2527 lines)\n",
      " - [13] txt: _local/auto_transcripts/iwtc session 015.txt  [auto_transcripts]  (2589 lines)\n",
      " - [14] txt: _local/auto_transcripts/iwtc session 017.txt  [auto_transcripts]  (1367 lines)\n",
      " - [15] txt: _local/auto_transcripts/iwtc session 018.txt  [auto_transcripts]  (3939 lines)\n",
      " - [16] txt: _local/auto_transcripts/iwtc session 019.txt  [auto_transcripts]  (2739 lines)\n",
      " - [17] txt: _local/auto_transcripts/iwtc session 020.txt  [auto_transcripts]  (3667 lines)\n",
      " - [18] txt: _local/auto_transcripts/iwtc session 021.txt  [auto_transcripts]  (3989 lines)\n",
      " - [19] txt: _local/auto_transcripts/iwtc session 022.txt  [auto_transcripts]  (3229 lines)\n",
      " - [20] txt: _local/auto_transcripts/iwtc session 023.txt  [auto_transcripts]  (2767 lines)\n",
      " - [21] txt: _local/auto_transcripts/iwtc session 025.txt  [auto_transcripts]  (3035 lines)\n",
      " - [22] txt: _local/auto_transcripts/iwtc session 028.1.txt  [auto_transcripts]  (1529 lines)\n",
      " - [23] txt: _local/auto_transcripts/iwtc session 028.2.txt  [auto_transcripts]  (2327 lines)\n",
      " - [24] txt: _local/auto_transcripts/iwtc session 030.txt  [auto_transcripts]  (3799 lines)\n",
      " - [25] txt: _local/auto_transcripts/iwtc session 031.txt  [auto_transcripts]  (3587 lines)\n",
      " - [26] txt: _local/auto_transcripts/iwtc session 032.5.txt  [auto_transcripts]  (1015 lines)\n",
      " - [27] txt: _local/auto_transcripts/iwtc session 032.txt  [auto_transcripts]  (1015 lines)\n",
      " - [28] txt: _local/auto_transcripts/iwtc session 033.txt  [auto_transcripts]  (4163 lines)\n",
      " - [29] txt: _local/auto_transcripts/iwtc session 034.txt  [auto_transcripts]  (3491 lines)\n",
      " - [30] txt: _local/auto_transcripts/iwtc session 035.txt  [auto_transcripts]  (3483 lines)\n",
      " - [31] txt: _local/auto_transcripts/iwtc session 036.1.txt  [auto_transcripts]  (2359 lines)\n",
      " - [32] txt: _local/auto_transcripts/iwtc session 036.2.txt  [auto_transcripts]  (831 lines)\n",
      " - [33] txt: _local/auto_transcripts/iwtc session 038.txt  [auto_transcripts]  (3761 lines)\n",
      " - [34] txt: _local/auto_transcripts/iwtc session 039.txt  [auto_transcripts]  (4299 lines)\n",
      " - [35] txt: _local/auto_transcripts/iwtc session 040.txt  [auto_transcripts]  (5111 lines)\n",
      " - [36] txt: _local/auto_transcripts/iwtc session 041.txt  [auto_transcripts]  (3867 lines)\n",
      " - [37] txt: _local/auto_transcripts/iwtc session 042.txt  [auto_transcripts]  (4089 lines)\n",
      " - [38] txt: _local/auto_transcripts/iwtc session 043.txt  [auto_transcripts]  (2989 lines)\n",
      " - [39] txt: _local/auto_transcripts/iwtc session 044.txt  [auto_transcripts]  (3483 lines)\n",
      " - [40] txt: _local/auto_transcripts/iwtc session 045.txt  [auto_transcripts]  (3533 lines)\n",
      " - [41] txt: _local/auto_transcripts/iwtc session 046.txt  [auto_transcripts]  (4091 lines)\n",
      " - [42] txt: _local/auto_transcripts/iwtc session 047.txt  [auto_transcripts]  (1897 lines)\n",
      " - [43] txt: _local/auto_transcripts/iwtc session 048.txt  [auto_transcripts]  (4061 lines)\n",
      " - [44] txt: _local/auto_transcripts/iwtc session 049.txt  [auto_transcripts]  (1719 lines)\n",
      " - [45] txt: _local/auto_transcripts/iwtc session 050.txt  [auto_transcripts]  (3837 lines)\n",
      " - [46] txt: _local/auto_transcripts/iwtc session 051.txt  [auto_transcripts]  (3635 lines)\n",
      " - [47] txt: _local/auto_transcripts/iwtc session 052.txt  [auto_transcripts]  (2747 lines)\n",
      " - [48] txt: _local/auto_transcripts/iwtc session 054.txt  [auto_transcripts]  (2265 lines)\n",
      " - [49] txt: _local/auto_transcripts/iwtc session 055.txt  [auto_transcripts]  (2927 lines)\n",
      " - [50] txt: _local/auto_transcripts/iwtc session 056.txt  [auto_transcripts]  (3295 lines)\n",
      " - [51] txt: _local/auto_transcripts/iwtc session 057.txt  [auto_transcripts]  (3436 lines)\n",
      " - [52] txt: _local/auto_transcripts/iwtc session 058c.txt  [auto_transcripts]  (1131 lines)\n",
      " - [53] txt: _local/auto_transcripts/iwtc session 059.5.txt  [auto_transcripts]  (729 lines)\n",
      " - [54] txt: _local/auto_transcripts/iwtc session 059.txt  [auto_transcripts]  (2396 lines)\n",
      " - [55] txt: _local/auto_transcripts/iwtc session 060.txt  [auto_transcripts]  (3193 lines)\n",
      " - [56] txt: _local/auto_transcripts/iwtc session 061.txt  [auto_transcripts]  (4215 lines)\n",
      " - [57] txt: _local/auto_transcripts/iwtc session 062.txt  [auto_transcripts]  (3683 lines)\n",
      " - [58] txt: _local/auto_transcripts/iwtc session 063.txt  [auto_transcripts]  (3597 lines)\n",
      " - [59] txt: _local/auto_transcripts/iwtc session 064.txt  [auto_transcripts]  (2299 lines)\n",
      " - [60] txt: _local/auto_transcripts/iwtc session 065.txt  [auto_transcripts]  (3333 lines)\n",
      " - [61] txt: _local/auto_transcripts/iwtc session 066.txt  [auto_transcripts]  (3947 lines)\n",
      " - [62] txt: _local/auto_transcripts/iwtc session 067.txt  [auto_transcripts]  (1657 lines)\n",
      " - [63] txt: _local/auto_transcripts/iwtc session 068.txt  [auto_transcripts]  (3443 lines)\n",
      " - [64] txt: _local/auto_transcripts/iwtc session 069.txt  [auto_transcripts]  (1207 lines)\n",
      " - [65] txt: _local/auto_transcripts/iwtc session 070.txt  [auto_transcripts]  (4339 lines)\n",
      " - [66] txt: _local/auto_transcripts/iwtc session 071.txt  [auto_transcripts]  (3863 lines)\n",
      " - [67] txt: _local/auto_transcripts/iwtc session 072.txt  [auto_transcripts]  (4221 lines)\n",
      " - [68] txt: _local/auto_transcripts/iwtc session 073.txt  [auto_transcripts]  (3659 lines)\n",
      " - [69] txt: _local/auto_transcripts/iwtc session 074.txt  [auto_transcripts]  (4013 lines)\n",
      " - [70] txt: _local/auto_transcripts/iwtc session 075.txt  [auto_transcripts]  (3293 lines)\n",
      " - [71] txt: _local/auto_transcripts/iwtc session 077.txt  [auto_transcripts]  (3515 lines)\n",
      " - [72] txt: _local/auto_transcripts/iwtc session 078.txt  [auto_transcripts]  (3209 lines)\n",
      " - [73] txt: _local/auto_transcripts/iwtc session 079.txt  [auto_transcripts]  (2543 lines)\n",
      " - [74] txt: _local/auto_transcripts/iwtc session 080.txt  [auto_transcripts]  (3213 lines)\n",
      " - [75] txt: _local/auto_transcripts/iwtc session 081.txt  [auto_transcripts]  (4183 lines)\n",
      " - [76] txt: _local/auto_transcripts/iwtc session 082.txt  [auto_transcripts]  (3461 lines)\n",
      " - [77] txt: _local/auto_transcripts/iwtc session 083.txt  [auto_transcripts]  (3691 lines)\n",
      " - [78] txt: _local/auto_transcripts/iwtc session 084.txt  [auto_transcripts]  (4251 lines)\n",
      " - [79] txt: _local/auto_transcripts/iwtc session 085.txt  [auto_transcripts]  (4489 lines)\n",
      " - [80] txt: _local/auto_transcripts/iwtc session 086.txt  [auto_transcripts]  (4351 lines)\n",
      " - [81] txt: _local/auto_transcripts/iwtc session 087.txt  [auto_transcripts]  (2721 lines)\n",
      " - [82] txt: _local/auto_transcripts/iwtc session 088.txt  [auto_transcripts]  (3135 lines)\n",
      " - [83] txt: _local/auto_transcripts/iwtc session 089.txt  [auto_transcripts]  (3683 lines)\n",
      " - [84] txt: _local/auto_transcripts/iwtc session 090.txt  [auto_transcripts]  (4223 lines)\n",
      " - [85] txt: _local/auto_transcripts/iwtc session 091.txt  [auto_transcripts]  (2843 lines)\n",
      " - [86] txt: _local/auto_transcripts/iwtc session 092.txt  [auto_transcripts]  (3609 lines)\n",
      " - [87] txt: _local/auto_transcripts/iwtc session 093.txt  [auto_transcripts]  (4157 lines)\n",
      " - [88] txt: _local/auto_transcripts/iwtc session 094.txt  [auto_transcripts]  (4341 lines)\n",
      " - [89] txt: _local/auto_transcripts/iwtc session 095.txt  [auto_transcripts]  (2077 lines)\n",
      " - [90] txt: _local/auto_transcripts/iwtc session 096.txt  [auto_transcripts]  (3581 lines)\n",
      " - [91] txt: _local/auto_transcripts/iwtc session 097.txt  [auto_transcripts]  (4617 lines)\n",
      " - [92] txt: _local/auto_transcripts/iwtc session 098.txt  [auto_transcripts]  (3399 lines)\n",
      " - [93] txt: _local/auto_transcripts/iwtc session 099.txt  [auto_transcripts]  (2819 lines)\n",
      " - [94] txt: _local/auto_transcripts/iwtc session 101.txt  [auto_transcripts]  (4619 lines)\n",
      " - [95] txt: _local/auto_transcripts/iwtc session 102.txt  [auto_transcripts]  (2555 lines)\n",
      " - [96] txt: _local/auto_transcripts/iwtc session 103.txt  [auto_transcripts]  (3425 lines)\n",
      " - [97] txt: _local/auto_transcripts/iwtc session 104.txt  [auto_transcripts]  (4149 lines)\n",
      " - [98] txt: _local/auto_transcripts/iwtc session 105.txt  [auto_transcripts]  (3369 lines)\n",
      " - [99] txt: _local/auto_transcripts/iwtc session 106.txt  [auto_transcripts]  (2201 lines)\n",
      " - [100] txt: _local/auto_transcripts/iwtc session 107.txt  [auto_transcripts]  (3455 lines)\n",
      " - [101] txt: _local/auto_transcripts/iwtc session 108.txt  [auto_transcripts]  (4123 lines)\n",
      " - [102] txt: _local/auto_transcripts/iwtc session 109.txt  [auto_transcripts]  (4093 lines)\n",
      " - [103] txt: _local/auto_transcripts/iwtc session 110.txt  [auto_transcripts]  (2749 lines)\n",
      " - [104] txt: _local/auto_transcripts/iwtc session 111.txt  [auto_transcripts]  (3833 lines)\n",
      " - [105] md: _local/pbp_transcripts/PbP10 - The Second Camp.md  [pbp_transcripts]  (450 lines)\n",
      " - [106] md: _local/pbp_transcripts/PbP11 - Lia and the Tolanites.md  [pbp_transcripts]  (432 lines)\n",
      " - [107] md: _local/pbp_transcripts/PbP12 - Meeting in the Vestry.md  [pbp_transcripts]  (1142 lines)\n",
      " - [108] md: _local/pbp_transcripts/PbP13 - The Town Square Incident.md  [pbp_transcripts]  (252 lines)\n",
      " - [109] md: _local/pbp_transcripts/PbP14 - Recon.md  [pbp_transcripts]  (645 lines)\n",
      " - [110] md: _local/pbp_transcripts/PbP15 - Debrief and Safety.md  [pbp_transcripts]  (968 lines)\n",
      " - [111] md: _local/pbp_transcripts/PbP16 - Nightfall in Elysia.md  [pbp_transcripts]  (652 lines)\n",
      " - [112] md: _local/pbp_transcripts/PbP17 - Night Meetings.md  [pbp_transcripts]  (421 lines)\n",
      " - [113] md: _local/pbp_transcripts/PbP18 - TBD.md  [pbp_transcripts]  (125 lines)\n",
      " - [114] md: _local/planning_notes/Allip Encounter Notes.md  [planning_notes]  (51 lines)\n",
      " - [115] md: _local/planning_notes/current_Dhassa staged narration.md  [planning_notes]  (589 lines)\n",
      " - [116] md: _local/planning_notes/current_IWTC names.md  [planning_notes]  (428 lines)\n",
      " - [117] md: _local/planning_notes/current_IWTC planning notes.md  [planning_notes]  (404 lines)\n",
      " - [118] md: _local/planning_notes/current_Kwalish.md  [planning_notes]  (226 lines)\n",
      " - [119] md: _local/planning_notes/Elulind map descriptions.md  [planning_notes]  (41 lines)\n",
      " - [120] md: _local/planning_notes/The Premature Pods Mystery.md  [planning_notes]  (353 lines)\n",
      " - [121] md: _local/planning_notes/The Spencer Heir.md  [planning_notes]  (102 lines)\n",
      " - [122] md: _local/planning_notes/The Wolfstream Situation.md  [planning_notes]  (159 lines)\n",
      " - [123] md: _local/planning_notes/Victor’s Causal Chain.md  [planning_notes]  (62 lines)\n",
      " - [124] md: _local/planning_notes/Who Knows What.md  [planning_notes]  (203 lines)\n",
      " - [125] md: _local/session_notes/current_IWTC session notes.md  [session_notes]  (455 lines)\n",
      " - [126] md: _local/session_notes/IWTC session notes 1-50.md  [session_notes]  (1743 lines)\n",
      " - [127] md: _local/session_notes/IWTC session notes 101-150.md  [session_notes]  (1237 lines)\n",
      " - [128] md: _local/session_notes/IWTC session notes 51-100.md  [session_notes]  (2027 lines)\n",
      " - [129] md: _local/session_notes/Kavar notes.md  [session_notes]  (788 lines)\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Load selected sources into memory as raw lines\n",
    "# - Preserves original line order\n",
    "# - Preserves blank lines as empty strings\n",
    "# - Performs no interpretation or chunking\n",
    "LAST_PHASE_RUN = \"3\"\n",
    "\n",
    "from pathlib import Path\n",
    "import docx  # python-docx\n",
    "\n",
    "LOADED_SOURCES = []\n",
    "\n",
    "for source_id, item in enumerate(SOURCE_FILES):\n",
    "    path = Path(item[\"path\"])\n",
    "    relpath = item.get(\"relpath\", str(path))\n",
    "    source_type = item.get(\"source_type\", \"unknown\")\n",
    "\n",
    "    suffix = path.suffix.lower()\n",
    "\n",
    "    if suffix in (\".md\", \".txt\"):\n",
    "        file_type = suffix.lstrip(\".\")\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "        lines = text.splitlines()\n",
    "        del text  # text is no longer needed\n",
    "    elif suffix == \".docx\":\n",
    "        file_type = \"docx\"\n",
    "        doc = docx.Document(str(path))\n",
    "        lines = [p.text for p in doc.paragraphs]  # blank paragraphs preserved as \"\"\n",
    "        del doc\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported file type for source_id={source_id}: {path}\"\n",
    "        )\n",
    "\n",
    "    LOADED_SOURCES.append(\n",
    "        {\n",
    "            \"source_id\": source_id,\n",
    "            \"path\": path,\n",
    "            \"relpath\": relpath,\n",
    "            \"source_type\": source_type,\n",
    "            \"file_type\": file_type,\n",
    "            \"lines\": lines,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Loaded sources: {len(LOADED_SOURCES)}\")\n",
    "for s in LOADED_SOURCES:\n",
    "    print(\n",
    "        f\" - [{s['source_id']}] {s['file_type']}: {s['relpath']}  \"\n",
    "        f\"[{s['source_type']}]  ({len(s['lines'])} lines)\"\n",
    "    )\n",
    "\n",
    "del item, source_id, path, relpath, source_type, suffix, file_type, lines, s\n",
    "# clean up variables that have served their purpose\n",
    "del SOURCE_FILES, SOURCE_MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac0ce5-547d-42ae-ac25-1e7564e7a87a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 4: Data profiling\n",
    "\n",
    "In this phase, the notebook examines the **normalized source data** produced in Phase 3 in order to understand its real structural characteristics.\n",
    "\n",
    "At this point, all selected files have already been converted into a consistent in-memory representation:  \n",
    "each source is represented as an ordered sequence of text lines, preserved exactly as read.\n",
    "\n",
    "Before any chunking or indexing rules can be proposed, it is necessary to **observe how the data actually appears** after normalization.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Inspects line-level structure across normalized sources\n",
    "- Examines differences between file formats (e.g., `.txt`, `.md`, `.docx`) as they appear post-normalization\n",
    "- Identifies patterns such as blank lines, paragraph boundaries, headings, or artifacts\n",
    "- Produces human-readable summaries and previews for inspection\n",
    "- Supports interactive exploration of specific sources by ID\n",
    "\n",
    "This phase performs the task:\n",
    "\n",
    "**“Understand the shape and structure of the normalized data.”**\n",
    "\n",
    "This phase performs **no chunking, segmentation, or interpretation** of content.\n",
    "It does not define rules, thresholds, or heuristics.\n",
    "Its sole purpose is to inform later design decisions by grounding them in observed data rather than assumptions.\n",
    "\n",
    "The output of this phase is **human insight**, not transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195d60e-a65d-4aa9-a626-317b820b193d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Phase 4a: File-level line length profiling and charts\n",
    "# Observational only — no persistence, no mutation\n",
    "LAST_PHASE_RUN = \"4a\"\n",
    "\n",
    "from collections import defaultdict\n",
    "from statistics import mean, median\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if not LOADED_SOURCES:\n",
    "    raise ValueError(\"No loaded sources found. Run Phase 3 first.\")\n",
    "\n",
    "# --- collect per-file 5-number summaries ---\n",
    "file_profiles = []\n",
    "\n",
    "for src in LOADED_SOURCES:\n",
    "    lengths = sorted(len(l) for l in src[\"lines\"] if l)\n",
    "\n",
    "    if not lengths:\n",
    "        continue\n",
    "\n",
    "    q1 = np.percentile(lengths, 25)\n",
    "    q3 = np.percentile(lengths, 75)\n",
    "\n",
    "    file_profiles.append({\n",
    "        \"source_type\": src[\"source_type\"],\n",
    "        \"min\": lengths[0],\n",
    "        \"q1\": q1,\n",
    "        \"median\": median(lengths),\n",
    "        \"q3\": q3,\n",
    "        \"max\": lengths[-1],\n",
    "    })\n",
    "\n",
    "# --- group by source_type ---\n",
    "by_type = defaultdict(list)\n",
    "for p in file_profiles:\n",
    "    by_type[p[\"source_type\"]].append(p)\n",
    "\n",
    "# --- Box plots: distribution of medians by source type ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "\n",
    "for source_type in sorted(by_type.keys()):\n",
    "    labels.append(source_type)\n",
    "    data.append([p[\"median\"] for p in by_type[source_type]])\n",
    "\n",
    "plt.boxplot(data, tick_labels=labels, showfliers=True)\n",
    "plt.title(\"Distribution of per-file median line lengths by source type\")\n",
    "plt.ylabel(\"Median line length\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Box plots: distribution of max line lengths by source type ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "\n",
    "for source_type in sorted(by_type.keys()):\n",
    "    labels.append(source_type)\n",
    "    data.append([p[\"max\"] for p in by_type[source_type]])\n",
    "\n",
    "plt.boxplot(data, tick_labels=labels, showfliers=True)\n",
    "plt.title(\"Distribution of per-file max line lengths by source type\")\n",
    "plt.ylabel(\"Max line length\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Scatter: median vs max (outlier detector) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for source_type in sorted(by_type.keys()):\n",
    "    medians = [p[\"median\"] for p in by_type[source_type]]\n",
    "    maxes = [p[\"max\"] for p in by_type[source_type]]\n",
    "    plt.scatter(medians, maxes, label=source_type, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Median line length (per file)\")\n",
    "plt.ylabel(\"Max line length (per file)\")\n",
    "plt.title(\"Median vs Max line length per file\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# cleanup\n",
    "del src, lengths, q1, q3, p\n",
    "del labels, data, medians, maxes\n",
    "del file_profiles, by_type, source_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c17a02-41e9-4f6f-9534-cbb550a800e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 4b: Sanity check: preview a loaded source by ID\n",
    "LAST_PHASE_RUN = \"4b\"\n",
    "\n",
    "PREVIEW_SOURCE_ID = 120\n",
    "PREVIEW_MAX_LINES = 50\n",
    "\n",
    "s = next((x for x in LOADED_SOURCES if x[\"source_id\"] == PREVIEW_SOURCE_ID), None)\n",
    "if s is None:\n",
    "    available = sorted(x[\"source_id\"] for x in LOADED_SOURCES)\n",
    "    raise ValueError(\n",
    "        f\"No loaded source found with source_id={PREVIEW_SOURCE_ID}. \"\n",
    "        f\"Available source_id values: {available}\"\n",
    "    )\n",
    "\n",
    "print(f\"[{s['source_id']}] {s['file_type']}: {s['relpath']}  [{s['source_type']}]\")\n",
    "print(\"\")\n",
    "\n",
    "for i, line in enumerate(s[\"lines\"][:PREVIEW_MAX_LINES], start=1):\n",
    "    print(f\"{i:>4}: {line}\")\n",
    "\n",
    "# clean up local variables\n",
    "del i, line, s\n",
    "del PREVIEW_SOURCE_ID, PREVIEW_MAX_LINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be325c36-b167-4dc9-90a4-98ea047ddd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4c: deciling line lengths for a specific source type\n",
    "LAST_PHASE_RUN = \"4c\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for src in LOADED_SOURCES:\n",
    "    if src[\"source_type\"] != \"session_notes\":\n",
    "        continue\n",
    "\n",
    "    path = src[\"path\"]\n",
    "    lines = src[\"lines\"]\n",
    "\n",
    "    if not lines:\n",
    "        continue\n",
    "\n",
    "    # Extract last 3 characters before \".txt\"\n",
    "    stem = path.stem\n",
    "    file_id = stem[-3:]\n",
    "\n",
    "    line_lengths = [len(line) for line in lines]\n",
    "    deciles = np.percentile(line_lengths, range(0, 101, 10))\n",
    "\n",
    "    row = {\"file\": file_id}\n",
    "    for p, value in zip(range(0, 101, 10), deciles):\n",
    "        row[f\"p{p}\"] = value\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df_auto_deciles = pd.DataFrame(rows).sort_values(\"file\").reset_index(drop=True)\n",
    "df_auto_deciles\n",
    "\n",
    "# clean up local variables\n",
    "del rows, src, path, lines, stem, file_id, line_lengths\n",
    "del deciles, row, p, value, df_auto_deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fa69c-4bf4-4605-9680-1e86c8bc4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4d: Scan session_notes for \"session\" header signal\n",
    "LAST_PHASE_RUN = \"4d\"\n",
    "\n",
    "import re\n",
    "\n",
    "SESSION_WORD_REGEX = re.compile(r\"\\bsession\\b\", re.IGNORECASE)\n",
    "\n",
    "total_hits = 0\n",
    "files_with_hits = 0\n",
    "\n",
    "print('Scanning session_notes for word \"session\"...\\n')\n",
    "\n",
    "for src in LOADED_SOURCES:\n",
    "    if src.get(\"source_type\") != \"session_notes\":\n",
    "        continue\n",
    "\n",
    "    path = src[\"path\"]\n",
    "    lines = src[\"lines\"]\n",
    "\n",
    "    file_hits = 0\n",
    "\n",
    "    for idx, line in enumerate(lines, start=1):\n",
    "        if SESSION_WORD_REGEX.search(line):\n",
    "            if file_hits == 0:\n",
    "                files_with_hits += 1\n",
    "                print(f\"\\nFILE: {path.name}\")\n",
    "            file_hits += 1\n",
    "            total_hits += 1\n",
    "            print(f\"  L{idx}: {line}\")\n",
    "\n",
    "    if file_hits:\n",
    "        print(f\"  -> hits in file: {file_hits}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Files scanned: {sum(1 for s in LOADED_SOURCES if s.get('source_type') == 'session_notes')}\")\n",
    "print(f\"Files with hits: {files_with_hits}\")\n",
    "print(f\"Total 'session' hits: {total_hits}\")\n",
    "\n",
    "# cleanup\n",
    "del SESSION_WORD_REGEX, total_hits, files_with_hits, src, path, lines, idx, line, file_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8894e4-fa4b-4069-9803-6ce68604a006",
   "metadata": {},
   "source": [
    "### Phase 4: Data Profiling & Structural Findings\n",
    "\n",
    "This phase focused on **observing and understanding the real structure of raw source data** before proposing any chunking or indexing rules.\n",
    "\n",
    "No transformation, normalization, or chunking was performed in this phase.  \n",
    "All conclusions are based on direct inspection of normalized line-level data produced in Phase 3.\n",
    "\n",
    "The purpose of Phase 4 is to ensure that any future chunking rules are grounded in **how the data is actually written**, not assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "### What Was Analyzed\n",
    "\n",
    "All available source files were loaded and profiled:\n",
    "\n",
    "- OVERRIDE_PATHS = None\n",
    "- SOURCE_MODE = \"ALL\"\n",
    "- Total files analyzed: **125**\n",
    "- Formats included:\n",
    "  - `.txt`\n",
    "  - `.md`\n",
    "  - `.docx`\n",
    "\n",
    "For each file, line-length distributions were analyzed using:\n",
    "- minimum\n",
    "- deciles / quantiles\n",
    "- median\n",
    "- maximum\n",
    "- standard deviation\n",
    "\n",
    "Analysis was grouped by **source_type**, not file format.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings by Source Type\n",
    "\n",
    "#### 1. auto_transcripts\n",
    "\n",
    "**Observed structure**\n",
    "- Files follow a rigid, repeating pattern:\n",
    "  - One blank line\n",
    "  - Timestamp line\n",
    "  - One line of spoken text\n",
    "- Approximately 50% timestamps, 50% dialogue\n",
    "- Dialogue lines consistently range ~35–175 characters\n",
    "- Extremely low variance across files\n",
    "\n",
    "**Implications**\n",
    "- This source is highly regular and predictable\n",
    "- Structural units are already implicit in the data\n",
    "- Line-level alternation is meaningful and should be preserved\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. pbp_transcripts\n",
    "\n",
    "**Observed structure**\n",
    "- High structural variability\n",
    "- Lines include:\n",
    "  - speaker names\n",
    "  - dialogue\n",
    "  - emotes\n",
    "  - blank lines\n",
    "  - single-character markers (e.g., \"-\")\n",
    "- Markdown formatting appears in some files but not others\n",
    "- Datetimestamps are inconsistent and not always reliable\n",
    "- Line-length distributions show a long tail:\n",
    "  - Many very short lines\n",
    "  - Occasional very long narrative blocks\n",
    "\n",
    "**Implications**\n",
    "- Speaker attribution matters\n",
    "- Sequence matters more than timestamps\n",
    "- Chunking must tolerate mixed formatting\n",
    "- Line length alone is not a reliable delimiter\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. planning_notes (docx-heavy)\n",
    "\n",
    "**Observed structure**\n",
    "- Formatting information from `.docx` is not preserved\n",
    "- Blank lines consistently separate conceptual blocks\n",
    "- Many blocks consist of a **single line**, often serving as:\n",
    "  - section headers\n",
    "  - category labels\n",
    "- Lines are generally short; content is list-like\n",
    "- Long lines are rare but present\n",
    "\n",
    "**Implications**\n",
    "- Blank lines are the primary structural signal\n",
    "- Single-line blocks likely represent major sections\n",
    "- The first line of a block often labels the block\n",
    "- Formatting inference must be conservative\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. session_notes\n",
    "\n",
    "**Observed structure**\n",
    "- Files often contain multiple sessions\n",
    "- Session headers:\n",
    "  - usually include a session number and a date\n",
    "  - punctuation and exact format vary\n",
    "- Blank lines separate complete thoughts\n",
    "- Line-length distribution:\n",
    "  - heavy use of blank lines\n",
    "  - short median lines\n",
    "  - very long tail (summaries, pasted narrative)\n",
    "- One file is expected to produce many logical documents\n",
    "\n",
    "**Implications**\n",
    "- Document-level segmentation is required\n",
    "- Blank lines represent paragraph boundaries\n",
    "- Chunking must occur *within* session boundaries\n",
    "- Over-splitting is safer than under-splitting\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-Cutting Observations\n",
    "\n",
    "- Blank lines are a **strong structural signal** across all human-authored sources\n",
    "- Line length alone is insufficient for chunking decisions\n",
    "- File format does not reliably indicate structure\n",
    "- Source type is the most important determinant of chunking behavior\n",
    "- Preserving original order and spacing is critical\n",
    "\n",
    "---\n",
    "\n",
    "### Resulting v0 Structural Contracts (Conceptual)\n",
    "\n",
    "These are **conceptual contracts**, not implementations.\n",
    "\n",
    "| Source Type        | Document Boundary        | Atomic Unit        |\n",
    "|--------------------|--------------------------|--------------------|\n",
    "| auto_transcripts   | File                     | Timestamp + text pair |\n",
    "| pbp_transcripts    | File                     | Speaker / dialogue block |\n",
    "| planning_notes     | File                     | Blank-line-delimited block |\n",
    "| session_notes      | Session header            | Blank-line-delimited paragraph |\n",
    "\n",
    "These contracts will guide future chunking design but are not yet codified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40df1c-874f-4eb0-b703-e08b5f116e86",
   "metadata": {},
   "source": [
    "## Phase 5: Chunking (v0)\n",
    "\n",
    "In this phase, the notebook defines and applies **v0 chunking rules** to the normalized sources produced in Phase 3.\n",
    "\n",
    "Chunking is the first structural transformation step: it groups contiguous lines into **chunks** that will later be indexed.\n",
    "\n",
    "v0 is intentionally simple:\n",
    "\n",
    "- A **new chunk begins** whenever a line matches any known *header pattern*\n",
    "- All following lines belong to that chunk until the next header line\n",
    "- Chunk boundaries preserve original line order and line numbers\n",
    "- Chunking is designed for iterative refinement; anomalies are expected and will usually be handled by fixing the source file and rerunning\n",
    "\n",
    "This phase produces:\n",
    "\n",
    "- `CHUNKS_V0`: a list of chunk dictionaries suitable for later indexing steps\n",
    "\n",
    "This phase performs **no semantic interpretation** and does not modify any source files on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80719e06-98db-48ad-9227-96e2db2fadda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked v0: 170131 chunks from 130 files.\n"
     ]
    }
   ],
   "source": [
    "# Phase 5: Chunking v0 - consolidated header-driven chunking\n",
    "# Input:  LOADED_SOURCES (list[dict])\n",
    "# Output: CHUNKS_V0 (list[dict])\n",
    "LAST_PHASE_RUN = \"5\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "TIME_LIKE_REGEX = r\"\\d{1,2}:\\d{2}(?::\\d{2})?(?:\\s*[AP]M)?\"\n",
    "\n",
    "# Header regexes are evaluated in order; first match wins.\n",
    "# Ordering reflects specificity and expected frequency.\n",
    "HEADER_REGEXES = [\n",
    "    # auto_transcripts (most frequent, most rigid)\n",
    "    (\"auto_ts\", re.compile(r\"^\\s*\\d{1,2}:\\d{2}(?::\\d{2})?\\s*$\")),\n",
    "\n",
    "    # pbp_transcripts (Discord-style headers, sometimes numbered like \"2. ### ...\")\n",
    "    (\"pbp_hash\", re.compile(rf\"^\\s*(?:\\d+\\.\\s*)?(?:[*-]\\s*)?###\\s+.*{TIME_LIKE_REGEX}.*$\")),\n",
    "\n",
    "    # pbp_transcripts (forum-style quoted bold header)\n",
    "    (\"pbp_forum\", re.compile(rf\"^\\s*>?\\s*\\*\\*.*{TIME_LIKE_REGEX}.*\\*\\*\\s*$\")),\n",
    "\n",
    "    # session_notes (lines beginning with optional format codes and \"Session\", omitting \"Session notes\")\n",
    "    (\"session\", re.compile(r\"^\\s*(?:\\d+\\.\\s*)?(?:[>#*_\\-\\s]+)?session\\s+(?!notes\\b)\\S.*$\", re.IGNORECASE)),\n",
    "\n",
    "    # planning_notes (lines beginning with markdown headers)\n",
    "    (\"md_heading\", re.compile(r\"^\\s*(?:[*\\-]\\s*)?#{1,6}\\s+\\S.*$\")),\n",
    "]\n",
    "\n",
    "CHUNKS_V0 = []\n",
    "chunk_global_id = 1  # global (not per-file) to allow stable cross-file references\n",
    "\n",
    "world_root_resolved = Path(WORLD_ROOT).resolve()\n",
    "\n",
    "for src in LOADED_SOURCES:\n",
    "    source_id = src[\"source_id\"]\n",
    "    path = src[\"path\"]\n",
    "    relpath = src.get(\"relpath\", str(path))\n",
    "    source_type = src.get(\"source_type\", \"unknown\")\n",
    "    file_type = src.get(\"file_type\", \"unknown\")\n",
    "    lines = src[\"lines\"]\n",
    "\n",
    "    current_kind = \"preamble\"\n",
    "    current_lines = []\n",
    "    chunk_start_line = 1\n",
    "\n",
    "    for idx, line in enumerate(lines, start=1):\n",
    "        matched_kind = next(\n",
    "            (kind for kind, header_regex in HEADER_REGEXES if header_regex.match(line)),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if matched_kind:\n",
    "            # Flush what we have so far (literal fidelity: keep preamble content too)\n",
    "            if current_lines:\n",
    "                CHUNKS_V0.append(\n",
    "                    {\n",
    "                        \"chunk_id\": chunk_global_id,\n",
    "                        \"source_id\": source_id,\n",
    "                        \"source_type\": source_type,\n",
    "                        \"file_type\": file_type,\n",
    "                        \"path\": path,\n",
    "                        \"relpath\": relpath,\n",
    "                        \"start_line\": chunk_start_line,\n",
    "                        \"end_line\": idx - 1,\n",
    "                        \"header_kind\": current_kind,\n",
    "                        \"lines\": list(current_lines),\n",
    "                    }\n",
    "                )\n",
    "                chunk_global_id += 1\n",
    "\n",
    "            # Start a new chunk at this header line (header is included)\n",
    "            current_kind = matched_kind\n",
    "            current_lines = [line]\n",
    "            chunk_start_line = idx\n",
    "\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "\n",
    "    # Flush final chunk (including files with no headers)\n",
    "    if current_lines:\n",
    "        CHUNKS_V0.append(\n",
    "            {\n",
    "                \"chunk_id\": chunk_global_id,\n",
    "                \"source_id\": source_id,\n",
    "                \"source_type\": source_type,\n",
    "                \"file_type\": file_type,\n",
    "                \"path\": path,\n",
    "                \"relpath\": relpath,\n",
    "                \"start_line\": chunk_start_line,\n",
    "                \"end_line\": len(lines),\n",
    "                \"header_kind\": current_kind,\n",
    "                \"lines\": list(current_lines),\n",
    "            }\n",
    "        )\n",
    "        chunk_global_id += 1\n",
    "\n",
    "print(f\"Chunked v0: {len(CHUNKS_V0)} chunks from {len(LOADED_SOURCES)} files.\")\n",
    "\n",
    "# Cleanup locals (keep CHUNKS_V0)\n",
    "del TIME_LIKE_REGEX, HEADER_REGEXES, chunk_global_id\n",
    "del src, source_id, path, relpath, source_type, file_type, lines\n",
    "del current_kind, current_lines, chunk_start_line, idx, line, matched_kind\n",
    "del world_root_resolved\n",
    "del Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e5e1a-0e60-43a5-953b-a08091cfc381",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 6: Vocabulary bootstrap (candidate proper nouns)\n",
    "\n",
    "In this phase, the notebook generates a first-pass list of candidate proper nouns and named entities from the v0 chunks.\n",
    "\n",
    "Goal:\n",
    "- Produce a ranked candidate list of names/vocabs worth adding to the world vocabulary.\n",
    "- Provide evidence snippets so a human can confirm canon spellings and create aliases.\n",
    "\n",
    "Approach (v0):\n",
    "- Exclude `auto_transcripts` to avoid overwhelming the candidate list with conversational noise.\n",
    "- Extract:\n",
    "  - Multiword Title Case vocabs (e.g., \"Temple of the Bronze Flame\")\n",
    "  - Single-word Proper Nouns (e.g., \"Dhassa\", \"Killeth\")\n",
    "  - PbP authors (e.g. \"CroweTheDualityKing\")\n",
    "- Aggregate counts by:\n",
    "  - total mentions\n",
    "  - chunks mentioned\n",
    "  - files mentioned\n",
    "  - source_type distribution\n",
    "- Capture a small number of evidence snippets per candidate.\n",
    "\n",
    "This phase does not resolve ambiguity (e.g., \"temple\" vs \"monastery\", or \"Lia\" vs \"Liavarah\").\n",
    "It produces the evidence needed to curate aliases and canonical forms in a later step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08de51d4-a1b4-48c8-8c41-7820dc346500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 6a ready.\n"
     ]
    }
   ],
   "source": [
    "# Phase 6a: Vocabulary bootstrap setup (v0)\n",
    "# Input:  CHUNKS_V0\n",
    "# Output: shared config + aggregate structures for Phase 6b/6c\n",
    "LAST_PHASE_RUN = \"6a\"\n",
    "\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "RUN_STAMP_6 = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "del datetime\n",
    "\n",
    "# Content candidate scan excludes noisy sources (keeps current behavior).\n",
    "EXCLUDE_SOURCE_TYPES_CONTENT = {\"auto_transcripts\"}\n",
    "\n",
    "# Header author scan should NOT filter on source_type.\n",
    "# It keys exclusively off header_kind == \"pbp_hash\".\n",
    "\n",
    "CONNECTORS = {\n",
    "    \"of\", \"the\", \"and\", \"to\", \"in\", \"at\", \"on\", \"for\", \"from\", \"with\", \"by\", \"a\", \"an\",\n",
    "}\n",
    "\n",
    "STOP_SINGLE = {\n",
    "    \"I\", \"A\", \"An\", \"The\", \"And\", \"Or\", \"But\", \"We\", \"You\", \"He\", \"She\", \"They\",\n",
    "    \"This\", \"That\", \"These\", \"Those\", \"It\", \"Its\", \"Our\", \"My\", \"Your\", \"His\", \"Her\",\n",
    "    \"Session\", \"Sessions\",\n",
    "    \"No\",\"Yes\",\"What\",\"Why\",\"How\",\"When\",\"Where\",\"Who\",\"Whom\",\"Which\",\n",
    "    \"As\",\"If\",\"At\",\"In\",\"On\",\"Not\",\"With\",\"Without\",\"Within\",\"For\",\"From\",\"To\",\"Of\",\n",
    "    \"And\",\"Or\",\"But\",\"So\",\"Then\",\"Than\",\"Now\",\"Just\",\"Only\",\"Also\",\"Still\",\"Even\",\n",
    "    \"There\",\"Here\",\"This\",\"That\",\"These\",\"Those\",\"It\",\"Its\",\"We\",\"You\",\"He\",\"She\",\"They\",\n",
    "    \"Do\",\"Does\",\"Did\",\"Can\",\"Could\",\"Will\",\"Would\",\"Shall\",\"Should\",\"May\",\"Might\",\"Must\",\n",
    "    \"All\",\"One\",\"Well\",\"Go\",\"Ah\",\"After\",\"Oh\",\"Let\",\"AM\",\"PM\"\n",
    "}\n",
    "\n",
    "STOP_TOKENS = {\"##\", \"#\", \"###\", \">\", \"*\", \"-\", \"_\", \"`\"}\n",
    "\n",
    "MAX_EVIDENCE_PER_CANDIDATE = 5\n",
    "MAX_SNIPPET_CHARS = 160\n",
    "\n",
    "# Regexes (content scan)\n",
    "WORD_REGEX = re.compile(r\"[A-Za-z][A-Za-z0-9'\\-]*\")\n",
    "TITLE_WORD_REGEX = re.compile(r\"^[A-Z][a-z][A-Za-z'\\-]*$\")\n",
    "ACRONYM_REGEX = re.compile(r\"^[A-Z]{2,8}$\")\n",
    "\n",
    "# Regex (pbp_hash header author scan)\n",
    "# Example: * ### **Shworn** **7/3/25, 6:12 PM**\n",
    "PBP_HASH_HEADER_REGEX = re.compile(\n",
    "    r\"^\\s*(?:\\d+\\.\\s*)?(?:[*-]\\s*)?###\\s+\\*\\*(?P<author>[^*]+)\\*\\*\\s+\\*\\*(?P<ts>[^*]+)\\*\\*\\s*$\"\n",
    ")\n",
    "\n",
    "# Aggregates for content candidate vocab\n",
    "mentions = Counter()\n",
    "chunks_mentioned = defaultdict(set)\n",
    "files_mentioned = defaultdict(set)\n",
    "by_source_type = defaultdict(Counter)\n",
    "evidence = defaultdict(list)\n",
    "\n",
    "# Aggregates for pbp header authors\n",
    "header_mentions = Counter()          # author -> total header occurrences\n",
    "header_chunks = defaultdict(set)     # author -> set(chunk_id)\n",
    "header_files = defaultdict(set)      # author -> set(path)\n",
    "header_evidence = defaultdict(list)  # author -> evidence dicts\n",
    "\n",
    "print(\"Phase 6a ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2608c8d-6dda-41f5-848d-e8ba8c2db564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content chunks scanned: 1486 (excluded: ['auto_transcripts'])\n",
      "Content candidates found: 16655\n",
      "PbP header authors found: 6\n"
     ]
    }
   ],
   "source": [
    "# Phase 6b: Vocabulary bootstrap scans (v0)\n",
    "# Input:  CHUNKS_V0 and Phase 6a globals\n",
    "# Output: populated aggregates for Phase 6c\n",
    "#\n",
    "# Design:\n",
    "# - Header scan: does NOT filter by source_type. It keys only off header_kind == \"pbp_hash\".\n",
    "# - Content scan: filters by source_type (exclude auto_transcripts) via included_chunks_content.\n",
    "LAST_PHASE_RUN = \"6b\"\n",
    "\n",
    "# -----------------------\n",
    "# 6b.0: Select chunks for content scan\n",
    "# -----------------------\n",
    "included_chunks_content = [\n",
    "    c for c in CHUNKS_V0\n",
    "    if c.get(\"source_type\") not in EXCLUDE_SOURCE_TYPES_CONTENT\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# 6b.1: PbP header author scan (no source_type filtering)\n",
    "# -----------------------\n",
    "for chunk in CHUNKS_V0:\n",
    "    lines = chunk.get(\"lines\", [])\n",
    "    header_kind = chunk.get(\"header_kind\")\n",
    "\n",
    "    if lines and header_kind == \"pbp_hash\":\n",
    "        header_line = (lines[0] or \"\").strip()\n",
    "\n",
    "        if header_line:\n",
    "            m = PBP_HASH_HEADER_REGEX.match(header_line)\n",
    "            if m:\n",
    "                author = \" \".join((m.group(\"author\") or \"\").split())\n",
    "                ts = \" \".join((m.group(\"ts\") or \"\").split())\n",
    "\n",
    "                if author:\n",
    "                    chunk_id = chunk.get(\"chunk_id\")\n",
    "                    source_type = chunk.get(\"source_type\", \"unknown\")\n",
    "                    path = str(chunk.get(\"path\", \"\"))\n",
    "\n",
    "                    header_mentions[author] += 1\n",
    "                    header_chunks[author].add(chunk_id)\n",
    "                    header_files[author].add(path)\n",
    "\n",
    "                    if len(header_evidence[author]) < MAX_EVIDENCE_PER_CANDIDATE:\n",
    "                        header_evidence[author].append(\n",
    "                            {\n",
    "                                \"chunk_id\": chunk_id,\n",
    "                                \"source_type\": source_type,\n",
    "                                \"path\": path,\n",
    "                                \"start_line\": chunk.get(\"start_line\"),\n",
    "                                \"end_line\": chunk.get(\"end_line\"),\n",
    "                                \"header\": header_line,\n",
    "                                \"author\": author,\n",
    "                                \"timestamp\": ts,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    del chunk_id, source_type, path\n",
    "                del author, ts\n",
    "            del m\n",
    "\n",
    "# -----------------------\n",
    "# 6b.2: Content candidate scan (exclude auto_transcripts by source_type)\n",
    "# -----------------------\n",
    "for chunk in included_chunks_content:\n",
    "    lines = chunk.get(\"lines\", [])\n",
    "    if lines:\n",
    "        header_kind = chunk.get(\"header_kind\")\n",
    "\n",
    "        # Split header from content (instead of dropping header entirely)\n",
    "        is_header_chunk = bool(header_kind in {\"pbp_hash\", \"pbp_forum\", \"session\"} and lines)\n",
    "        content_lines = lines[1:] if is_header_chunk else lines\n",
    "\n",
    "        # Collapse all chunk content text to single-space whitespace\n",
    "        concat_text = \" \".join(\" \".join(content_lines).split())\n",
    "\n",
    "        if concat_text:\n",
    "            chunk_id = chunk.get(\"chunk_id\")\n",
    "            source_type = chunk.get(\"source_type\", \"unknown\")\n",
    "            path = str(chunk.get(\"path\", \"\"))\n",
    "\n",
    "            # Evidence snippet (reuse for all candidates found in this chunk)\n",
    "            if len(concat_text) > MAX_SNIPPET_CHARS:\n",
    "                snippet = concat_text[: MAX_SNIPPET_CHARS - 3] + \"...\"\n",
    "            else:\n",
    "                snippet = concat_text\n",
    "\n",
    "            words = WORD_REGEX.findall(concat_text)\n",
    "            candidates = []\n",
    "\n",
    "            # ---- extract Title Case vocabs ----\n",
    "            i = 0\n",
    "            n = len(words)\n",
    "\n",
    "            while i < n:\n",
    "                w = words[i]\n",
    "                if TITLE_WORD_REGEX.match(w) or ACRONYM_REGEX.match(w):\n",
    "                    parts = [w]\n",
    "                    cap_count = 1\n",
    "                    j = i + 1\n",
    "\n",
    "                    while j < n:\n",
    "                        wj = words[j]\n",
    "                        wj_lower = wj.lower()\n",
    "\n",
    "                        if TITLE_WORD_REGEX.match(wj) or ACRONYM_REGEX.match(wj):\n",
    "                            parts.append(wj)\n",
    "                            cap_count += 1\n",
    "                            j += 1\n",
    "                        elif wj_lower in CONNECTORS:\n",
    "                            parts.append(wj_lower)\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    if cap_count >= 2:\n",
    "                        candidates.append(\" \".join(parts))\n",
    "\n",
    "                    i = j\n",
    "                    \n",
    "                    del parts, cap_count, j\n",
    "                else:\n",
    "                    i += 1\n",
    "                del w\n",
    "            del i,n\n",
    "\n",
    "            # ---- extract single-word propers ----\n",
    "            if words:\n",
    "                first_word = words[0]\n",
    "                tail_set = set(words[1:])\n",
    "\n",
    "                for w in words:\n",
    "                    if (\n",
    "                        (TITLE_WORD_REGEX.match(w) or ACRONYM_REGEX.match(w))\n",
    "                        and w not in STOP_TOKENS\n",
    "                        and w not in STOP_SINGLE\n",
    "                        and (w != first_word or w in tail_set)\n",
    "                    ):\n",
    "                        candidates.append(w)\n",
    "                del w\n",
    "\n",
    "            # Record aggregates\n",
    "            if candidates:\n",
    "                seen_this_chunk = set()\n",
    "\n",
    "                # Count mentions (raw frequency)\n",
    "                for cand in candidates:\n",
    "                    mentions[cand] += 1\n",
    "                    seen_this_chunk.add(cand)\n",
    "\n",
    "                # Per-chunk / per-file coverage + by_source_type + evidence\n",
    "                for cand in seen_this_chunk:\n",
    "                    chunks_mentioned[cand].add(chunk_id)\n",
    "                    files_mentioned[cand].add(path)\n",
    "                    by_source_type[cand][source_type] += 1\n",
    "\n",
    "                    if len(evidence[cand]) < MAX_EVIDENCE_PER_CANDIDATE:\n",
    "                        evidence[cand].append(\n",
    "                            {\n",
    "                                \"chunk_id\": chunk_id,\n",
    "                                \"source_type\": source_type,\n",
    "                                \"path\": path,\n",
    "                                \"start_line\": chunk.get(\"start_line\"),\n",
    "                                \"end_line\": chunk.get(\"end_line\"),\n",
    "                                \"snippet\": snippet,\n",
    "                            }\n",
    "                        )\n",
    "                del cand\n",
    "\n",
    "print(f\"Content chunks scanned: {len(included_chunks_content)} (excluded: {sorted(EXCLUDE_SOURCE_TYPES_CONTENT)})\")\n",
    "print(f\"Content candidates found: {len(mentions)}\")\n",
    "print(f\"PbP header authors found: {len(header_mentions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "698bdb15-6ade-4f05-a126-07bb93bf6790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded source types (content scan): ['auto_transcripts']\n",
      "Content chunks scanned: 1486\n",
      "Candidates found: 16655\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>mentions_total</th>\n",
       "      <th>chunks_mentioned</th>\n",
       "      <th>files_mentioned</th>\n",
       "      <th>source_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Henry</td>\n",
       "      <td>832</td>\n",
       "      <td>293</td>\n",
       "      <td>20</td>\n",
       "      <td>{'pbp_transcripts': 157, 'planning_notes': 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alivyre</td>\n",
       "      <td>535</td>\n",
       "      <td>233</td>\n",
       "      <td>20</td>\n",
       "      <td>{'pbp_transcripts': 111, 'planning_notes': 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Faeryne</td>\n",
       "      <td>847</td>\n",
       "      <td>239</td>\n",
       "      <td>19</td>\n",
       "      <td>{'pbp_transcripts': 93, 'planning_notes': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lia</td>\n",
       "      <td>405</td>\n",
       "      <td>169</td>\n",
       "      <td>19</td>\n",
       "      <td>{'pbp_transcripts': 86, 'planning_notes': 7, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Victor</td>\n",
       "      <td>724</td>\n",
       "      <td>338</td>\n",
       "      <td>18</td>\n",
       "      <td>{'pbp_transcripts': 283, 'planning_notes': 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Crafthold</td>\n",
       "      <td>337</td>\n",
       "      <td>142</td>\n",
       "      <td>17</td>\n",
       "      <td>{'pbp_transcripts': 32, 'planning_notes': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Luminia</td>\n",
       "      <td>285</td>\n",
       "      <td>133</td>\n",
       "      <td>14</td>\n",
       "      <td>{'pbp_transcripts': 49, 'planning_notes': 2, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Elysia</td>\n",
       "      <td>74</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>{'pbp_transcripts': 45, 'planning_notes': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Urgulk</td>\n",
       "      <td>167</td>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>{'pbp_transcripts': 25, 'planning_notes': 3, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dhassa</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>{'pbp_transcripts': 5, 'planning_notes': 8, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Temple</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>{'pbp_transcripts': 12, 'planning_notes': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Elulind</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>{'pbp_transcripts': 11, 'planning_notes': 8, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Terry</td>\n",
       "      <td>152</td>\n",
       "      <td>83</td>\n",
       "      <td>12</td>\n",
       "      <td>{'pbp_transcripts': 56, 'planning_notes': 4, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bear</td>\n",
       "      <td>101</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>{'pbp_transcripts': 17, 'planning_notes': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bysickle</td>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>{'pbp_transcripts': 26, 'session_notes': 23}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tolanite</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>{'pbp_transcripts': 14, 'planning_notes': 6, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kalina</td>\n",
       "      <td>74</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>{'pbp_transcripts': 22, 'session_notes': 44}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Remedy</td>\n",
       "      <td>65</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>{'pbp_transcripts': 29, 'planning_notes': 17, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>King</td>\n",
       "      <td>64</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>{'pbp_transcripts': 28, 'planning_notes': 9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Shworn</td>\n",
       "      <td>176</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>{'pbp_transcripts': 20, 'planning_notes': 5, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Two</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>{'pbp_transcripts': 6, 'planning_notes': 2, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Little</td>\n",
       "      <td>47</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>{'pbp_transcripts': 17, 'planning_notes': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Piers</td>\n",
       "      <td>129</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>{'pbp_transcripts': 72, 'planning_notes': 3, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kavar</td>\n",
       "      <td>108</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>{'pbp_transcripts': 3, 'planning_notes': 3, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Party</td>\n",
       "      <td>78</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>{'pbp_transcripts': 2, 'planning_notes': 8, 's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    candidate  mentions_total  chunks_mentioned  files_mentioned  \\\n",
       "0       Henry             832               293               20   \n",
       "1     Alivyre             535               233               20   \n",
       "2     Faeryne             847               239               19   \n",
       "3         Lia             405               169               19   \n",
       "4      Victor             724               338               18   \n",
       "5   Crafthold             337               142               17   \n",
       "6     Luminia             285               133               14   \n",
       "7      Elysia              74                58               14   \n",
       "8      Urgulk             167                54               13   \n",
       "9      Dhassa              80                32               13   \n",
       "10     Temple              42                28               13   \n",
       "11    Elulind              32                22               13   \n",
       "12      Terry             152                83               12   \n",
       "13       Bear             101                63               12   \n",
       "14   Bysickle              53                49               12   \n",
       "15   Tolanite              34                25               12   \n",
       "16     Kalina              74                66               11   \n",
       "17     Remedy              65                48               11   \n",
       "18       King              64                44               11   \n",
       "19     Shworn             176                43               11   \n",
       "20        Two              14                14               11   \n",
       "21     Little              47                39               10   \n",
       "22      Piers             129                76                9   \n",
       "23      Kavar             108                58                9   \n",
       "24      Party              78                52                9   \n",
       "\n",
       "                                         source_types  \n",
       "0   {'pbp_transcripts': 157, 'planning_notes': 21,...  \n",
       "1   {'pbp_transcripts': 111, 'planning_notes': 11,...  \n",
       "2   {'pbp_transcripts': 93, 'planning_notes': 9, '...  \n",
       "3   {'pbp_transcripts': 86, 'planning_notes': 7, '...  \n",
       "4   {'pbp_transcripts': 283, 'planning_notes': 13,...  \n",
       "5   {'pbp_transcripts': 32, 'planning_notes': 9, '...  \n",
       "6   {'pbp_transcripts': 49, 'planning_notes': 2, '...  \n",
       "7   {'pbp_transcripts': 45, 'planning_notes': 9, '...  \n",
       "8   {'pbp_transcripts': 25, 'planning_notes': 3, '...  \n",
       "9   {'pbp_transcripts': 5, 'planning_notes': 8, 's...  \n",
       "10  {'pbp_transcripts': 12, 'planning_notes': 9, '...  \n",
       "11  {'pbp_transcripts': 11, 'planning_notes': 8, '...  \n",
       "12  {'pbp_transcripts': 56, 'planning_notes': 4, '...  \n",
       "13  {'pbp_transcripts': 17, 'planning_notes': 1, '...  \n",
       "14       {'pbp_transcripts': 26, 'session_notes': 23}  \n",
       "15  {'pbp_transcripts': 14, 'planning_notes': 6, '...  \n",
       "16       {'pbp_transcripts': 22, 'session_notes': 44}  \n",
       "17  {'pbp_transcripts': 29, 'planning_notes': 17, ...  \n",
       "18  {'pbp_transcripts': 28, 'planning_notes': 9, '...  \n",
       "19  {'pbp_transcripts': 20, 'planning_notes': 5, '...  \n",
       "20  {'pbp_transcripts': 6, 'planning_notes': 2, 's...  \n",
       "21  {'pbp_transcripts': 17, 'planning_notes': 1, '...  \n",
       "22  {'pbp_transcripts': 72, 'planning_notes': 3, '...  \n",
       "23  {'pbp_transcripts': 3, 'planning_notes': 3, 's...  \n",
       "24  {'pbp_transcripts': 2, 'planning_notes': 8, 's...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PbP header authors found: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>mentions_total</th>\n",
       "      <th>chunks_mentioned</th>\n",
       "      <th>files_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charis</td>\n",
       "      <td>229</td>\n",
       "      <td>229</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CroweTheDualityKing</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bysickle</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalina Hitana</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shworn</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lia</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  mentions_total  chunks_mentioned  files_mentioned\n",
       "0               Charis             229               229                6\n",
       "1  CroweTheDualityKing             193               193                6\n",
       "2             Bysickle              57                57                6\n",
       "3        Kalina Hitana              42                42                6\n",
       "4               Shworn              28                28                3\n",
       "5                  Lia               8                 8                2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: _local/machine_wip/candidate_vocab_v0_excluding_auto_transcripts_20260210_160247.csv\n",
      "Wrote: _local/machine_wip/candidate_vocab_v0_evidence_top150_20260210_160247.md\n",
      "Wrote: _local/machine_wip/candidate_pbp_authors_v0_20260210_160247.csv\n",
      "Wrote: _local/machine_wip/candidate_pbp_authors_v0_evidence_20260210_160247.md\n"
     ]
    }
   ],
   "source": [
    "# Phase 6c: Materialize outputs + cleanup (v0)\n",
    "# Input:  aggregates from 6a/6b, WORKING_DRAFTS_PATH, WORLD_ROOT (for relpaths in outputs)\n",
    "# Output: CANDIDATE_VOCAB + written files\n",
    "LAST_PHASE_RUN = \"6c\"\n",
    "\n",
    "# Resolve WORLD_ROOT once for relpath output\n",
    "world_root_resolved = Path(WORLD_ROOT).resolve()\n",
    "\n",
    "# ---- build CANDIDATE_VOCAB ----\n",
    "rows = []\n",
    "for cand, total in mentions.most_common():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"candidate\": cand,\n",
    "            \"mentions_total\": total,\n",
    "            \"chunks_mentioned\": len(chunks_mentioned[cand]),\n",
    "            \"files_mentioned\": len(files_mentioned[cand]),\n",
    "            \"source_types\": dict(by_source_type[cand]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "CANDIDATE_VOCAB = pd.DataFrame(rows)\n",
    "if not CANDIDATE_VOCAB.empty:\n",
    "    CANDIDATE_VOCAB = (\n",
    "        CANDIDATE_VOCAB\n",
    "        .sort_values(\n",
    "            by=[\"files_mentioned\", \"chunks_mentioned\", \"mentions_total\"],\n",
    "            ascending=[False, False, False],\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(f\"Excluded source types (content scan): {sorted(EXCLUDE_SOURCE_TYPES_CONTENT)}\")\n",
    "print(f\"Content chunks scanned: {len(included_chunks_content)}\")\n",
    "print(f\"Candidates found: {len(CANDIDATE_VOCAB)}\")\n",
    "display(CANDIDATE_VOCAB.head(25))\n",
    "\n",
    "# ---- build HEADER_AUTHORS_V0 ----\n",
    "header_rows = []\n",
    "for author, total in header_mentions.most_common():\n",
    "    header_rows.append(\n",
    "        {\n",
    "            \"author\": author,\n",
    "            \"mentions_total\": total,\n",
    "            \"chunks_mentioned\": len(header_chunks[author]),\n",
    "            \"files_mentioned\": len(header_files[author]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "HEADER_AUTHORS_V0 = pd.DataFrame(header_rows)\n",
    "if not HEADER_AUTHORS_V0.empty:\n",
    "    HEADER_AUTHORS_V0 = (\n",
    "        HEADER_AUTHORS_V0\n",
    "        .sort_values(\n",
    "            by=[\"files_mentioned\", \"chunks_mentioned\", \"mentions_total\"],\n",
    "            ascending=[False, False, False],\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(f\"PbP header authors found: {len(HEADER_AUTHORS_V0)}\")\n",
    "display(HEADER_AUTHORS_V0.head(25))\n",
    "\n",
    "# ---- write outputs ----\n",
    "out_dir = Path(WORKING_DRAFTS_PATH)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# vocab outputs (same naming convention as before)\n",
    "vocab_csv_path = out_dir / f\"candidate_vocab_v0_excluding_auto_transcripts_{RUN_STAMP_6}.csv\"\n",
    "CANDIDATE_VOCAB.to_csv(vocab_csv_path, index=False, encoding=\"utf-8\")\n",
    "try:\n",
    "    vocab_csv_rel = str(vocab_csv_path.resolve().relative_to(world_root_resolved))\n",
    "except Exception:\n",
    "    vocab_csv_rel = str(vocab_csv_path)\n",
    "print(f\"Wrote: {vocab_csv_rel}\")\n",
    "\n",
    "TOP_N = 150\n",
    "vocab_md_path = out_dir / f\"candidate_vocab_v0_evidence_top{TOP_N}_{RUN_STAMP_6}.md\"\n",
    "with vocab_md_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Candidate vocab evidence (v0, excluding auto_transcripts)\\n\\n\")\n",
    "    f.write(\"For human review. Ranked by file/chunk coverage.\\n\\n\")\n",
    "\n",
    "    for i, row in CANDIDATE_VOCAB.head(TOP_N).iterrows():\n",
    "        cand = row[\"candidate\"]\n",
    "        f.write(f\"## {i+1}. {cand}\\n\\n\")\n",
    "        f.write(f\"- mentions_total: {row['mentions_total']}\\n\")\n",
    "        f.write(f\"- chunks_mentioned: {row['chunks_mentioned']}\\n\")\n",
    "        f.write(f\"- files_mentioned: {row['files_mentioned']}\\n\")\n",
    "        f.write(f\"- source_types: {row['source_types']}\\n\\n\")\n",
    "\n",
    "        for ev in evidence.get(cand, []):\n",
    "            p = ev[\"path\"]\n",
    "            try:\n",
    "                rel = str(Path(p).resolve().relative_to(world_root_resolved))\n",
    "            except Exception:\n",
    "                rel = p\n",
    "\n",
    "            f.write(\n",
    "                f\"- chunk {ev['chunk_id']} [{ev['source_type']}] \"\n",
    "                f\"{rel} L{ev['start_line']}-L{ev['end_line']}\\n\"\n",
    "            )\n",
    "            f.write(f\"  - {ev['snippet']}\\n\")\n",
    "            del p, rel\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "    del i,row\n",
    "\n",
    "try:\n",
    "    vocab_md_rel = str(vocab_md_path.resolve().relative_to(world_root_resolved))\n",
    "except Exception:\n",
    "    vocab_md_rel = str(vocab_md_path)\n",
    "print(f\"Wrote: {vocab_md_rel}\")\n",
    "\n",
    "# pbp header author outputs (new)\n",
    "authors_csv_path = out_dir / f\"candidate_pbp_authors_v0_{RUN_STAMP_6}.csv\"\n",
    "HEADER_AUTHORS_V0.to_csv(authors_csv_path, index=False, encoding=\"utf-8\")\n",
    "try:\n",
    "    authors_csv_rel = str(authors_csv_path.resolve().relative_to(world_root_resolved))\n",
    "except Exception:\n",
    "    authors_csv_rel = str(authors_csv_path)\n",
    "print(f\"Wrote: {authors_csv_rel}\")\n",
    "\n",
    "authors_md_path = out_dir / f\"candidate_pbp_authors_v0_evidence_{RUN_STAMP_6}.md\"\n",
    "with authors_md_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Candidate PbP authors from headers (v0)\\n\\n\")\n",
    "    f.write(\"For human review. Extracted from pbp_hash header lines.\\n\\n\")\n",
    "\n",
    "    for i, row in HEADER_AUTHORS_V0.iterrows():\n",
    "        author = row[\"author\"]\n",
    "        f.write(f\"## {i+1}. {author}\\n\\n\")\n",
    "        f.write(f\"- mentions_total: {row['mentions_total']}\\n\")\n",
    "        f.write(f\"- chunks_mentioned: {row['chunks_mentioned']}\\n\")\n",
    "        f.write(f\"- files_mentioned: {row['files_mentioned']}\\n\\n\")\n",
    "\n",
    "        for ev in header_evidence.get(author, []):\n",
    "            p = ev[\"path\"]\n",
    "            try:\n",
    "                rel = str(Path(p).resolve().relative_to(world_root_resolved))\n",
    "            except Exception:\n",
    "                rel = p\n",
    "\n",
    "            f.write(\n",
    "                f\"- chunk {ev['chunk_id']} [{ev['source_type']}] \"\n",
    "                f\"{rel} L{ev['start_line']}-L{ev['end_line']}\\n\"\n",
    "            )\n",
    "            f.write(f\"  - {ev['header']}\\n\")\n",
    "            del ev, p, rel\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    del i, row\n",
    "\n",
    "try:\n",
    "    authors_md_rel = str(authors_md_path.resolve().relative_to(world_root_resolved))\n",
    "except Exception:\n",
    "    authors_md_rel = str(authors_md_path)\n",
    "print(f\"Wrote: {authors_md_rel}\")\n",
    "\n",
    "# ---- cleanup locals (keep CANDIDATE_VOCAB, HEADER_AUTHORS_V0) ----\n",
    "# clean up imports\n",
    "del re, defaultdict, Counter, Path, pd\n",
    "\n",
    "# clean up config variables\n",
    "del RUN_STAMP_6, EXCLUDE_SOURCE_TYPES_CONTENT\n",
    "del CONNECTORS, STOP_SINGLE, STOP_TOKENS\n",
    "del MAX_EVIDENCE_PER_CANDIDATE, MAX_SNIPPET_CHARS\n",
    "del WORD_REGEX, TITLE_WORD_REGEX, ACRONYM_REGEX, PBP_HASH_HEADER_REGEX\n",
    "del mentions, chunks_mentioned, files_mentioned, by_source_type, evidence\n",
    "del header_mentions, header_chunks, header_files, header_evidence\n",
    "\n",
    "# clean up working variables\n",
    "del included_chunks_content, chunk, lines, header_kind, is_header_chunk\n",
    "del content_lines, concat_text, chunk_id, source_type, path, snippet\n",
    "del words, candidates, header_line, first_word, tail_set, seen_this_chunk\n",
    "del world_root_resolved, cand, rows, total, header_rows, author, out_dir\n",
    "del vocab_csv_path, vocab_csv_rel, TOP_N, vocab_md_path, vocab_md_rel\n",
    "del authors_csv_path, authors_csv_rel, authors_md_path, authors_md_rel\n",
    "del wj, wj_lower, f\n",
    "del HEADER_AUTHORS_V0, CANDIDATE_VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878b7c1-eb97-467a-8b9f-7b723dc31154",
   "metadata": {},
   "source": [
    "## Phase 7: Vocabulary-based linking\n",
    "\n",
    "In this phase, the notebook applies **curated vocabulary files** to the chunked source data in order to link known identifiers to specific regions of text.\n",
    "\n",
    "At this point, the source material has already been normalized and segmented into `CHUNKS_V0`. Each chunk represents a bounded region of text with stable source metadata and preserved line structure.\n",
    "\n",
    "Phase 7 introduces the first **interpretive pass** over the corpus, but it is intentionally conservative. Only explicitly defined vocabulary entries are considered, and no new identifiers are inferred or generated.\n",
    "\n",
    "This phase answers two closely related questions:\n",
    "\n",
    "- **Where do known entities appear in the corpus?**\n",
    "- **Who authored PbP content, where that information is explicitly available?**\n",
    "\n",
    "### What this phase does\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Loads authoritative vocabulary files (entities, aliases, author mappings)\n",
    "- Normalizes vocabulary into a unified matching table\n",
    "- Applies boundary-aware matching to chunk text\n",
    "- Links canonical entities and approved aliases to their occurrences\n",
    "- Extracts PbP authorship from header metadata\n",
    "- Records each match with stable source and positional context\n",
    "\n",
    "No semantic interpretation occurs here. All matches are literal and vocabulary-driven.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "Phase 7 produces structured reference tables:\n",
    "\n",
    "- **Entity mentions**: where known entities appear in chunked text\n",
    "- **Author mentions**: which player authored which PbP chunks, when available\n",
    "\n",
    "These outputs form a **reference index**, suitable for later aggregation, querying, attribution analysis, or graph construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d48fc91-a97e-4e62-b7b5-5f913d488130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded entities: 176 [_meta/indexes/vocab_entities.csv]\n",
      "Loaded aliases: 87 [_meta/indexes/vocab_aliases.csv]\n",
      "Loaded author aliases: 6 [_meta/indexes/vocab_author_aliases.csv]\n",
      "Loaded PC map rows: 42 [_meta/indexes/vocab_map_player_character.csv]\n",
      "Search vocabs: 251 (longest-first)\n"
     ]
    }
   ],
   "source": [
    "# Phase 7a: Linking preparation (v0)\n",
    "# Output:\n",
    "#   - entities_df\n",
    "#   - aliases_df (may be empty)\n",
    "#   - author_aliases_df (may be empty)\n",
    "#   - pc_map_df (may be empty)\n",
    "#   - VOCAB_DF (canonical + aliases, regex-ready)\n",
    "#\n",
    "# Notes:\n",
    "# - CSV schemas are human-facing and may contain extra columns.\n",
    "# - We normalize to semantic column names via *_COLS maps below.\n",
    "# - Any columns not mapped are intentionally ignored.\n",
    "LAST_PHASE_RUN = \"7a\"\n",
    "\n",
    "from datetime import datetime\n",
    "RUN_STAMP_7 = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "del datetime\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CASE_INSENSITIVE = True\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Semantic column mappings for vocab CSVs\n",
    "# Each semantic field -> acceptable column names in the CSV\n",
    "# Any other CSV columns are ignored on purpose.\n",
    "# ------------------------------------------------------------------\n",
    "ENTITY_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"canonical\": [\"canonical\", \"canonical_name\", \"name\"],\n",
    "}\n",
    "ALIAS_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"alias\": [\"alias\", \"alt\", \"alternate\"],\n",
    "}\n",
    "AUTHOR_ALIAS_COLS = {\n",
    "    \"author\": [\"author\", \"discord_name\", \"handle\"],\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"ambig_char_id\": [\"ambig_char_id\", \"ambiguous_character\", \"ambig_character\"],\n",
    "}\n",
    "PC_MAP_COLS = {\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"char_entity_id\": [\"char_entity_id\", \"character_entity_id\", \"character\"],\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper function to handle CSV column names\n",
    "# ------------------------------------------------------------------\n",
    "def _normalize_vocab_csv(df, col_map, label):\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=list(col_map.keys()))\n",
    "\n",
    "    # Build rename map: first matching option wins\n",
    "    rename = {}\n",
    "    for semantic, options in col_map.items():\n",
    "        found = next((c for c in options if c in df.columns), None)\n",
    "        if found:\n",
    "            rename[found] = semantic\n",
    "\n",
    "    # If there are rows but we couldn't resolve any semantic columns, warn loudly\n",
    "    if len(df) > 0 and not rename:\n",
    "        header_line = \", \".join(list(df.columns))\n",
    "        expected = {k: v for k, v in col_map.items()}\n",
    "        print(\n",
    "            f\"WARNING [{label}]: CSV has rows but none of the expected columns were found.\\n\"\n",
    "            f\"  CSV columns: {header_line}\\n\"\n",
    "            f\"  Expected (semantic -> acceptable names): {expected}\"\n",
    "        )\n",
    "        return pd.DataFrame(columns=list(col_map.keys()))\n",
    "\n",
    "    out = df.rename(columns=rename)\n",
    "\n",
    "    # Warn on missing semantic fields (but keep what we have)\n",
    "    missing_semantic = [k for k in col_map.keys() if k not in out.columns]\n",
    "    if len(df) > 0 and missing_semantic:\n",
    "        print(\n",
    "            f\"WARNING [{label}]: missing semantic columns after normalization: {missing_semantic}\\n\"\n",
    "            f\"  CSV columns: {list(df.columns)}\\n\"\n",
    "            f\"  Using: {list(out.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Keep only the semantic columns that exist (stable order)\n",
    "    keep = [k for k in col_map.keys() if k in out.columns]\n",
    "    return out[keep].copy()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate required globals\n",
    "# ------------------------------------------------------------------\n",
    "if \"CHUNKS_V0\" not in globals() or not CHUNKS_V0:\n",
    "    raise ValueError(\"CHUNKS_V0 is missing or empty. Rerun Phase 5.\")\n",
    "\n",
    "required_paths = {\"VOCAB_ENTITIES_PATH\", \"WORKING_DRAFTS_PATH\"}\n",
    "missing = [k for k in required_paths if k not in globals() or not globals()[k]]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required descriptor paths: {missing}\")\n",
    "\n",
    "# Optional paths (do not overwrite globals)\n",
    "vocab_aliases_path = globals().get(\"VOCAB_ALIASES_PATH\")\n",
    "vocab_authors_path = globals().get(\"VOCAB_AUTHORS_PATH\")\n",
    "vocab_pc_map_path = globals().get(\"VOCAB_PC_MAP_PATH\")\n",
    "\n",
    "# Relpaths (expected to be published by Phase 1c; fall back to \"none\")\n",
    "entities_rel = globals().get(\"VOCAB_ENTITIES_RELPATH\", \"none\")\n",
    "aliases_rel = globals().get(\"VOCAB_ALIASES_RELPATH\", \"none\")\n",
    "authors_rel = globals().get(\"VOCAB_AUTHORS_RELPATH\", \"none\")\n",
    "pc_map_rel = globals().get(\"VOCAB_PC_MAP_RELPATH\", \"none\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load entities (required) + normalize schema\n",
    "# ------------------------------------------------------------------\n",
    "entities_path = Path(VOCAB_ENTITIES_PATH)\n",
    "if not entities_path.exists():\n",
    "    raise ValueError(f\"Entities vocab file not found: {entities_path}\")\n",
    "\n",
    "entities_raw = pd.read_csv(entities_path, dtype=str).fillna(\"\")\n",
    "entities_df = _normalize_vocab_csv(entities_raw, ENTITY_COLS, \"entities\")\n",
    "\n",
    "if entities_df.empty:\n",
    "    raise ValueError(\"Entities CSV did not produce any usable rows after normalization.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load aliases (optional) + normalize schema\n",
    "# ------------------------------------------------------------------\n",
    "aliases_df = pd.DataFrame(columns=list(ALIAS_COLS.keys()))\n",
    "if vocab_aliases_path:\n",
    "    p = Path(vocab_aliases_path)\n",
    "    if p.exists():\n",
    "        aliases_raw = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "        aliases_df = _normalize_vocab_csv(aliases_raw, ALIAS_COLS, \"aliases\")\n",
    "    del p\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load author aliases (optional) + normalize schema\n",
    "# ------------------------------------------------------------------\n",
    "author_aliases_df = pd.DataFrame(columns=list(AUTHOR_ALIAS_COLS.keys()))\n",
    "if vocab_authors_path:\n",
    "    p = Path(vocab_authors_path)\n",
    "    if p.exists():\n",
    "        authors_raw = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "        author_aliases_df = _normalize_vocab_csv(authors_raw, AUTHOR_ALIAS_COLS, \"author_aliases\")\n",
    "        # Whitespace normalization on any columns that exist\n",
    "    del p\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load player-character map (optional) + normalize schema\n",
    "# ------------------------------------------------------------------\n",
    "pc_map_df = pd.DataFrame(columns=list(PC_MAP_COLS.keys()))\n",
    "if vocab_pc_map_path:\n",
    "    p = Path(vocab_pc_map_path)\n",
    "    if p.exists():\n",
    "        pc_raw = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "        pc_map_df = _normalize_vocab_csv(pc_raw, PC_MAP_COLS, \"pc_map\")\n",
    "    del p\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build VOCAB_DF (entities + aliases only)\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "for _, r in entities_df.iterrows():\n",
    "    if r.get(\"entity_id\", \"\") and r.get(\"canonical\", \"\"):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"vocab\": r[\"canonical\"],\n",
    "                \"entity_id\": r[\"entity_id\"],\n",
    "                \"canonical\": r[\"canonical\"],\n",
    "                \"match_kind\": \"canonical\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "if not aliases_df.empty:\n",
    "    canon_by_id = dict(zip(entities_df[\"entity_id\"], entities_df[\"canonical\"]))\n",
    "    for _, r in aliases_df.iterrows():\n",
    "        if r.get(\"entity_id\", \"\") and r.get(\"alias\", \"\"):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"vocab\": r[\"alias\"],\n",
    "                    \"entity_id\": r[\"entity_id\"],\n",
    "                    \"canonical\": canon_by_id.get(r[\"entity_id\"], \"\"),\n",
    "                    \"match_kind\": \"alias\",\n",
    "                }\n",
    "            )\n",
    "    del canon_by_id\n",
    "\n",
    "VOCAB_DF = pd.DataFrame(rows).drop_duplicates(subset=[\"vocab\", \"entity_id\"]).reset_index(drop=True)\n",
    "if VOCAB_DF.empty:\n",
    "    raise ValueError(\"No vocabs available for linking.\")\n",
    "\n",
    "VOCAB_DF[\"vocab_len\"] = VOCAB_DF[\"vocab\"].str.len()\n",
    "VOCAB_DF = VOCAB_DF.sort_values([\"vocab_len\", \"vocab\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "# Compile regex patterns (boundary-aware). Avoid backslashes inside f-string expressions.\n",
    "flags = re.IGNORECASE if CASE_INSENSITIVE else 0\n",
    "patterns = []\n",
    "for vocab in VOCAB_DF[\"vocab\"]:\n",
    "    esc = re.escape(vocab)\n",
    "    esc_ws = esc.replace(r\"\\ \", r\"\\s+\")\n",
    "    patterns.append(re.compile(rf\"(?<!\\w){esc_ws}(?!\\w)\", flags))\n",
    "VOCAB_DF[\"pattern\"] = patterns\n",
    "\n",
    "print(f\"Loaded entities: {len(entities_df)} [{entities_rel}]\")\n",
    "print(f\"Loaded aliases: {len(aliases_df)} [{aliases_rel}]\")\n",
    "print(f\"Loaded author aliases: {len(author_aliases_df)} [{authors_rel}]\")\n",
    "print(f\"Loaded PC map rows: {len(pc_map_df)} [{pc_map_rel}]\")\n",
    "print(f\"Search vocabs: {len(VOCAB_DF)} (longest-first)\")\n",
    "\n",
    "# Cleanup locals (keep *_df, VOCAB_DF, RUN_STAMP_7, CASE_INSENSITIVE)\n",
    "del required_paths, missing\n",
    "del vocab_aliases_path, vocab_authors_path, vocab_pc_map_path\n",
    "del entities_path, entities_raw\n",
    "del rows, r, vocab, patterns, flags, esc, esc_ws\n",
    "del entities_rel, aliases_rel, authors_rel, pc_map_rel\n",
    "del ENTITY_COLS, ALIAS_COLS, AUTHOR_ALIAS_COLS, PC_MAP_COLS\n",
    "del aliases_raw, authors_raw, pc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e78234ed-6c8f-49f5-93a7-031817567eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks scanned: 1486\n",
      "Entity mentions (rows): 5362\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention_id</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>matched_vocab</th>\n",
       "      <th>match_kind</th>\n",
       "      <th>match_count_in_chunk</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_type</th>\n",
       "      <th>path</th>\n",
       "      <th>relpath</th>\n",
       "      <th>chunk_start_line</th>\n",
       "      <th>chunk_end_line</th>\n",
       "      <th>header_kind</th>\n",
       "      <th>content_start_line</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>faction_spencer</td>\n",
       "      <td>Spencer</td>\n",
       "      <td>Spencer</td>\n",
       "      <td>canonical</td>\n",
       "      <td>1</td>\n",
       "      <td>168646</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>2</td>\n",
       "      <td>* \"although a Spencer is a greater threat i st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>person_shworn</td>\n",
       "      <td>Shworn Sleepsong</td>\n",
       "      <td>Shworn</td>\n",
       "      <td>alias</td>\n",
       "      <td>1</td>\n",
       "      <td>168646</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>2</td>\n",
       "      <td>* \"although a Spencer is a greater threat i st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>person_victor</td>\n",
       "      <td>Victor D Evernight</td>\n",
       "      <td>Victor</td>\n",
       "      <td>alias</td>\n",
       "      <td>1</td>\n",
       "      <td>168646</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>2</td>\n",
       "      <td>* \"although a Spencer is a greater threat i st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>person_piers</td>\n",
       "      <td>Piers Ashton</td>\n",
       "      <td>Piers</td>\n",
       "      <td>alias</td>\n",
       "      <td>1</td>\n",
       "      <td>168646</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>2</td>\n",
       "      <td>* \"although a Spencer is a greater threat i st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>person_shworn</td>\n",
       "      <td>Shworn Sleepsong</td>\n",
       "      <td>Shworn</td>\n",
       "      <td>alias</td>\n",
       "      <td>2</td>\n",
       "      <td>168647</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>7</td>\n",
       "      <td>* *Henry turns to his brother, letting out a s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mention_id        entity_id           canonical matched_vocab match_kind  \\\n",
       "0           1  faction_spencer             Spencer       Spencer  canonical   \n",
       "1           2    person_shworn    Shworn Sleepsong        Shworn      alias   \n",
       "2           3    person_victor  Victor D Evernight        Victor      alias   \n",
       "3           4     person_piers        Piers Ashton         Piers      alias   \n",
       "4           5    person_shworn    Shworn Sleepsong        Shworn      alias   \n",
       "\n",
       "   match_count_in_chunk  chunk_id  source_id      source_type  \\\n",
       "0                     1    168646        105  pbp_transcripts   \n",
       "1                     1    168646        105  pbp_transcripts   \n",
       "2                     1    168646        105  pbp_transcripts   \n",
       "3                     1    168646        105  pbp_transcripts   \n",
       "4                     2    168647        105  pbp_transcripts   \n",
       "\n",
       "                                                path  \\\n",
       "0  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "1  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "2  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "3  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "4  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "\n",
       "                                             relpath  chunk_start_line  \\\n",
       "0  _local/pbp_transcripts/PbP10 - The Second Camp.md                 1   \n",
       "1  _local/pbp_transcripts/PbP10 - The Second Camp.md                 1   \n",
       "2  _local/pbp_transcripts/PbP10 - The Second Camp.md                 1   \n",
       "3  _local/pbp_transcripts/PbP10 - The Second Camp.md                 1   \n",
       "4  _local/pbp_transcripts/PbP10 - The Second Camp.md                 6   \n",
       "\n",
       "   chunk_end_line header_kind  content_start_line  \\\n",
       "0               5    pbp_hash                   2   \n",
       "1               5    pbp_hash                   2   \n",
       "2               5    pbp_hash                   2   \n",
       "3               5    pbp_hash                   2   \n",
       "4              10    pbp_hash                   7   \n",
       "\n",
       "                                             snippet  \n",
       "0  * \"although a Spencer is a greater threat i st...  \n",
       "1  * \"although a Spencer is a greater threat i st...  \n",
       "2  * \"although a Spencer is a greater threat i st...  \n",
       "3  * \"although a Spencer is a greater threat i st...  \n",
       "4  * *Henry turns to his brother, letting out a s...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: _local/machine_wip/entity_mentions_v0_20260210_185146.csv\n"
     ]
    }
   ],
   "source": [
    "# Phase 7b: Entity linking (v0)\n",
    "# Input:  CHUNKS_V0, VOCAB_DF, RUN_STAMP_7, WORKING_DRAFTS_PATH\n",
    "# Output: ENTITY_MENTIONS_V0 (DataFrame) + CSV written to WORKING_DRAFTS_PATH\n",
    "LAST_PHASE_RUN = \"7b\"\n",
    "\n",
    "EXCLUDE_SOURCE_TYPES = {\"auto_transcripts\"}\n",
    "MAX_SNIPPET_CHARS = 220\n",
    "\n",
    "vocabs = VOCAB_DF[\"vocab\"].tolist()\n",
    "entity_ids = VOCAB_DF[\"entity_id\"].tolist()\n",
    "canonicals = VOCAB_DF[\"canonical\"].tolist()\n",
    "match_kinds = VOCAB_DF[\"match_kind\"].tolist()\n",
    "patterns = VOCAB_DF[\"pattern\"].tolist()\n",
    "\n",
    "rows = []\n",
    "mention_id = 1\n",
    "\n",
    "for chunk in CHUNKS_V0:\n",
    "    source_type = chunk.get(\"source_type\", \"unknown\")\n",
    "    if source_type in EXCLUDE_SOURCE_TYPES:\n",
    "        continue\n",
    "\n",
    "    chunk_id = chunk.get(\"chunk_id\")\n",
    "    source_id = chunk.get(\"source_id\")\n",
    "    path = chunk.get(\"path\")\n",
    "    relpath = chunk.get(\"relpath\", \"\")  # <- use what Phase 2/3/5 already computed\n",
    "\n",
    "    lines = chunk.get(\"lines\", [])\n",
    "    header_kind = chunk.get(\"header_kind\")\n",
    "\n",
    "    # Drop pbp/session header line (metadata, not narrative)\n",
    "    if header_kind in {\"pbp_hash\", \"pbp_forum\", \"session\"} and lines:\n",
    "        content_lines = lines[1:]\n",
    "        content_start_line = (chunk.get(\"start_line\") or 1) + 1\n",
    "    else:\n",
    "        content_lines = lines\n",
    "        content_start_line = chunk.get(\"start_line\") or 1\n",
    "\n",
    "    concat_text = \" \".join(\" \".join(content_lines).split())\n",
    "    if not concat_text:\n",
    "        continue\n",
    "\n",
    "    if len(concat_text) > MAX_SNIPPET_CHARS:\n",
    "        snippet = concat_text[: MAX_SNIPPET_CHARS - 3] + \"...\"\n",
    "    else:\n",
    "        snippet = concat_text\n",
    "\n",
    "    # Longest-first matching with span masking\n",
    "    consumed = [False] * len(concat_text)\n",
    "\n",
    "    for vocab, entity_id, canonical, match_kind, pat in zip(\n",
    "        vocabs, entity_ids, canonicals, match_kinds, patterns\n",
    "    ):\n",
    "        hits = [(m.start(), m.end()) for m in pat.finditer(concat_text)]\n",
    "        if not hits:\n",
    "            continue\n",
    "\n",
    "        kept = []\n",
    "        for a, b in hits:\n",
    "            if a < 0 or b <= a:\n",
    "                continue\n",
    "            if any(consumed[a:b]):\n",
    "                continue\n",
    "            kept.append((a, b))\n",
    "\n",
    "        if not kept:\n",
    "            continue\n",
    "\n",
    "        for a, b in kept:\n",
    "            for k in range(a, b):\n",
    "                consumed[k] = True\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"mention_id\": mention_id,\n",
    "                \"entity_id\": entity_id,\n",
    "                \"canonical\": canonical,\n",
    "                \"matched_vocab\": vocab,\n",
    "                \"match_kind\": match_kind,\n",
    "                \"match_count_in_chunk\": len(kept),\n",
    "        \n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_id\": source_id,\n",
    "                \"source_type\": source_type,\n",
    "                \"path\": str(path),\n",
    "                \"relpath\": relpath,\n",
    "                \"chunk_start_line\": chunk.get(\"start_line\") or 1,\n",
    "                \"chunk_end_line\": chunk.get(\"end_line\"),\n",
    "                \"header_kind\": header_kind,\n",
    "        \n",
    "                \"content_start_line\": content_start_line,\n",
    "                \"snippet\": snippet,\n",
    "            }\n",
    "        )\n",
    "        mention_id += 1\n",
    "\n",
    "ENTITY_MENTIONS_V0 = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"Chunks scanned: {sum(1 for c in CHUNKS_V0 if c.get('source_type') not in EXCLUDE_SOURCE_TYPES)}\")\n",
    "print(f\"Entity mentions (rows): {len(ENTITY_MENTIONS_V0)}\")\n",
    "display(ENTITY_MENTIONS_V0.head(5))\n",
    "\n",
    "out_dir = Path(WORKING_DRAFTS_PATH)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_name = f\"entity_mentions_v0_{RUN_STAMP_7}.csv\"\n",
    "csv_path = out_dir / csv_name\n",
    "ENTITY_MENTIONS_V0.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote: {WORKING_DRAFTS_RELPATH}/{csv_name}\")\n",
    "\n",
    "# Cleanup locals (keep ENTITY_MENTIONS_V0)\n",
    "del EXCLUDE_SOURCE_TYPES, MAX_SNIPPET_CHARS\n",
    "del vocabs, entity_ids, canonicals, match_kinds, patterns\n",
    "del rows, mention_id\n",
    "del chunk, source_type, chunk_id, source_id, path, relpath, lines, header_kind\n",
    "del content_lines, content_start_line, concat_text, snippet, consumed\n",
    "del vocab, entity_id, canonical, match_kind, pat, hits, kept, a, b, k\n",
    "del out_dir, csv_name, csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b3fd469-6b97-49ea-8e2c-10c820920938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PbP chunks scanned: 860\n",
      "Author mentions (rows): 557\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention_id</th>\n",
       "      <th>player_entity_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>matched_vocab</th>\n",
       "      <th>match_kind</th>\n",
       "      <th>match_count_in_chunk</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_type</th>\n",
       "      <th>path</th>\n",
       "      <th>relpath</th>\n",
       "      <th>chunk_start_line</th>\n",
       "      <th>chunk_end_line</th>\n",
       "      <th>header_kind</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>player_venric</td>\n",
       "      <td>Venric</td>\n",
       "      <td>Shworn</td>\n",
       "      <td>author_header</td>\n",
       "      <td>1</td>\n",
       "      <td>168646</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>* ### **Shworn** **7/3/25, 6:12 PM**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>player_bysickle</td>\n",
       "      <td>Bysickle</td>\n",
       "      <td>Bysickle</td>\n",
       "      <td>author_header</td>\n",
       "      <td>1</td>\n",
       "      <td>168647</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>* ### **Bysickle** **7/3/25, 6:46 PM**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>player_charis</td>\n",
       "      <td>Charis</td>\n",
       "      <td>Charis</td>\n",
       "      <td>author_header</td>\n",
       "      <td>1</td>\n",
       "      <td>168648</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>* ### **Charis** **7/3/25, 6:53 PM**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>player_crowe</td>\n",
       "      <td>Crowe</td>\n",
       "      <td>CroweTheDualityKing</td>\n",
       "      <td>author_header</td>\n",
       "      <td>1</td>\n",
       "      <td>168649</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>* ### **CroweTheDualityKing** **7/3/25, 6:54 PM**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>player_charis</td>\n",
       "      <td>Charis</td>\n",
       "      <td>Charis</td>\n",
       "      <td>author_header</td>\n",
       "      <td>1</td>\n",
       "      <td>168650</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>/Users/charissophia/obsidian/Iron Wolf Trading...</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>pbp_hash</td>\n",
       "      <td>* ### **Charis** **7/3/25, 6:58 PM**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mention_id player_entity_id canonical        matched_vocab     match_kind  \\\n",
       "0           1    player_venric    Venric               Shworn  author_header   \n",
       "1           2  player_bysickle  Bysickle             Bysickle  author_header   \n",
       "2           3    player_charis    Charis               Charis  author_header   \n",
       "3           4     player_crowe     Crowe  CroweTheDualityKing  author_header   \n",
       "4           5    player_charis    Charis               Charis  author_header   \n",
       "\n",
       "   match_count_in_chunk  chunk_id  source_id      source_type  \\\n",
       "0                     1    168646        105  pbp_transcripts   \n",
       "1                     1    168647        105  pbp_transcripts   \n",
       "2                     1    168648        105  pbp_transcripts   \n",
       "3                     1    168649        105  pbp_transcripts   \n",
       "4                     1    168650        105  pbp_transcripts   \n",
       "\n",
       "                                                path  \\\n",
       "0  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "1  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "2  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "3  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "4  /Users/charissophia/obsidian/Iron Wolf Trading...   \n",
       "\n",
       "                                             relpath  chunk_start_line  \\\n",
       "0  _local/pbp_transcripts/PbP10 - The Second Camp.md                 1   \n",
       "1  _local/pbp_transcripts/PbP10 - The Second Camp.md                 6   \n",
       "2  _local/pbp_transcripts/PbP10 - The Second Camp.md                11   \n",
       "3  _local/pbp_transcripts/PbP10 - The Second Camp.md                16   \n",
       "4  _local/pbp_transcripts/PbP10 - The Second Camp.md                21   \n",
       "\n",
       "   chunk_end_line header_kind  \\\n",
       "0               5    pbp_hash   \n",
       "1              10    pbp_hash   \n",
       "2              15    pbp_hash   \n",
       "3              20    pbp_hash   \n",
       "4              25    pbp_hash   \n",
       "\n",
       "                                             snippet  \n",
       "0               * ### **Shworn** **7/3/25, 6:12 PM**  \n",
       "1             * ### **Bysickle** **7/3/25, 6:46 PM**  \n",
       "2               * ### **Charis** **7/3/25, 6:53 PM**  \n",
       "3  * ### **CroweTheDualityKing** **7/3/25, 6:54 PM**  \n",
       "4               * ### **Charis** **7/3/25, 6:58 PM**  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: _local/machine_wip/author_mentions_v0_20260210_185146.csv\n"
     ]
    }
   ],
   "source": [
    "# Phase 7c: PbP authorship linking (v0)\n",
    "# Input:\n",
    "#   - CHUNKS_V0 (list[dict]) with header_kind + lines + relpath/path\n",
    "#   - author_aliases_df (author -> player_entity_id, optional ambig_char_id)\n",
    "#   - entities_df (entity_id -> canonical)\n",
    "#   - RUN_STAMP_7, WORKING_DRAFTS_PATH (+ WORKING_DRAFTS_RELPATH if present)\n",
    "# Output:\n",
    "#   - AUTHOR_MENTIONS_V0 (DataFrame)\n",
    "#   - CSV written to WORKING_DRAFTS_PATH\n",
    "LAST_PHASE_RUN = \"7c\"\n",
    "\n",
    "EXCLUDE_SOURCE_TYPES = {\"auto_transcripts\"}\n",
    "\n",
    "# Discord-style PbP header (as chunked in Phase 5): \"* ### **Author** **timestamp**\"\n",
    "PBP_HASH_HEADER_REGEX = re.compile(\n",
    "    r\"^\\s*(?:\\d+\\.\\s*)?(?:[*-]\\s*)?###\\s+\\*\\*(?P<author>[^*]+?)\\*\\*\\s+\\*\\*(?P<ts>[^*]+?)\\*\\*\\s*$\"\n",
    ")\n",
    "\n",
    "# --- lookups ---\n",
    "player_canon_by_id = dict(zip(entities_df[\"entity_id\"], entities_df[\"canonical\"]))\n",
    "\n",
    "author_to_player = {}\n",
    "\n",
    "if \"author_aliases_df\" in globals() and isinstance(author_aliases_df, pd.DataFrame) and not author_aliases_df.empty:\n",
    "    # tolerate missing cols gracefully (but 7a should have normalized these already)\n",
    "    if {\"author\", \"player_entity_id\"} <= set(author_aliases_df.columns):\n",
    "        for _, r in author_aliases_df.iterrows():\n",
    "            a = \" \".join(str(r.get(\"author\", \"\")).split())\n",
    "            p = \" \".join(str(r.get(\"player_entity_id\", \"\")).split())\n",
    "            if a and p:\n",
    "                author_to_player[a] = p\n",
    "            del a, p\n",
    "        del r\n",
    "\n",
    "# --- link headers -> players ---\n",
    "rows = []\n",
    "mention_id = 1\n",
    "\n",
    "for chunk in CHUNKS_V0:\n",
    "    source_type = chunk.get(\"source_type\", \"unknown\")\n",
    "    if source_type in EXCLUDE_SOURCE_TYPES:\n",
    "        continue\n",
    "\n",
    "    if chunk.get(\"header_kind\") != \"pbp_hash\":\n",
    "        continue\n",
    "\n",
    "    lines = chunk.get(\"lines\") or []\n",
    "    if not lines:\n",
    "        continue\n",
    "\n",
    "    header_line = str(lines[0]).strip()\n",
    "    if not header_line:\n",
    "        continue\n",
    "\n",
    "    m = PBP_HASH_HEADER_REGEX.match(header_line)\n",
    "    if not m:\n",
    "        # header_kind says pbp_hash but regex didn't match; skip (or could emit an error row later)\n",
    "        continue\n",
    "\n",
    "    author = \" \".join(m.group(\"author\").split())\n",
    "\n",
    "    player_entity_id = author_to_player.get(author, \"\")\n",
    "    canonical = player_canon_by_id.get(player_entity_id, \"\") if player_entity_id else \"\"\n",
    "\n",
    "    match_kind = \"author_header\" if player_entity_id else \"author_header_unmapped\"\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"mention_id\": mention_id,\n",
    "            \"player_entity_id\": player_entity_id,\n",
    "            \"canonical\": canonical,\n",
    "            \"matched_vocab\": author,\n",
    "            \"match_kind\": match_kind,\n",
    "            \"match_count_in_chunk\": 1,\n",
    "            \"chunk_id\": chunk.get(\"chunk_id\"),\n",
    "            \"source_id\": chunk.get(\"source_id\"),\n",
    "            \"source_type\": source_type,\n",
    "            \"path\": str(chunk.get(\"path\", \"\")),\n",
    "            \"relpath\": str(chunk.get(\"relpath\", \"\")),\n",
    "            \"chunk_start_line\": chunk.get(\"start_line\") or 1,\n",
    "            \"chunk_end_line\": chunk.get(\"end_line\"),\n",
    "            \"header_kind\": chunk.get(\"header_kind\"),\n",
    "            \"snippet\": header_line,\n",
    "        }\n",
    "    )\n",
    "    mention_id += 1\n",
    "\n",
    "AUTHOR_MENTIONS_V0 = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"PbP chunks scanned: {sum(1 for c in CHUNKS_V0 if c.get('source_type') not in EXCLUDE_SOURCE_TYPES and c.get('header_kind') == 'pbp_hash')}\")\n",
    "print(f\"Author mentions (rows): {len(AUTHOR_MENTIONS_V0)}\")\n",
    "display(AUTHOR_MENTIONS_V0.head(5))\n",
    "\n",
    "out_dir = Path(WORKING_DRAFTS_PATH)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_name = f\"author_mentions_v0_{RUN_STAMP_7}.csv\"\n",
    "csv_path = out_dir / csv_name\n",
    "AUTHOR_MENTIONS_V0.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "drafts_rel = globals().get(\"WORKING_DRAFTS_RELPATH\", str(out_dir))\n",
    "print(f\"Wrote: {drafts_rel.rstrip('/')}/{csv_name}\")\n",
    "\n",
    "# Cleanup locals (keep AUTHOR_MENTIONS_V0)\n",
    "del RUN_STAMP_7, EXCLUDE_SOURCE_TYPES, PBP_HASH_HEADER_REGEX, CASE_INSENSITIVE\n",
    "del player_canon_by_id, author_to_player\n",
    "del rows, mention_id, chunk, source_type, lines, header_line\n",
    "del m, author, player_entity_id, canonical, match_kind\n",
    "del out_dir, csv_name, csv_path, drafts_rel\n",
    "del aliases_df, author_aliases_df, entities_df, pc_map_df\n",
    "del re, pd, Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691acd11-ea55-41a1-8e03-f33dabf54db3",
   "metadata": {},
   "source": [
    "## Phase 8: Index materialization and persistence\n",
    "\n",
    "In this phase, the notebook transitions from **mention detection** to **index construction**.\n",
    "\n",
    "At the end of Phase 7, the corpus has been fully scanned against curated vocabularies, producing structured mention tables for both entities and authors. These tables precisely record *where* known concepts and speakers appear, but they are still raw observations rather than navigable indexes.\n",
    "\n",
    "Phase 8 is responsible for:\n",
    "- Transforming mention tables into stable, queryable index structures\n",
    "- Aggregating mentions across chunks, files, and source types\n",
    "- Normalizing outputs for long-term storage\n",
    "- Writing durable index artifacts suitable for reuse outside the notebook\n",
    "\n",
    "This phase answers the question:\n",
    "\n",
    "**“How do we turn detected mentions into usable indexes?”**\n",
    "\n",
    "Phase 8 does **not** introduce new interpretation, inference, or vocabulary. It operates entirely on the outputs of earlier phases, focusing on organization, aggregation, and persistence.\n",
    "\n",
    "The outputs of this phase are intended to be:\n",
    "- Stable across notebook runs\n",
    "- Decoupled from in-memory state\n",
    "- Suitable for downstream querying, visualization, and graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34541471-2594-4960-8599-8c9f524cb388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 8 wrote index artifacts:\n",
      " - _local/machine_wip/index_entity_to_chunks_v0.csv\n",
      " - _local/machine_wip/index_chunk_to_entities_v0.csv\n",
      " - _local/machine_wip/index_source_files_v0.csv\n",
      " - _local/machine_wip/index_player_to_chunks_v0.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>chunk_ids</th>\n",
       "      <th>chunk_count</th>\n",
       "      <th>file_relpaths</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person_victor</td>\n",
       "      <td>Victor D Evernight</td>\n",
       "      <td>168646|168650|168653|168654|168657|168658|1686...</td>\n",
       "      <td>391</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person_henry</td>\n",
       "      <td>Henry Sleepsong</td>\n",
       "      <td>168647|168673|168675|168682|168684|168687|1686...</td>\n",
       "      <td>327</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person_alivyre</td>\n",
       "      <td>Alivyre Dawntracker</td>\n",
       "      <td>168648|168654|168665|168687|168688|168689|1686...</td>\n",
       "      <td>264</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person_faeryne</td>\n",
       "      <td>Faeryne</td>\n",
       "      <td>168660|168661|168662|168665|168677|168683|1686...</td>\n",
       "      <td>255</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org_party</td>\n",
       "      <td>Party</td>\n",
       "      <td>168706|168710|168742|168748|168768|168771|1687...</td>\n",
       "      <td>167</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity_id            canonical  \\\n",
       "0   person_victor   Victor D Evernight   \n",
       "1    person_henry      Henry Sleepsong   \n",
       "2  person_alivyre  Alivyre Dawntracker   \n",
       "3  person_faeryne              Faeryne   \n",
       "4       org_party                Party   \n",
       "\n",
       "                                           chunk_ids  chunk_count  \\\n",
       "0  168646|168650|168653|168654|168657|168658|1686...          391   \n",
       "1  168647|168673|168675|168682|168684|168687|1686...          327   \n",
       "2  168648|168654|168665|168687|168688|168689|1686...          264   \n",
       "3  168660|168661|168662|168665|168677|168683|1686...          255   \n",
       "4  168706|168710|168742|168748|168768|168771|1687...          167   \n",
       "\n",
       "                                       file_relpaths  file_count  \n",
       "0  _local/pbp_transcripts/PbP10 - The Second Camp...          18  \n",
       "1  _local/pbp_transcripts/PbP10 - The Second Camp...          20  \n",
       "2  _local/pbp_transcripts/PbP10 - The Second Camp...          20  \n",
       "3  _local/pbp_transcripts/PbP10 - The Second Camp...          19  \n",
       "4  _local/pbp_transcripts/PbP10 - The Second Camp...          21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_type</th>\n",
       "      <th>relpath</th>\n",
       "      <th>chunk_start_line</th>\n",
       "      <th>chunk_end_line</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>canonicals</th>\n",
       "      <th>entity_count</th>\n",
       "      <th>matched_vocabs</th>\n",
       "      <th>match_kinds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168646</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>faction_spencer|person_piers|person_shworn|per...</td>\n",
       "      <td>Piers Ashton|Shworn Sleepsong|Spencer|Victor D...</td>\n",
       "      <td>4</td>\n",
       "      <td>Piers|Shworn|Spencer|Victor</td>\n",
       "      <td>alias|canonical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168647</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>person_henry|person_shworn</td>\n",
       "      <td>Henry Sleepsong|Shworn Sleepsong</td>\n",
       "      <td>2</td>\n",
       "      <td>Henry|Shworn</td>\n",
       "      <td>alias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168648</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>person_alivyre</td>\n",
       "      <td>Alivyre Dawntracker</td>\n",
       "      <td>1</td>\n",
       "      <td>Alivyre</td>\n",
       "      <td>alias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168649</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>person_piers</td>\n",
       "      <td>Piers Ashton</td>\n",
       "      <td>1</td>\n",
       "      <td>Piers</td>\n",
       "      <td>alias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168650</td>\n",
       "      <td>105</td>\n",
       "      <td>pbp_transcripts</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp.md</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>person_piers|person_victor</td>\n",
       "      <td>Piers Ashton|Victor D Evernight</td>\n",
       "      <td>2</td>\n",
       "      <td>Piers|Victor</td>\n",
       "      <td>alias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id  source_id      source_type  \\\n",
       "0    168646        105  pbp_transcripts   \n",
       "1    168647        105  pbp_transcripts   \n",
       "2    168648        105  pbp_transcripts   \n",
       "3    168649        105  pbp_transcripts   \n",
       "4    168650        105  pbp_transcripts   \n",
       "\n",
       "                                             relpath  chunk_start_line  \\\n",
       "0  _local/pbp_transcripts/PbP10 - The Second Camp.md                 1   \n",
       "1  _local/pbp_transcripts/PbP10 - The Second Camp.md                 6   \n",
       "2  _local/pbp_transcripts/PbP10 - The Second Camp.md                11   \n",
       "3  _local/pbp_transcripts/PbP10 - The Second Camp.md                16   \n",
       "4  _local/pbp_transcripts/PbP10 - The Second Camp.md                21   \n",
       "\n",
       "   chunk_end_line                                         entity_ids  \\\n",
       "0               5  faction_spencer|person_piers|person_shworn|per...   \n",
       "1              10                         person_henry|person_shworn   \n",
       "2              15                                     person_alivyre   \n",
       "3              20                                       person_piers   \n",
       "4              25                         person_piers|person_victor   \n",
       "\n",
       "                                          canonicals  entity_count  \\\n",
       "0  Piers Ashton|Shworn Sleepsong|Spencer|Victor D...             4   \n",
       "1                   Henry Sleepsong|Shworn Sleepsong             2   \n",
       "2                                Alivyre Dawntracker             1   \n",
       "3                                       Piers Ashton             1   \n",
       "4                    Piers Ashton|Victor D Evernight             2   \n",
       "\n",
       "                matched_vocabs      match_kinds  \n",
       "0  Piers|Shworn|Spencer|Victor  alias|canonical  \n",
       "1                 Henry|Shworn            alias  \n",
       "2                      Alivyre            alias  \n",
       "3                        Piers            alias  \n",
       "4                 Piers|Victor            alias  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_entity_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>chunk_ids</th>\n",
       "      <th>chunk_count</th>\n",
       "      <th>file_relpaths</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>player_charis</td>\n",
       "      <td>Charis</td>\n",
       "      <td>168648|168650|168654|168655|168656|168658|1686...</td>\n",
       "      <td>229</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>player_crowe</td>\n",
       "      <td>Crowe</td>\n",
       "      <td>168649|168652|168657|168663|168680|168697|1686...</td>\n",
       "      <td>193</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player_bysickle</td>\n",
       "      <td>Bysickle</td>\n",
       "      <td>168647|168675|168681|168683|168689|168695|1687...</td>\n",
       "      <td>57</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>player_kalina</td>\n",
       "      <td>Kalina</td>\n",
       "      <td>168653|168659|168661|168722|168724|168726|1687...</td>\n",
       "      <td>42</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>player_venric</td>\n",
       "      <td>Venric</td>\n",
       "      <td>168646|168651|168666|168668|168670|168672|1686...</td>\n",
       "      <td>28</td>\n",
       "      <td>_local/pbp_transcripts/PbP10 - The Second Camp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  player_entity_id canonical  \\\n",
       "0    player_charis    Charis   \n",
       "1     player_crowe     Crowe   \n",
       "2  player_bysickle  Bysickle   \n",
       "3    player_kalina    Kalina   \n",
       "4    player_venric    Venric   \n",
       "\n",
       "                                           chunk_ids  chunk_count  \\\n",
       "0  168648|168650|168654|168655|168656|168658|1686...          229   \n",
       "1  168649|168652|168657|168663|168680|168697|1686...          193   \n",
       "2  168647|168675|168681|168683|168689|168695|1687...           57   \n",
       "3  168653|168659|168661|168722|168724|168726|1687...           42   \n",
       "4  168646|168651|168666|168668|168670|168672|1686...           28   \n",
       "\n",
       "                                       file_relpaths  file_count  \n",
       "0  _local/pbp_transcripts/PbP10 - The Second Camp...           6  \n",
       "1  _local/pbp_transcripts/PbP10 - The Second Camp...           6  \n",
       "2  _local/pbp_transcripts/PbP10 - The Second Camp...           6  \n",
       "3  _local/pbp_transcripts/PbP10 - The Second Camp...           6  \n",
       "4  _local/pbp_transcripts/PbP10 - The Second Camp...           3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 8 complete: index artifacts generated in working_drafts.\n",
      "\n",
      "Next steps:\n",
      "1) Review the generated CSV files in:\n",
      "   _local/machine_wip\n",
      "\n",
      "   Files created:\n",
      "   - index_entity_to_chunks_v0.csv\n",
      "   - index_chunk_to_entities_v0.csv\n",
      "   - index_player_to_chunks_v0.csv\n",
      "\n",
      "2) If satisfied, move (or copy) these files into your canonical indexes directory, e.g.:\n",
      "   _meta/indexes/\n",
      "\n",
      "3) Update your world_repository.yml to declare the indexes block, for example:\n",
      "\n",
      "indexes:\n",
      "  # Canonical machine-generated index artifacts.\n",
      "  # These are reproducible but intended to be version-controlled.\n",
      "  # Relative paths are resolved under world_root unless absolute.\n",
      "  path: _meta/indexes\n",
      "\n",
      "\n",
      "4) Commit the index files and updated descriptor to version control.\n",
      "\n",
      "This notebook does NOT modify canonical data. All artifacts were written to working_drafts.\n",
      "After committing indexes, you may start a new notebook that consumes them.\n"
     ]
    }
   ],
   "source": [
    "# Phase 8: Build index artifacts (v0) - simplified\n",
    "# Output (written to WORKING_DRAFTS_PATH only):\n",
    "#   - INDEX_ENTITY_TO_CHUNKS_V0 (entity_id -> chunks/files)\n",
    "#   - INDEX_CHUNK_TO_ENTITIES_V0 (chunk_id -> entities)\n",
    "#   - INDEX_PLAYER_TO_CHUNKS_V0 (player_entity_id -> chunks/files) [if AUTHOR_MENTIONS_V0 present]\n",
    "#   - SOURCE_FILES_DF (source_id/relpath/source_type)\n",
    "LAST_PHASE_RUN = \"8\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Validate inputs (presence only)\n",
    "# ------------------------------------------------------------------\n",
    "if \"ENTITY_MENTIONS_V0\" not in globals() or ENTITY_MENTIONS_V0 is None or ENTITY_MENTIONS_V0.empty:\n",
    "    raise ValueError(\"ENTITY_MENTIONS_V0 is missing or empty. Run Phase 7 first.\")\n",
    "\n",
    "if \"CHUNKS_V0\" not in globals() or not CHUNKS_V0:\n",
    "    raise ValueError(\"CHUNKS_V0 is missing or empty. Run Phase 5 first.\")\n",
    "\n",
    "has_author_mentions = (\"AUTHOR_MENTIONS_V0\" in globals()) and (AUTHOR_MENTIONS_V0 is not None) and (not AUTHOR_MENTIONS_V0.empty)\n",
    "\n",
    "if \"WORKING_DRAFTS_PATH\" not in globals() or not WORKING_DRAFTS_PATH:\n",
    "    raise ValueError(\"WORKING_DRAFTS_PATH is missing. Rerun Phase 1.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CHUNKS_DF -> SOURCE_FILES_DF (file = source file: source_id + relpath)\n",
    "# ------------------------------------------------------------------\n",
    "CHUNKS_DF = pd.DataFrame(CHUNKS_V0)\n",
    "\n",
    "SOURCE_FILES_DF = (\n",
    "    CHUNKS_DF[[\"source_id\", \"relpath\", \"source_type\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Small inline helper\n",
    "# ------------------------------------------------------------------\n",
    "def _uniq_sorted(series):\n",
    "    vals = [x for x in series.dropna().astype(str).tolist() if x != \"\"]\n",
    "    return sorted(set(vals))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8a) Entity -> chunks/files\n",
    "# ------------------------------------------------------------------\n",
    "INDEX_ENTITY_TO_CHUNKS_V0 = (\n",
    "    ENTITY_MENTIONS_V0.groupby([\"entity_id\", \"canonical\"], dropna=False)\n",
    "      .agg(\n",
    "          chunk_ids=(\"chunk_id\", lambda s: \"|\".join([str(x) for x in _uniq_sorted(s)])),\n",
    "          chunk_count=(\"chunk_id\", lambda s: len(set(s.tolist()))),\n",
    "          file_relpaths=(\"relpath\", lambda s: \"|\".join(_uniq_sorted(s))),\n",
    "          file_count=(\"relpath\", lambda s: len(set([x for x in s.tolist() if pd.notna(x) and str(x) != \"\"]))),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"chunk_count\", \"file_count\", \"entity_id\"], ascending=[False, False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8b) Chunk -> entities\n",
    "# ------------------------------------------------------------------\n",
    "INDEX_CHUNK_TO_ENTITIES_V0 = (\n",
    "    ENTITY_MENTIONS_V0.groupby(\n",
    "        [\"chunk_id\", \"source_id\", \"source_type\", \"relpath\", \"chunk_start_line\", \"chunk_end_line\"],\n",
    "        dropna=False\n",
    "    )\n",
    "    .agg(\n",
    "        entity_ids=(\"entity_id\", lambda s: \"|\".join(_uniq_sorted(s))),\n",
    "        canonicals=(\"canonical\", lambda s: \"|\".join(_uniq_sorted(s))),\n",
    "        entity_count=(\"entity_id\", lambda s: len(set([x for x in s.tolist() if pd.notna(x) and str(x) != \"\"]))),\n",
    "        matched_vocabs=(\"matched_vocab\", lambda s: \"|\".join(_uniq_sorted(s))),\n",
    "        match_kinds=(\"match_kind\", lambda s: \"|\".join(_uniq_sorted(s))),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"chunk_id\"], ascending=[True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8c) Player -> chunks/files (PbP authorship)\n",
    "# ------------------------------------------------------------------\n",
    "INDEX_PLAYER_TO_CHUNKS_V0 = pd.DataFrame()\n",
    "if has_author_mentions:\n",
    "    INDEX_PLAYER_TO_CHUNKS_V0 = (\n",
    "        AUTHOR_MENTIONS_V0.groupby([\"player_entity_id\", \"canonical\"], dropna=False)\n",
    "          .agg(\n",
    "              chunk_ids=(\"chunk_id\", lambda s: \"|\".join([str(x) for x in _uniq_sorted(s)])),\n",
    "              chunk_count=(\"chunk_id\", lambda s: len(set(s.tolist()))),\n",
    "              file_relpaths=(\"relpath\", lambda s: \"|\".join(_uniq_sorted(s))),\n",
    "              file_count=(\"relpath\", lambda s: len(set([x for x in s.tolist() if pd.notna(x) and str(x) != \"\"]))),\n",
    "          )\n",
    "          .reset_index()\n",
    "          .sort_values([\"chunk_count\", \"file_count\", \"player_entity_id\"], ascending=[False, False, True])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8d) Write artifacts to WORKING_DRAFTS_PATH\n",
    "# ------------------------------------------------------------------\n",
    "out_dir = Path(WORKING_DRAFTS_PATH)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_rel = globals().get(\"WORKING_DRAFTS_RELPATH\", str(out_dir)).rstrip(\"/\")\n",
    "\n",
    "INDEX_ENTITY_TO_CHUNKS_V0.to_csv(out_dir / \"index_entity_to_chunks_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "INDEX_CHUNK_TO_ENTITIES_V0.to_csv(out_dir / \"index_chunk_to_entities_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "SOURCE_FILES_DF.to_csv(out_dir / \"index_source_files_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "if has_author_mentions:\n",
    "    INDEX_PLAYER_TO_CHUNKS_V0.to_csv(out_dir / \"index_player_to_chunks_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Phase 8 wrote index artifacts:\")\n",
    "print(f\" - {out_rel}/index_entity_to_chunks_v0.csv\")\n",
    "print(f\" - {out_rel}/index_chunk_to_entities_v0.csv\")\n",
    "print(f\" - {out_rel}/index_source_files_v0.csv\")\n",
    "if has_author_mentions:\n",
    "    print(f\" - {out_rel}/index_player_to_chunks_v0.csv\")\n",
    "\n",
    "display(INDEX_ENTITY_TO_CHUNKS_V0.head(5))\n",
    "display(INDEX_CHUNK_TO_ENTITIES_V0.head(5))\n",
    "if has_author_mentions:\n",
    "    display(INDEX_PLAYER_TO_CHUNKS_V0.head(5))\n",
    "\n",
    "print(\"\\nPhase 8 complete: index artifacts generated in working_drafts.\\n\")\n",
    "\n",
    "print(\"Next steps:\")\n",
    "print(\"1) Review the generated CSV files in:\")\n",
    "print(f\"   {WORKING_DRAFTS_RELPATH}\")\n",
    "\n",
    "print(\"\\n   Files created:\")\n",
    "print(\"   - index_entity_to_chunks_v0.csv\")\n",
    "print(\"   - index_chunk_to_entities_v0.csv\")\n",
    "if has_author_mentions:\n",
    "    print(\"   - index_player_to_chunks_v0.csv\")\n",
    "\n",
    "print(\"\\n2) If satisfied, move (or copy) these files into your canonical indexes directory, e.g.:\")\n",
    "print(\"   _meta/indexes/\")\n",
    "\n",
    "print(\"\\n3) Update your world_repository.yml to declare the indexes block, for example:\")\n",
    "print(\"\"\"\n",
    "indexes:\n",
    "  # Canonical machine-generated index artifacts.\n",
    "  # These are reproducible but intended to be version-controlled.\n",
    "  # Relative paths are resolved under world_root unless absolute.\n",
    "  path: _meta/indexes\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4) Commit the index files and updated descriptor to version control.\")\n",
    "\n",
    "print(\"\\nThis notebook does NOT modify canonical data. All artifacts were written to working_drafts.\")\n",
    "print(\"After committing indexes, you may start a new notebook that consumes them.\")\n",
    "\n",
    "# Cleanup locals (keep index DataFrames)\n",
    "del out_dir, out_rel, has_author_mentions\n",
    "del CHUNKS_DF, SOURCE_FILES_DF\n",
    "del Path, pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac67b4-4201-4f8e-8d73-a7a35d583418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iwtc-tools)",
   "language": "python",
   "name": "iwtc-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
