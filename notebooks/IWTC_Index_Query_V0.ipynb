{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df40db7-15a3-4b78-a8c9-fb9d3582680b",
   "metadata": {},
   "source": [
    "# IWTC Index Querying (v0)\n",
    "\n",
    "This notebook executes the index querying workflow defined in:\n",
    "\n",
    "- `docs/index_querying_design.md`\n",
    "\n",
    "It is intended for hands-on execution and experimentation. Conceptual scope, responsibilities, and query pattern design are defined in the linked design document.\n",
    "\n",
    "This notebook operates on a single world repository.\n",
    "\n",
    "A minimal example of `world_repository.yml` is provided in this repository\n",
    "under:\n",
    "\n",
    "- `data/config_examples/world_repository.yml`\n",
    "\n",
    "You may copy and adapt that example for your own world repository.\n",
    "\n",
    "This notebook loads raw index CSV artifacts generated by the prior indexing notebook:\n",
    "- `INDEX_ENTITY_TO_CHUNKS_V0`\n",
    "- `INDEX_CHUNK_TO_ENTITIES_V0`\n",
    "- `INDEX_PLAYER_TO_CHUNKS_V0`\n",
    "- `INDEX_SOURCE_FILES_V0`\n",
    "\n",
    "No normalization, schema changes, or canonical file modifications are performed in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5a1b2-02a1-4f7b-893b-a49f97d649df",
   "metadata": {},
   "source": [
    "## Phase 0: Parameters\n",
    "\n",
    "This notebook operates on a **campaign world repository** and loads previously\n",
    "generated index artifacts for interactive querying.\n",
    "\n",
    "In this phase, you tell the notebook:\n",
    "\n",
    "- Which world repository it is operating on.\n",
    "- Which index artifact version it should expect to load.\n",
    "\n",
    "This notebook does **not** generate new indexes.\n",
    "It does **not** modify canonical files.\n",
    "It does **not** alter schema.\n",
    "\n",
    "The goal is simply to answer:\n",
    "*\"What indexed world am I querying right now?\"*\n",
    "\n",
    "The code cell below contains inline comments explaining each parameter.\n",
    "\n",
    "**IMPORTANT:** This notebook assumes index artifacts already exist and will fail\n",
    "if required CSV files are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2006ad16-9ab8-4d7d-8c73-2907148f0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run initialized at: 2026-02-10 20:11\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Parameters\n",
    "LAST_PHASE_RUN = \"0\"\n",
    "\n",
    "# Absolute path to the world_repository.yml descriptor.\n",
    "WORLD_REPOSITORY_DESCRIPTOR = (\n",
    "    \"/Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/descriptors/world_repository.yml\"\n",
    ")\n",
    "\n",
    "# Index version to load (must match previously generated artifacts)\n",
    "INDEX_VERSION = \"V0\"\n",
    "\n",
    "# Internal run metadata (do not edit)\n",
    "from datetime import datetime\n",
    "print(f\"Notebook run initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "del datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b681fe8-22ed-4702-b4b2-2b4af1b299e7",
   "metadata": {},
   "source": [
    "## Phase 1: Load and validate world descriptor\n",
    "\n",
    "Before this notebook can safely read or write anything, it must be confident that it understands the **structure of the world repository**.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Loads the world repository descriptor file you provided\n",
    "- Confirms that it is readable and structurally valid\n",
    "- Extracts only the information this notebook needs\n",
    "- Verifies that referenced paths actually exist and are usable\n",
    "\n",
    "This phase answers a single question:\n",
    "\n",
    "**“Can I trust this descriptor enough to proceed?”**\n",
    "\n",
    "If the answer is *no*, the notebook will stop with clear, actionable error messages explaining what needs to be fixed in the descriptor file.  \n",
    "Nothing is modified, created, or scanned until this check succeeds.\n",
    "\n",
    "This phase does **not** interpret world lore, indexing rules, or heuristics.  \n",
    "It only establishes that the filesystem layout described by the world is coherent and usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b0f94f-6d0d-4dc4-93c4-3fc52b9b8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World repository descriptor loaded successfully: world_repository.yml\n",
      "Descriptor paths are usable for this notebook.\n",
      "world_root: /Users/charissophia/obsidian/Iron Wolf Trading Company\n",
      "indexes: _meta/indexes\n",
      "vocab.entities: _meta/indexes/vocab_entities.csv (exists=True)\n",
      "vocab.aliases: _meta/indexes/vocab_aliases.csv (exists=True)\n",
      "vocab.author_aliases: _meta/indexes/vocab_author_aliases.csv (exists=True)\n",
      "vocab.player_character_map: _meta/indexes/vocab_map_player_character.csv (exists=True)\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Load and validate world repository descriptor (Index Querying v0)\n",
    "LAST_PHASE_RUN = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# --- Load descriptor file ---\n",
    "descriptor_path = Path(WORLD_REPOSITORY_DESCRIPTOR)\n",
    "\n",
    "if not descriptor_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"World repository descriptor file was not found.\\n\"\n",
    "        f\"Path provided:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Confirm the file exists at this location or fix WORLD_REPOSITORY_DESCRIPTOR in Phase 0\\n\"\n",
    "        \"- If you just edited Phase 0, rerun Phase 0 and then rerun this cell\\n\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    with descriptor_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        world_repo = yaml.safe_load(f)\n",
    "except Exception:\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor could not be read.\\n\"\n",
    "        \"This usually indicates a YAML formatting problem.\\n\\n\"\n",
    "        f\"File:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Paste the contents into https://www.yamllint.com/\\n\"\n",
    "        \"- Fix any reported issues, save the file, and rerun this cell\"\n",
    "    )\n",
    "\n",
    "if not isinstance(world_repo, dict):\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor was read, but its structure is not usable.\\n\"\n",
    "        \"The file must be a YAML mapping (top-level `name: value` entries).\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Ensure it uses clear `name: value` lines\\n\"\n",
    "        \"- Fix the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "print(f\"World repository descriptor loaded successfully: {descriptor_path.name}\")\n",
    "\n",
    "# --- Extract required entries ---\n",
    "WORLD_ROOT_RAW = world_repo.get(\"world_root\")\n",
    "\n",
    "indexes_block = world_repo.get(\"indexes\")\n",
    "INDEXES_RAW = indexes_block.get(\"path\") if isinstance(indexes_block, dict) else None\n",
    "\n",
    "vocab = world_repo.get(\"vocabulary\")\n",
    "ENTITIES_RAW = vocab.get(\"entities\") if isinstance(vocab, dict) else None\n",
    "ALIASES_RAW = vocab.get(\"aliases\") if isinstance(vocab, dict) else None\n",
    "AUTHORS_RAW = vocab.get(\"author_aliases\") if isinstance(vocab, dict) else None\n",
    "PC_MAP_RAW = vocab.get(\"player_character_map\") if isinstance(vocab, dict) else None\n",
    "\n",
    "if not WORLD_ROOT_RAW:\n",
    "    errors.append(\"Missing required entry: world_root\")\n",
    "\n",
    "if not INDEXES_RAW:\n",
    "    errors.append(\"Missing required entry: indexes.path\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"World repository descriptor is missing required entries:\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Edit your world_repository.yml and add/fix the missing entries\\n\"\n",
    "          \"- Save the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "# --- Validate and resolve world_root ---\n",
    "WORLD_ROOT = Path(WORLD_ROOT_RAW)\n",
    "\n",
    "if str(WORLD_ROOT).startswith(\"~\"):\n",
    "    errors.append(\"world_root: '~' is not allowed. Use a full absolute path.\")\n",
    "elif not WORLD_ROOT.is_absolute():\n",
    "    errors.append(\"world_root must be an absolute path (starts with / on macOS/Linux, or C:\\\\ on Windows).\")\n",
    "elif not WORLD_ROOT.is_dir():\n",
    "    errors.append(f\"world_root must be an existing directory: {WORLD_ROOT}\")\n",
    "else:\n",
    "    WORLD_ROOT = WORLD_ROOT.resolve()\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "# --- Resolve and validate indexes path ---\n",
    "INDEXES_PATH = Path(INDEXES_RAW)\n",
    "if not INDEXES_PATH.is_absolute():\n",
    "    INDEXES_PATH = WORLD_ROOT / INDEXES_PATH\n",
    "INDEXES_PATH = INDEXES_PATH.resolve()\n",
    "\n",
    "try:\n",
    "    INDEXES_RELPATH = str(INDEXES_PATH.relative_to(WORLD_ROOT))\n",
    "except Exception:\n",
    "    INDEXES_RELPATH = str(INDEXES_PATH)\n",
    "\n",
    "if not INDEXES_PATH.exists():\n",
    "    errors.append(f\"indexes: path does not exist: {INDEXES_PATH}\")\n",
    "elif not INDEXES_PATH.is_dir():\n",
    "    errors.append(f\"indexes: {INDEXES_PATH} must be a directory\")\n",
    "\n",
    "# --- Resolve vocabulary paths (optional) ---\n",
    "VOCAB_ENTITIES_PATH = None\n",
    "VOCAB_ENTITIES_RELPATH = None\n",
    "VOCAB_ALIASES_PATH = None\n",
    "VOCAB_ALIASES_RELPATH = None\n",
    "VOCAB_AUTHORS_PATH = None\n",
    "VOCAB_AUTHORS_RELPATH = None\n",
    "VOCAB_PC_MAP_PATH = None\n",
    "VOCAB_PC_MAP_RELPATH = None\n",
    "\n",
    "vocab_entries = [\n",
    "    (\"entities\", \"vocab.entities\"),\n",
    "    (\"aliases\", \"vocab.aliases\"),\n",
    "    (\"author_aliases\", \"vocab.author_aliases\"),\n",
    "    (\"player_character_map\", \"vocab.player_character_map\"),\n",
    "]\n",
    "\n",
    "for key, label in vocab_entries:\n",
    "    raw = vocab.get(key)\n",
    "    if not raw:\n",
    "        continue\n",
    "\n",
    "    p = Path(raw)\n",
    "    if not p.is_absolute():\n",
    "        p = WORLD_ROOT / p\n",
    "    p = p.resolve()\n",
    "\n",
    "    try:\n",
    "        rel = str(p.relative_to(WORLD_ROOT))\n",
    "    except Exception:\n",
    "        rel = str(p)\n",
    "\n",
    "    if p.exists() and p.is_dir():\n",
    "        warnings.append(f\"{label}: {p} must be a file (got directory). Ignoring.\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"{label}: file does not exist: {p} (name resolution may be limited).\")\n",
    "\n",
    "    if key == \"entities\":\n",
    "        VOCAB_ENTITIES_PATH = p\n",
    "        VOCAB_ENTITIES_RELPATH = rel\n",
    "    elif key == \"aliases\":\n",
    "        VOCAB_ALIASES_PATH = p\n",
    "        VOCAB_ALIASES_RELPATH = rel\n",
    "    elif key == \"author_aliases\":\n",
    "        VOCAB_AUTHORS_PATH = p\n",
    "        VOCAB_AUTHORS_RELPATH = rel\n",
    "    elif key == \"player_character_map\":\n",
    "        VOCAB_PC_MAP_PATH = p\n",
    "        VOCAB_PC_MAP_RELPATH = rel\n",
    "\n",
    "print(\"Descriptor paths are usable for this notebook.\")\n",
    "print(f\"world_root: {WORLD_ROOT}\")\n",
    "print(f\"indexes: {INDEXES_RELPATH}\")\n",
    "print(f\"vocab.entities: {VOCAB_ENTITIES_RELPATH} (exists={VOCAB_ENTITIES_PATH.exists() if VOCAB_ENTITIES_PATH else False})\")\n",
    "print(f\"vocab.aliases: {VOCAB_ALIASES_RELPATH} (exists={VOCAB_ALIASES_PATH.exists() if VOCAB_ALIASES_PATH else False})\")\n",
    "print(f\"vocab.author_aliases: {VOCAB_AUTHORS_RELPATH} (exists={VOCAB_AUTHORS_PATH.exists() if VOCAB_AUTHORS_PATH else False})\")\n",
    "print(f\"vocab.player_character_map: {VOCAB_PC_MAP_RELPATH} (exists={VOCAB_PC_MAP_PATH.exists() if VOCAB_PC_MAP_PATH else False})\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del yaml, Path, descriptor_path, world_repo, indexes_block, vocab\n",
    "del WORLD_ROOT_RAW, INDEXES_RAW, ENTITIES_RAW, ALIASES_RAW, AUTHORS_RAW, PC_MAP_RAW\n",
    "del vocab_entries, key, label, raw, p, rel, errors, warnings, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd12a7-16be-4694-a3a0-e5dc6c577da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 2: Load index artifacts\n",
    "\n",
    "Before this notebook can execute any queries, it must confirm that the\n",
    "required index artifacts already exist and can be loaded.\n",
    "\n",
    "In this phase, the notebook:\n",
    "\n",
    "- Constructs the expected index artifact filenames based on `INDEX_VERSION`\n",
    "- Confirms those files exist under the repository’s declared `indexes.path`\n",
    "- Loads each artifact as a raw dataframe\n",
    "- Verifies that required columns are present\n",
    "- Publishes stable dataframe variables for downstream query logic\n",
    "\n",
    "This phase answers a single question:\n",
    "\n",
    "**“Are the required index artifacts present and structurally usable?”**\n",
    "\n",
    "If any required artifact is missing or malformed, the notebook will stop\n",
    "with clear instructions explaining how to regenerate them.\n",
    "\n",
    "No canonical files are modified.\n",
    "No schema transformations are performed.\n",
    "No normalization occurs.\n",
    "\n",
    "This phase does not execute queries.\n",
    "It only establishes the concrete, in-memory tables that the query layer will operate on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fe5508-7c3b-4b91-99b3-efc5463ac9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 OK: index artifacts loaded.\n",
      "indexes.path: /Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/indexes\n",
      "index version: v0\n",
      "\n",
      "Loaded tables:\n",
      "- DF_ENTITY_TO_CHUNKS:        168 rows,   6 cols\n",
      "- DF_CHUNK_TO_ENTITIES:      1139 rows,  11 cols\n",
      "- DF_PLAYER_TO_CHUNKS:          6 rows,   6 cols\n",
      "- DF_SOURCE_FILES:            130 rows,   3 cols\n",
      "\n",
      "DF_ENTITY_TO_CHUNKS columns: ['entity_id', 'canonical', 'chunk_ids', 'chunk_count', 'file_relpaths', 'file_count']\n",
      "DF_CHUNK_TO_ENTITIES columns: ['chunk_id', 'source_id', 'source_type', 'relpath', 'chunk_start_line', 'chunk_end_line', 'entity_ids', 'canonicals', 'entity_count', 'matched_vocabs', 'match_kinds']\n",
      "DF_PLAYER_TO_CHUNKS columns: ['player_entity_id', 'canonical', 'chunk_ids', 'chunk_count', 'file_relpaths', 'file_count']\n",
      "DF_SOURCE_FILES columns: ['source_id', 'relpath', 'source_type']\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Load index artifacts (v0)\n",
    "LAST_PHASE_RUN = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "\n",
    "# Normalize INDEX_VERSION into the on-disk suffix (your files use lowercase v0)\n",
    "# Accepts \"V0\", \"v0\", \"0\" (if you ever use that), but publishes \"v0\"\n",
    "INDEX_VERSION_SUFFIX = f\"v{str(INDEX_VERSION).lower().lstrip('v')}\"\n",
    "\n",
    "# Required artifact filenames (fixed contract for this notebook)\n",
    "required = {\n",
    "    \"entity_to_chunks\": f\"index_entity_to_chunks_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"chunk_to_entities\": f\"index_chunk_to_entities_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"player_to_chunks\": f\"index_player_to_chunks_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"source_files\": f\"index_source_files_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "}\n",
    "\n",
    "# Resolve paths and validate existence\n",
    "INDEX_FILES = {}\n",
    "for key, fname in required.items():\n",
    "    p = (INDEXES_PATH / fname).resolve()\n",
    "    INDEX_FILES[key] = p\n",
    "    if not p.exists():\n",
    "        errors.append(f\"Missing required index artifact: {fname}\\n  Expected at: {p}\")\n",
    "\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase 2 cannot proceed because required index artifacts are missing.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Rerun IWTC_Raw_Source_Indexing.ipynb to generate the v0 artifacts\\n\"\n",
    "          \"- Ensure the resulting index_*.csv files are placed under your indexes.path directory\\n\"\n",
    "          f\"- indexes.path resolved to:\\n  {INDEXES_PATH}\\n\"\n",
    "          \"- Then rerun Phase 2\"\n",
    "    )\n",
    "\n",
    "# Load CSVs (raw)\n",
    "DF_ENTITY_TO_CHUNKS = pd.read_csv(INDEX_FILES[\"entity_to_chunks\"])\n",
    "DF_CHUNK_TO_ENTITIES = pd.read_csv(INDEX_FILES[\"chunk_to_entities\"])\n",
    "DF_PLAYER_TO_CHUNKS = pd.read_csv(INDEX_FILES[\"player_to_chunks\"])\n",
    "DF_SOURCE_FILES = pd.read_csv(INDEX_FILES[\"source_files\"])\n",
    "\n",
    "# Validate required columns (presence only)\n",
    "expected_cols = {\n",
    "    \"DF_ENTITY_TO_CHUNKS\": {\"entity_id\", \"canonical\", \"chunk_ids\", \"chunk_count\", \"file_relpaths\", \"file_count\"},\n",
    "    \"DF_CHUNK_TO_ENTITIES\": {\n",
    "        \"chunk_id\", \"source_id\", \"source_type\", \"relpath\",\n",
    "        \"chunk_start_line\", \"chunk_end_line\",\n",
    "        \"entity_ids\", \"canonicals\", \"entity_count\",\n",
    "        \"matched_vocabs\", \"match_kinds\",\n",
    "    },\n",
    "    \"DF_PLAYER_TO_CHUNKS\": {\"player_entity_id\", \"canonical\", \"chunk_ids\", \"chunk_count\", \"file_relpaths\", \"file_count\"},\n",
    "    \"DF_SOURCE_FILES\": {\"source_id\", \"relpath\", \"source_type\"},\n",
    "}\n",
    "\n",
    "for df_name, cols in expected_cols.items():\n",
    "    df = globals()[df_name]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"{df_name}: missing expected columns: {missing}\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"One or more index artifacts were loaded but do not match expected v0 columns.\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Confirm you are using the v0 CSVs produced by IWTC_Raw_Source_Indexing.ipynb\\n\"\n",
    "          \"- Do not edit the CSVs manually\\n\"\n",
    "          \"- If you changed the producer notebook, re-run it to regenerate indexes and retry\"\n",
    "    )\n",
    "\n",
    "# Summary prints\n",
    "print(\"Phase 2 OK: index artifacts loaded.\")\n",
    "print(f\"indexes.path: {INDEXES_PATH}\")\n",
    "print(f\"index version: {INDEX_VERSION_SUFFIX}\")\n",
    "\n",
    "print(\"\\nLoaded tables:\")\n",
    "print(f\"- DF_ENTITY_TO_CHUNKS:   {len(DF_ENTITY_TO_CHUNKS):>8} rows, {len(DF_ENTITY_TO_CHUNKS.columns):>3} cols\")\n",
    "print(f\"- DF_CHUNK_TO_ENTITIES:  {len(DF_CHUNK_TO_ENTITIES):>8} rows, {len(DF_CHUNK_TO_ENTITIES.columns):>3} cols\")\n",
    "print(f\"- DF_PLAYER_TO_CHUNKS:   {len(DF_PLAYER_TO_CHUNKS):>8} rows, {len(DF_PLAYER_TO_CHUNKS.columns):>3} cols\")\n",
    "print(f\"- DF_SOURCE_FILES:       {len(DF_SOURCE_FILES):>8} rows, {len(DF_SOURCE_FILES.columns):>3} cols\")\n",
    "\n",
    "# Optional: quick column display (helps debugging early)\n",
    "print(\"\\nDF_ENTITY_TO_CHUNKS columns:\", list(DF_ENTITY_TO_CHUNKS.columns))\n",
    "print(\"DF_CHUNK_TO_ENTITIES columns:\", list(DF_CHUNK_TO_ENTITIES.columns))\n",
    "print(\"DF_PLAYER_TO_CHUNKS columns:\", list(DF_PLAYER_TO_CHUNKS.columns))\n",
    "print(\"DF_SOURCE_FILES columns:\", list(DF_SOURCE_FILES.columns))\n",
    "\n",
    "# cleanup locals\n",
    "del pd, Path, errors, required, key, fname, p, cols, df_name, df, missing\n",
    "del expected_cols, INDEX_VERSION_SUFFIX, INDEX_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1dabab-fe10-4900-bc88-1161173fba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up INDEXES path variables that have been loaded into dataframes\n",
    "del INDEXES_PATH, INDEXES_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41329c-603e-4a02-be37-22d39598c08e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase 3: Load vocabulary tables\n",
    "\n",
    "This phase loads optional vocabulary tables that enable human-readable\n",
    "resolution and display during querying.\n",
    "\n",
    "The notebook:\n",
    "\n",
    "- Loads `vocab_entities.csv`\n",
    "- Loads `vocab_aliases.csv`\n",
    "- Loads `vocab_author_aliases.csv`\n",
    "- Loads `vocab_map_player_character.csv`\n",
    "- Validates minimal required columns (presence only)\n",
    "- Publishes vocabulary dataframes for use in resolution helpers\n",
    "\n",
    "This phase does not modify index tables and does not merge data.\n",
    "It only prepares lookup tables for name resolution and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf325ee-17d6-471c-9339-966f173dbbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3 OK: vocabulary tables loaded (human-authored schemas supported).\n",
      "indexes.path: /Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/indexes\n",
      "\n",
      "Loaded vocab tables:\n",
      "- DF_VOCAB_ENTITIES:      176 rows,   2 cols\n",
      "- DF_VOCAB_ALIASES:        87 rows,   2 cols\n",
      "- DF_VOCAB_AUTHORS:         6 rows,   3 cols\n",
      "- DF_VOCAB_PC_MAP:         42 rows,   2 cols\n",
      "\n",
      "DF_VOCAB_ENTITIES columns: ['entity_id', 'canonical']\n",
      "DF_VOCAB_ALIASES columns: ['entity_id', 'alias']\n",
      "DF_VOCAB_AUTHORS columns: ['author', 'player_entity_id', 'ambig_char_id']\n",
      "DF_VOCAB_PC_MAP columns: ['player_entity_id', 'char_entity_id']\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Load vocabulary tables (human-authored CSVs; entities required)\n",
    "LAST_PHASE_RUN = \"3\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Semantic column mappings for vocab CSVs\n",
    "# Each semantic field -> acceptable column names in the CSV\n",
    "# Any other CSV columns are ignored on purpose.\n",
    "# ------------------------------------------------------------------\n",
    "ENTITY_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"canonical\": [\"canonical\", \"canonical_name\", \"name\"],\n",
    "}\n",
    "ALIAS_COLS = {\n",
    "    \"entity_id\": [\"entity_id\", \"id\"],\n",
    "    \"alias\": [\"alias\", \"alt\", \"alternate\"],\n",
    "}\n",
    "AUTHOR_ALIAS_COLS = {\n",
    "    \"author\": [\"author\", \"discord_name\", \"handle\"],\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"ambig_char_id\": [\"ambig_char_id\", \"ambiguous_character\", \"ambig_character\"],\n",
    "}\n",
    "PC_MAP_COLS = {\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"char_entity_id\": [\"char_entity_id\", \"character_entity_id\", \"character\"],\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Expected vocab artifact filenames\n",
    "# - entities required\n",
    "# - others optional\n",
    "# ------------------------------------------------------------------\n",
    "vocab_files = [\n",
    "    (\"entities\", \"vocab_entities.csv\", ENTITY_COLS, True),\n",
    "    (\"aliases\", \"vocab_aliases.csv\", ALIAS_COLS, False),\n",
    "    (\"author_aliases\", \"vocab_author_aliases.csv\", AUTHOR_ALIAS_COLS, False),\n",
    "    (\"pc_map\", \"vocab_map_player_character.csv\", PC_MAP_COLS, False),\n",
    "]\n",
    "\n",
    "# Outputs (published)\n",
    "DF_VOCAB_ENTITIES = pd.DataFrame(columns=list(ENTITY_COLS.keys()))\n",
    "DF_VOCAB_ALIASES = pd.DataFrame(columns=list(ALIAS_COLS.keys()))\n",
    "DF_VOCAB_AUTHORS = pd.DataFrame(columns=list(AUTHOR_ALIAS_COLS.keys()))\n",
    "DF_VOCAB_PC_MAP = pd.DataFrame(columns=list(PC_MAP_COLS.keys()))\n",
    "\n",
    "VOCAB_FILES = {}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load + normalize each vocab CSV (in place, no helper function)\n",
    "# ------------------------------------------------------------------\n",
    "for key, fname, col_map, required in vocab_files:\n",
    "    p = (INDEXES_PATH / fname).resolve()\n",
    "    VOCAB_FILES[key] = p\n",
    "\n",
    "    if required and not p.exists():\n",
    "        errors.append(f\"Missing required vocabulary file: {fname}\\n  Expected at: {p}\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"Optional vocab file not found: {fname}\")\n",
    "        continue\n",
    "\n",
    "    raw_df = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "\n",
    "    # Build rename map: first matching option wins\n",
    "    rename = {}\n",
    "    for semantic, options in col_map.items():\n",
    "        found = next((c for c in options if c in raw_df.columns), None)\n",
    "        if found:\n",
    "            rename[found] = semantic\n",
    "\n",
    "    # If there are rows but we couldn't resolve any semantic columns, warn loudly\n",
    "    if len(raw_df) > 0 and not rename:\n",
    "        header_line = \", \".join(list(raw_df.columns))\n",
    "        expected = {k: v for k, v in col_map.items()}\n",
    "        warnings.append(\n",
    "            f\"WARNING [{key}]: CSV has rows but none of the expected columns were found.\\n\"\n",
    "            f\"  CSV columns: {header_line}\\n\"\n",
    "            f\"  Expected (semantic -> acceptable names): {expected}\\n\"\n",
    "            f\"  File: {p}\"\n",
    "        )\n",
    "        norm_df = pd.DataFrame(columns=list(col_map.keys()))\n",
    "    else:\n",
    "        out = raw_df.rename(columns=rename)\n",
    "\n",
    "        missing_semantic = [k for k in col_map.keys() if k not in out.columns]\n",
    "        if len(raw_df) > 0 and missing_semantic:\n",
    "            warnings.append(\n",
    "                f\"WARNING [{key}]: missing semantic columns after normalization: {missing_semantic}\\n\"\n",
    "                f\"  CSV columns: {list(raw_df.columns)}\\n\"\n",
    "                f\"  Using: {list(out.columns)}\\n\"\n",
    "                f\"  File: {p}\"\n",
    "            )\n",
    "\n",
    "        keep = [k for k in col_map.keys() if k in out.columns]\n",
    "        norm_df = out[keep].copy()\n",
    "\n",
    "    # Publish to the appropriate DF_*\n",
    "    if key == \"entities\":\n",
    "        DF_VOCAB_ENTITIES = norm_df\n",
    "    elif key == \"aliases\":\n",
    "        DF_VOCAB_ALIASES = norm_df\n",
    "    elif key == \"author_aliases\":\n",
    "        DF_VOCAB_AUTHORS = norm_df\n",
    "    elif key == \"pc_map\":\n",
    "        DF_VOCAB_PC_MAP = norm_df\n",
    "\n",
    "    # cleanup per-iteration locals\n",
    "    del raw_df, rename, semantic, options, found, col_map, required, fname, p, norm_df\n",
    "\n",
    "# Required entities must be usable\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase 3 cannot proceed because required vocabulary files are missing.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Create or place the required vocab files under indexes.path\\n\"\n",
    "          \"- If you need a starter schema, use the config_examples and existing IWTC files as reference\\n\"\n",
    "          f\"- indexes.path resolved to:\\n  {INDEXES_PATH}\\n\"\n",
    "          \"- Then rerun Phase 3\"\n",
    "    )\n",
    "\n",
    "if DF_VOCAB_ENTITIES.empty:\n",
    "    raise ValueError(\n",
    "        \"Entities vocab file was loaded but did not produce any usable rows after normalization.\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Confirm the entities CSV includes columns for entity_id and canonical name\\n\"\n",
    "        \"- Acceptable column names:\\n\"\n",
    "        f\"  entity_id: {ENTITY_COLS['entity_id']}\\n\"\n",
    "        f\"  canonical: {ENTITY_COLS['canonical']}\\n\"\n",
    "        \"- Fix the file and rerun Phase 3\"\n",
    "    )\n",
    "\n",
    "# Summary prints (match Phase 2 style)\n",
    "print(\"Phase 3 OK: vocabulary tables loaded (human-authored schemas supported).\")\n",
    "print(f\"indexes.path: {INDEXES_PATH}\")\n",
    "\n",
    "print(\"\\nLoaded vocab tables:\")\n",
    "print(f\"- DF_VOCAB_ENTITIES: {len(DF_VOCAB_ENTITIES):>8} rows, {len(DF_VOCAB_ENTITIES.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_ALIASES:  {len(DF_VOCAB_ALIASES):>8} rows, {len(DF_VOCAB_ALIASES.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_AUTHORS:  {len(DF_VOCAB_AUTHORS):>8} rows, {len(DF_VOCAB_AUTHORS.columns):>3} cols\")\n",
    "print(f\"- DF_VOCAB_PC_MAP:   {len(DF_VOCAB_PC_MAP):>8} rows, {len(DF_VOCAB_PC_MAP.columns):>3} cols\")\n",
    "\n",
    "print(\"\\nDF_VOCAB_ENTITIES columns:\", list(DF_VOCAB_ENTITIES.columns))\n",
    "print(\"DF_VOCAB_ALIASES columns:\", list(DF_VOCAB_ALIASES.columns))\n",
    "print(\"DF_VOCAB_AUTHORS columns:\", list(DF_VOCAB_AUTHORS.columns))\n",
    "print(\"DF_VOCAB_PC_MAP columns:\", list(DF_VOCAB_PC_MAP.columns))\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup locals (keep published DFs and VOCAB_FILES)\n",
    "del pd, Path\n",
    "del errors, warnings, vocab_files, key, keep, missing_semantic, out\n",
    "del ENTITY_COLS, ALIAS_COLS, AUTHOR_ALIAS_COLS, PC_MAP_COLS, VOCAB_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ff87e7-38c1-4d2c-8ed6-3eb7327b8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up VOCAB path variables\n",
    "del VOCAB_ENTITIES_PATH, VOCAB_ENTITIES_RELPATH\n",
    "del VOCAB_ALIASES_PATH, VOCAB_ALIASES_RELPATH\n",
    "del VOCAB_AUTHORS_PATH, VOCAB_AUTHORS_RELPATH\n",
    "del VOCAB_PC_MAP_PATH, VOCAB_PC_MAP_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b7934-287f-414d-a9ab-b6314596956b",
   "metadata": {},
   "source": [
    "## Phase 4: Query building blocks\n",
    "\n",
    "This phase defines the notebook's basic query tools (\"building blocks\") that later phases combine into DM-friendly questions.\n",
    "\n",
    "Key ideas:\n",
    "- A **chunk** is a small excerpt of a file, identified by `chunk_id` plus the file `relpath` and line range.\n",
    "- An **entity** is a tracked name (character, place, faction, etc.) identified by `entity_id` and shown by `canonical`.\n",
    "\n",
    "These tools let you type either a name (canonical or alias) or an ID, and then:\n",
    "- find which chunks mention an entity or a player\n",
    "- list which entities appear inside a given chunk\n",
    "- pull the index rows needed to navigate back to the source files\n",
    "\n",
    "In Phase 5/6 we will use these building blocks to answer practical questions like:\n",
    "\"Where does X appear?\" and \"What entities appear in file Y?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05792c5-2e16-40d4-a39e-ad0e912d10d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 4 OK: query building blocks defined.\n",
      "Building blocks:\n",
      "- find_entity_ids(name_or_id, include_aliases=True)\n",
      "- find_player_ids(author_or_player_id)\n",
      "- find_character_ids_for_player(player_entity_id)\n",
      "- find_chunk_ids_for_entity(entity_name_or_id, include_aliases=True)\n",
      "- find_chunk_ids_for_player(player_name_or_id)\n",
      "- get_chunk_row(chunk_id)\n",
      "- list_entity_ids_in_chunk(chunk_id)\n",
      "- list_entity_names_in_chunk(chunk_id)\n",
      "- get_chunk_rows(chunk_ids)\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Query building blocks (DM-facing names, v0)\n",
    "LAST_PHASE_RUN = \"4\"\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Internal: parse list-encoded fields from CSVs\n",
    "# -------------------------------------------------------------------\n",
    "def _parse_list_field(raw):\n",
    "    \"\"\"\n",
    "    Parse a list-like field (stored as text in the CSVs) into list[str].\n",
    "\n",
    "    Supported forms:\n",
    "    - \"\" / None / NaN -> []\n",
    "    - JSON: [\"a\",\"b\"]\n",
    "    - Python repr: ['a','b']\n",
    "    - Delimited: \"a|b|c\" or \"a;b;c\" or \"a,b,c\"\n",
    "    - Single token: \"a\" -> [\"a\"]\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return []\n",
    "    if isinstance(raw, float) and pd.isna(raw):\n",
    "        return []\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        # JSON\n",
    "        try:\n",
    "            v = json.loads(s)\n",
    "            if isinstance(v, list):\n",
    "                return [str(x).strip() for x in v if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Python repr\n",
    "        try:\n",
    "            v = ast.literal_eval(s)\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                return [str(x).strip() for x in v if str(x).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for delim in [\"|\", \";\", \",\"]:\n",
    "        if delim in s:\n",
    "            out = [x.strip() for x in s.split(delim)]\n",
    "            return [x for x in out if x]\n",
    "\n",
    "    return [s]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Name resolution (entity + player)\n",
    "# -------------------------------------------------------------------\n",
    "def find_entity_ids(name_or_id, include_aliases=True):\n",
    "    \"\"\"\n",
    "    Resolve a user input (name, alias, or ID) to matching entity_id(s).\n",
    "\n",
    "    You can type:\n",
    "    - an entity_id (exact)\n",
    "    - a canonical name (case-insensitive)\n",
    "    - an alias (case-insensitive) if aliases exist and include_aliases=True\n",
    "\n",
    "    Returns:\n",
    "    - list[str] of entity_id (possibly empty, possibly multiple)\n",
    "    \"\"\"\n",
    "    if name_or_id is None:\n",
    "        return []\n",
    "\n",
    "    q = str(name_or_id).strip()\n",
    "    if not q:\n",
    "        return []\n",
    "\n",
    "    # Direct ID match (fast path)\n",
    "    if \"entity_id\" in DF_VOCAB_ENTITIES.columns:\n",
    "        if q in set(DF_VOCAB_ENTITIES[\"entity_id\"]):\n",
    "            return [q]\n",
    "\n",
    "    q_lower = q.lower()\n",
    "\n",
    "    # Canonical match\n",
    "    canon_hits = DF_VOCAB_ENTITIES[\n",
    "        DF_VOCAB_ENTITIES[\"canonical\"].astype(str).str.lower() == q_lower\n",
    "    ][\"entity_id\"].tolist()\n",
    "\n",
    "    hits = list(dict.fromkeys([x for x in canon_hits if str(x).strip()]))\n",
    "\n",
    "    # Alias match (optional)\n",
    "    if include_aliases and DF_VOCAB_ALIASES is not None and not DF_VOCAB_ALIASES.empty:\n",
    "        alias_hits = DF_VOCAB_ALIASES[\n",
    "            DF_VOCAB_ALIASES[\"alias\"].astype(str).str.lower() == q_lower\n",
    "        ][\"entity_id\"].tolist()\n",
    "\n",
    "        for eid in alias_hits:\n",
    "            if eid and eid not in hits:\n",
    "                hits.append(eid)\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "def find_player_ids(author_or_player_id):\n",
    "    \"\"\"\n",
    "    Resolve a user input to player_entity_id(s).\n",
    "\n",
    "    You can type:\n",
    "    - a player_entity_id (exact)\n",
    "    - an author handle (Discord name) if author aliases exist\n",
    "\n",
    "    Returns:\n",
    "    - list[str] of player_entity_id\n",
    "    \"\"\"\n",
    "    if author_or_player_id is None:\n",
    "        return []\n",
    "\n",
    "    q = str(author_or_player_id).strip()\n",
    "    if not q:\n",
    "        return []\n",
    "\n",
    "    # Direct match against player index\n",
    "    if \"player_entity_id\" in DF_PLAYER_TO_CHUNKS.columns:\n",
    "        if q in set(DF_PLAYER_TO_CHUNKS[\"player_entity_id\"]):\n",
    "            return [q]\n",
    "\n",
    "    # Author handle lookup (optional)\n",
    "    if DF_VOCAB_AUTHORS is None or DF_VOCAB_AUTHORS.empty:\n",
    "        return []\n",
    "\n",
    "    q_lower = q.lower()\n",
    "    hits = DF_VOCAB_AUTHORS[\n",
    "        DF_VOCAB_AUTHORS[\"author\"].astype(str).str.lower() == q_lower\n",
    "    ][\"player_entity_id\"].tolist()\n",
    "\n",
    "    hits = [x.strip() for x in hits if str(x).strip()]\n",
    "    hits = list(dict.fromkeys(hits))\n",
    "    return hits\n",
    "\n",
    "\n",
    "def find_character_ids_for_player(player_entity_id):\n",
    "    \"\"\"\n",
    "    Return character entity_id(s) mapped to a player_entity_id (if a PC map exists).\n",
    "\n",
    "    Returns:\n",
    "    - list[str] (possibly empty)\n",
    "    \"\"\"\n",
    "    if not player_entity_id:\n",
    "        return []\n",
    "\n",
    "    if DF_VOCAB_PC_MAP is None or DF_VOCAB_PC_MAP.empty:\n",
    "        return []\n",
    "\n",
    "    q = str(player_entity_id).strip()\n",
    "    if not q:\n",
    "        return []\n",
    "\n",
    "    hits = DF_VOCAB_PC_MAP[\n",
    "        DF_VOCAB_PC_MAP[\"player_entity_id\"].astype(str) == q\n",
    "    ][\"char_entity_id\"].tolist()\n",
    "\n",
    "    hits = [x.strip() for x in hits if str(x).strip()]\n",
    "    hits = list(dict.fromkeys(hits))\n",
    "    return hits\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Chunk lookup\n",
    "# -------------------------------------------------------------------\n",
    "def find_chunk_ids_for_entity(entity_name_or_id, include_aliases=True):\n",
    "    \"\"\"\n",
    "    Return a set of chunk_id where an entity appears.\n",
    "\n",
    "    Input can be a canonical name, alias (if enabled), or entity_id.\n",
    "    \"\"\"\n",
    "    eids = find_entity_ids(entity_name_or_id, include_aliases=include_aliases)\n",
    "    if not eids:\n",
    "        return set()\n",
    "\n",
    "    rows = DF_ENTITY_TO_CHUNKS[DF_ENTITY_TO_CHUNKS[\"entity_id\"].isin(eids)]\n",
    "    out = set()\n",
    "\n",
    "    for raw in rows[\"chunk_ids\"].tolist():\n",
    "        for cid in _parse_list_field(raw):\n",
    "            out.add(cid)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def find_chunk_ids_for_player(player_name_or_id):\n",
    "    \"\"\"\n",
    "    Return a set of chunk_id associated with a player.\n",
    "\n",
    "    Input can be a player_entity_id or an author handle (if author aliases exist).\n",
    "    \"\"\"\n",
    "    pids = find_player_ids(player_name_or_id)\n",
    "    if not pids:\n",
    "        return set()\n",
    "\n",
    "    rows = DF_PLAYER_TO_CHUNKS[DF_PLAYER_TO_CHUNKS[\"player_entity_id\"].isin(pids)]\n",
    "    out = set()\n",
    "\n",
    "    for raw in rows[\"chunk_ids\"].tolist():\n",
    "        for cid in _parse_list_field(raw):\n",
    "            out.add(cid)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Chunk inspection\n",
    "# -------------------------------------------------------------------\n",
    "def get_chunk_row(chunk_id):\n",
    "    \"\"\"\n",
    "    Return the DF_CHUNK_TO_ENTITIES row for a chunk_id (0 or 1 rows).\n",
    "    \"\"\"\n",
    "    q = str(chunk_id).strip() if chunk_id is not None else \"\"\n",
    "    if not q:\n",
    "        return DF_CHUNK_TO_ENTITIES.iloc[0:0].copy()\n",
    "\n",
    "    return DF_CHUNK_TO_ENTITIES[DF_CHUNK_TO_ENTITIES[\"chunk_id\"] == q].copy()\n",
    "\n",
    "\n",
    "def list_entity_ids_in_chunk(chunk_id):\n",
    "    \"\"\"\n",
    "    Return entity_id list in a chunk (order as stored in the index).\n",
    "    \"\"\"\n",
    "    df = get_chunk_row(chunk_id)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    return _parse_list_field(df.iloc[0][\"entity_ids\"])\n",
    "\n",
    "\n",
    "def list_entity_names_in_chunk(chunk_id):\n",
    "    \"\"\"\n",
    "    Return canonical-name list in a chunk (order as stored in the index).\n",
    "    \"\"\"\n",
    "    df = get_chunk_row(chunk_id)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    return _parse_list_field(df.iloc[0][\"canonicals\"])\n",
    "\n",
    "\n",
    "def get_chunk_rows(chunk_ids):\n",
    "    \"\"\"\n",
    "    Return DF_CHUNK_TO_ENTITIES rows for a set/list of chunk_ids.\n",
    "    \"\"\"\n",
    "    ids = list(chunk_ids) if chunk_ids is not None else []\n",
    "    ids = [str(x).strip() for x in ids if str(x).strip()]\n",
    "    if not ids:\n",
    "        return DF_CHUNK_TO_ENTITIES.iloc[0:0].copy()\n",
    "\n",
    "    return DF_CHUNK_TO_ENTITIES[DF_CHUNK_TO_ENTITIES[\"chunk_id\"].isin(ids)].copy()\n",
    "\n",
    "\n",
    "print(\"Phase 4 OK: query building blocks defined.\")\n",
    "print(\"Building blocks:\")\n",
    "print(\"- find_entity_ids(name_or_id, include_aliases=True)\")\n",
    "print(\"- find_player_ids(author_or_player_id)\")\n",
    "print(\"- find_character_ids_for_player(player_entity_id)\")\n",
    "print(\"- find_chunk_ids_for_entity(entity_name_or_id, include_aliases=True)\")\n",
    "print(\"- find_chunk_ids_for_player(player_name_or_id)\")\n",
    "print(\"- get_chunk_row(chunk_id)\")\n",
    "print(\"- list_entity_ids_in_chunk(chunk_id)\")\n",
    "print(\"- list_entity_names_in_chunk(chunk_id)\")\n",
    "print(\"- get_chunk_rows(chunk_ids)\")\n",
    "\n",
    "# cleanup module imports (functions remain usable)\n",
    "del ast, json, pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f2492-57ec-4603-9e94-10ac6d8c6070",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "\n",
    "## Phase 5: Implement query primitives\n",
    "\n",
    "This phase implements the primitives defined above.\n",
    "\n",
    "Key responsibilities:\n",
    "\n",
    "- Parse list-encoded fields (`chunk_ids`, `entity_ids`)\n",
    "- Convert serialized lists into Python sets\n",
    "- Ensure stable return types\n",
    "- Avoid modifying any canonical data\n",
    "\n",
    "All transformations occur in-memory only.\n",
    "\n",
    "\n",
    "## Phase 6: Implement composed query patterns\n",
    "\n",
    "This phase builds higher-level query patterns using primitives.\n",
    "\n",
    "Supported patterns:\n",
    "\n",
    "- Where does entity X appear?\n",
    "- What entities appear in file Y?\n",
    "- What chunks are associated with player Z?\n",
    "- What entities co-occur with entity X?\n",
    "\n",
    "Each pattern:\n",
    "\n",
    "- Returns a dataframe formatted for inspection\n",
    "- Uses only existing index tables\n",
    "- Performs no normalization or schema rewriting\n",
    "\n",
    "\n",
    "## Phase 7: Output formatting conventions\n",
    "\n",
    "This phase standardizes how results are presented.\n",
    "\n",
    "Defines:\n",
    "\n",
    "- Chunk reference row format:\n",
    "  - chunk_id\n",
    "  - relpath\n",
    "  - chunk_start_line\n",
    "  - chunk_end_line\n",
    "  - source_type\n",
    "  - entity_count\n",
    "\n",
    "- Entity summary row format:\n",
    "  - entity_id\n",
    "  - canonical\n",
    "  - n_chunks\n",
    "  - n_files\n",
    "\n",
    "- Sorting rules (e.g., relpath + line order)\n",
    "\n",
    "No data is modified in this phase.\n",
    "This phase concerns display consistency only.\n",
    "\n",
    "\n",
    "## Phase 8: Interactive scratchpad\n",
    "\n",
    "This phase provides example queries to validate notebook behavior.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Query a known entity\n",
    "- Query a known player\n",
    "- Inspect a known file\n",
    "- Test entity co-occurrence\n",
    "\n",
    "This section serves as a reusable verification block for future runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbef3c-8c64-42a7-8ce7-51b1664a5982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iwtc-tools)",
   "language": "python",
   "name": "iwtc-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
