{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c793bd4d-7e1c-4a3e-a589-5efc504a10a7",
   "metadata": {},
   "source": [
    "# IWTC Graph Querying\n",
    "\n",
    "This notebook answers DM questions by treating graph edges as grammatical statements and chaining them into evidence trails.\n",
    "\n",
    "It loads graph CSV artifacts (nodes + edges) and provides query recipes that traverse relationships rather than composing table joins.\n",
    "\n",
    "Read-only: no writes, no promotion, no index regeneration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426cfdd0-e44c-4e61-aa84-a3f5efdcb166",
   "metadata": {},
   "source": [
    "# Pre-Build: Load descriptor and canonical artifacts\n",
    "\n",
    "Run these phases to point the notebook at a world repository, validate paths, and load canonical graph artifacts.\n",
    "\n",
    "Once this section succeeds, you can collapse it. The query recipes below assume these DataFrames exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188b4ce-ec4e-426a-bcb5-c3182d7aae3d",
   "metadata": {},
   "source": [
    "## Phase P0: Parameters\n",
    "\n",
    "Set the world repository descriptor and the index version you want to query.\n",
    "\n",
    "This phase only selects *which* world and *which* artifact version is in scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d42d5e-e4e6-44af-aade-55f882d0648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run initialized at: 2026-02-17 19:28\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Phase P0: Parameters\n",
    "# -------------------------------------------------------------------\n",
    "LAST_PHASE_RUN = \"P0\"\n",
    "\n",
    "# Absolute path to the world_repository.yml descriptor.\n",
    "WORLD_REPOSITORY_DESCRIPTOR = (\n",
    "    \"/Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/descriptors/world_repository.yml\"\n",
    ")\n",
    "\n",
    "# Artifact version to load (must match previously generated artifacts).\n",
    "# In v0, graph artifacts are versioned using the same INDEX_VERSION as the index artifacts they were built from.\n",
    "INDEX_VERSION = \"V0\"\n",
    "\n",
    "# Internal run metadata (do not edit)\n",
    "from datetime import datetime\n",
    "print(f\"Notebook run initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "del datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6461d5-3396-48bb-bc9c-23a3a50ef202",
   "metadata": {},
   "source": [
    "## Phase P1: Load and validate world descriptor\n",
    "\n",
    "Loads the repository descriptor and validates the filesystem layout it declares.\n",
    "\n",
    "This phase answers: \"Can I trust these paths enough to read artifacts safely?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c933dc7-913d-458a-abac-639b89d0161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World repository descriptor loaded successfully: world_repository.yml\n",
      "Descriptor paths are usable for this notebook.\n",
      "world_root: /Users/charissophia/obsidian/Iron Wolf Trading Company\n",
      "indexes: _meta/indexes\n",
      "vocab.entities: _meta/indexes/vocab_entities.csv (exists=True)\n",
      "vocab.aliases: _meta/indexes/vocab_aliases.csv (exists=True)\n",
      "vocab.author_aliases: _meta/indexes/vocab_author_aliases.csv (exists=True)\n"
     ]
    }
   ],
   "source": [
    "# Phase P1: Load and validate world repository descriptor (Graph Querying v0)\n",
    "LAST_PHASE_RUN = \"P1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# --- Load descriptor file ---\n",
    "descriptor_path = Path(WORLD_REPOSITORY_DESCRIPTOR)\n",
    "\n",
    "if not descriptor_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"World repository descriptor file was not found.\\n\"\n",
    "        f\"Path provided:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Confirm the file exists at this location or fix WORLD_REPOSITORY_DESCRIPTOR in Phase 0\\n\"\n",
    "        \"- If you just edited Phase 0, rerun Phase 0 and then rerun this cell\\n\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    with descriptor_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        world_repo = yaml.safe_load(f)\n",
    "except Exception:\n",
    "    raise ValueError(\n",
    "        \"The world repository descriptor could not be read.\\n\"\n",
    "        \"This usually indicates a YAML formatting problem.\\n\\n\"\n",
    "        f\"File:\\n  {descriptor_path}\\n\\n\"\n",
    "        \"What to do:\\n\"\n",
    "        \"- Compare the file against the example world_repository.yml\\n\"\n",
    "        \"- Paste the contents into https://www.yamllint.com/\\n\"\n",
    "        \"- Fix any reported issues, save the file, and rerun this cell\"\n",
    "    )\n",
    "\n",
    "if not isinstance(world_repo, dict):\n",
    "    raise ValueError(\n",
    "        \"World repository descriptor structure is not usable.\\n\"\n",
    "        \"The file must be a YAML mapping (top-level `name: value` entries).\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"World repository descriptor loaded successfully: {descriptor_path.name}\")\n",
    "\n",
    "# --- Extract required entries ---\n",
    "WORLD_ROOT_RAW = world_repo.get(\"world_root\")\n",
    "\n",
    "indexes_block = world_repo.get(\"indexes\")\n",
    "INDEXES_RAW = indexes_block.get(\"path\") if isinstance(indexes_block, dict) else None\n",
    "\n",
    "vocab = world_repo.get(\"vocabulary\") or {}\n",
    "ENTITIES_RAW = vocab.get(\"entities\")\n",
    "ALIASES_RAW = vocab.get(\"aliases\")\n",
    "AUTHORS_RAW = vocab.get(\"author_aliases\")\n",
    "\n",
    "if not WORLD_ROOT_RAW:\n",
    "    errors.append(\"Missing required entry: world_root\")\n",
    "if not INDEXES_RAW:\n",
    "    errors.append(\"Missing required entry: indexes.path\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"World repository descriptor is missing required entries:\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Edit your world_repository.yml and add/fix the missing entries\\n\"\n",
    "          \"- Save the file and rerun this cell\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Published outputs (initialize up front for later phases)\n",
    "# ------------------------------------------------------------------\n",
    "WORLD_ROOT = None\n",
    "\n",
    "INDEXES_PATH = None\n",
    "INDEXES_RELPATH = None\n",
    "\n",
    "VOCAB_ENTITIES_PATH = None\n",
    "VOCAB_ENTITIES_RELPATH = None\n",
    "VOCAB_ALIASES_PATH = None\n",
    "VOCAB_ALIASES_RELPATH = None\n",
    "VOCAB_AUTHORS_PATH = None\n",
    "VOCAB_AUTHORS_RELPATH = None\n",
    "\n",
    "# --- Validate and resolve world_root ---\n",
    "WORLD_ROOT = Path(WORLD_ROOT_RAW)\n",
    "\n",
    "if str(WORLD_ROOT).startswith(\"~\"):\n",
    "    errors.append(\"world_root: '~' is not allowed. Use a full absolute path.\")\n",
    "elif not WORLD_ROOT.is_absolute():\n",
    "    errors.append(\"world_root must be an absolute path (starts with / on macOS/Linux, or C:\\\\ on Windows).\")\n",
    "elif not WORLD_ROOT.is_dir():\n",
    "    errors.append(f\"world_root must be an existing directory: {WORLD_ROOT}\")\n",
    "else:\n",
    "    WORLD_ROOT = WORLD_ROOT.resolve()\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "def _resolve_under_world_root(raw_path: str, label: str):\n",
    "    if raw_path is None or str(raw_path).strip() == \"\":\n",
    "        return None, None\n",
    "\n",
    "    p = Path(str(raw_path))\n",
    "\n",
    "    if str(p).startswith(\"~\"):\n",
    "        errors.append(f\"{label}: '~' is not allowed: {raw_path}\")\n",
    "        return None, None\n",
    "\n",
    "    if not p.is_absolute():\n",
    "        p = WORLD_ROOT / p\n",
    "    p = p.resolve()\n",
    "\n",
    "    try:\n",
    "        rel = str(p.relative_to(WORLD_ROOT))\n",
    "    except Exception:\n",
    "        rel = str(p)\n",
    "\n",
    "    return p, rel\n",
    "\n",
    "# --- Resolve and validate indexes path (required, directory) ---\n",
    "INDEXES_PATH, INDEXES_RELPATH = _resolve_under_world_root(INDEXES_RAW, \"indexes.path\")\n",
    "\n",
    "if INDEXES_PATH is None:\n",
    "    errors.append(\"indexes.path: missing or invalid.\")\n",
    "else:\n",
    "    if not INDEXES_PATH.exists():\n",
    "        errors.append(f\"indexes.path: path does not exist: {INDEXES_PATH}\")\n",
    "    elif not INDEXES_PATH.is_dir():\n",
    "        errors.append(f\"indexes.path: must be a directory: {INDEXES_PATH}\")\n",
    "\n",
    "# --- Resolve vocabulary paths (optional; warn if missing) ---\n",
    "vocab_entries = [\n",
    "    (\"entities\", \"vocab.entities\", ENTITIES_RAW),\n",
    "    (\"aliases\", \"vocab.aliases\", ALIASES_RAW),\n",
    "    (\"author_aliases\", \"vocab.author_aliases\", AUTHORS_RAW),\n",
    "]\n",
    "\n",
    "for key, label, raw in vocab_entries:\n",
    "    if not raw:\n",
    "        continue\n",
    "\n",
    "    p, rel = _resolve_under_world_root(raw, label)\n",
    "    if p is None:\n",
    "        continue\n",
    "\n",
    "    if p.exists() and p.is_dir():\n",
    "        warnings.append(f\"{label}: {p} must be a file (got directory). Ignoring.\")\n",
    "        continue\n",
    "\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"{label}: file does not exist: {p} (name resolution may be limited).\")\n",
    "\n",
    "    if key == \"entities\":\n",
    "        VOCAB_ENTITIES_PATH, VOCAB_ENTITIES_RELPATH = p, rel\n",
    "    elif key == \"aliases\":\n",
    "        VOCAB_ALIASES_PATH, VOCAB_ALIASES_RELPATH = p, rel\n",
    "    elif key == \"author_aliases\":\n",
    "        VOCAB_AUTHORS_PATH, VOCAB_AUTHORS_RELPATH = p, rel\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\"Descriptor path validation failed:\\n- \" + \"\\n- \".join(errors))\n",
    "\n",
    "print(\"Descriptor paths are usable for this notebook.\")\n",
    "print(f\"world_root: {WORLD_ROOT}\")\n",
    "print(f\"indexes: {INDEXES_RELPATH}\")\n",
    "print(f\"vocab.entities: {VOCAB_ENTITIES_RELPATH} (exists={VOCAB_ENTITIES_PATH.exists() if VOCAB_ENTITIES_PATH else False})\")\n",
    "print(f\"vocab.aliases: {VOCAB_ALIASES_RELPATH} (exists={VOCAB_ALIASES_PATH.exists() if VOCAB_ALIASES_PATH else False})\")\n",
    "print(f\"vocab.author_aliases: {VOCAB_AUTHORS_RELPATH} (exists={VOCAB_AUTHORS_PATH.exists() if VOCAB_AUTHORS_PATH else False})\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del yaml, Path\n",
    "del descriptor_path, world_repo, indexes_block, vocab\n",
    "del WORLD_REPOSITORY_DESCRIPTOR\n",
    "del WORLD_ROOT_RAW, INDEXES_RAW, ENTITIES_RAW, ALIASES_RAW, AUTHORS_RAW\n",
    "del vocab_entries, key, label, raw, p, rel, warnings, errors, f\n",
    "del _resolve_under_world_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42b022-f547-4377-82fb-7c172eae0133",
   "metadata": {},
   "source": [
    "## Phase P2: Load graph artifacts\n",
    "\n",
    "Loads the graph CSVs for the selected index version:\n",
    "\n",
    "- graph_nodes_vN.csv\n",
    "- graph_edges_vN.csv\n",
    "\n",
    "This phase answers: \"Are the graph artifacts present and structurally usable?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3e8e255-5b82-4c7f-8b35-8e0ebaca16cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase P2 OK: graph artifacts loaded.\n",
      "indexes.path: /Users/charissophia/obsidian/Iron Wolf Trading Company/_meta/indexes\n",
      "graph version: v0\n",
      "\n",
      "Loaded tables:\n",
      "- DF_GRAPH_NODES:      1696 rows,   3 cols\n",
      "- DF_GRAPH_EDGES:     12691 rows,   4 cols\n",
      "\n",
      "DF_GRAPH_NODES columns: ['node_id', 'node_type', 'label']\n",
      "DF_GRAPH_EDGES columns: ['subject', 'predicate', 'object', 'weight']\n"
     ]
    }
   ],
   "source": [
    "# Phase P2: Load graph artifacts (v0)\n",
    "LAST_PHASE_RUN = \"P2\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "errors = []\n",
    "\n",
    "# Normalize INDEX_VERSION into the on-disk suffix (your files use lowercase v0)\n",
    "# Accepts \"V0\", \"v0\", \"0\" but publishes \"v0\"\n",
    "INDEX_VERSION_SUFFIX = f\"v{str(INDEX_VERSION).lower().lstrip('v')}\"\n",
    "\n",
    "# Required artifact filenames (fixed contract for this notebook)\n",
    "required = {\n",
    "    \"graph_nodes\": f\"graph_nodes_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "    \"graph_edges\": f\"graph_edges_{INDEX_VERSION_SUFFIX}.csv\",\n",
    "}\n",
    "\n",
    "# Resolve paths and validate existence\n",
    "GRAPH_FILES = {}\n",
    "for key, fname in required.items():\n",
    "    p = (INDEXES_PATH / fname).resolve()\n",
    "    GRAPH_FILES[key] = p\n",
    "    if not p.exists():\n",
    "        errors.append(f\"Missing required graph artifact: {fname}\\n  Expected at: {p}\")\n",
    "\n",
    "if errors:\n",
    "    raise FileNotFoundError(\n",
    "        \"Phase P2 cannot proceed because required graph artifacts are missing.\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Run IWTC_Graph_Indexing.ipynb to generate graph_nodes_*.csv and graph_edges_*.csv\\n\"\n",
    "          \"- Ensure the resulting graph_*.csv files are placed under your indexes.path directory\\n\"\n",
    "          f\"- indexes.path resolved to:\\n  {INDEXES_PATH}\\n\"\n",
    "          \"- Then rerun Phase P2\"\n",
    "    )\n",
    "\n",
    "# Load CSVs (raw)\n",
    "DF_GRAPH_NODES = pd.read_csv(GRAPH_FILES[\"graph_nodes\"])\n",
    "DF_GRAPH_EDGES = pd.read_csv(GRAPH_FILES[\"graph_edges\"])\n",
    "\n",
    "# Validate required columns (presence only)\n",
    "expected_cols = {\n",
    "    \"DF_GRAPH_NODES\": {\"node_id\", \"node_type\", \"label\"},\n",
    "    # weight is optional; if missing, we'll add it as an empty column.\n",
    "    \"DF_GRAPH_EDGES\": {\"subject\", \"predicate\", \"object\"},\n",
    "}\n",
    "\n",
    "for df_name, cols in expected_cols.items():\n",
    "    df = globals()[df_name]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"{df_name}: missing expected columns: {missing}\")\n",
    "\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"One or more graph artifacts were loaded but do not match expected v0 columns.\\n- \"\n",
    "        + \"\\n- \".join(errors)\n",
    "        + \"\\n\\nWhat to do:\\n\"\n",
    "          \"- Confirm you are using the v0 CSVs produced by IWTC_Graph_Indexing.ipynb\\n\"\n",
    "          \"- Do not edit the CSVs manually\\n\"\n",
    "          \"- If you changed the producer notebook, re-run it to regenerate graphs and retry\"\n",
    "    )\n",
    "\n",
    "# Summary prints\n",
    "print(\"Phase P2 OK: graph artifacts loaded.\")\n",
    "print(f\"indexes.path: {INDEXES_PATH}\")\n",
    "print(f\"graph version: {INDEX_VERSION_SUFFIX}\")\n",
    "\n",
    "print(\"\\nLoaded tables:\")\n",
    "print(f\"- DF_GRAPH_NODES:  {len(DF_GRAPH_NODES):>8} rows, {len(DF_GRAPH_NODES.columns):>3} cols\")\n",
    "print(f\"- DF_GRAPH_EDGES:  {len(DF_GRAPH_EDGES):>8} rows, {len(DF_GRAPH_EDGES.columns):>3} cols\")\n",
    "\n",
    "print(\"\\nDF_GRAPH_NODES columns:\", list(DF_GRAPH_NODES.columns))\n",
    "print(\"DF_GRAPH_EDGES columns:\", list(DF_GRAPH_EDGES.columns))\n",
    "\n",
    "# cleanup locals (keep DF_GRAPH_NODES, DF_GRAPH_EDGES, INDEXES_PATH)\n",
    "del pd, Path, errors, required, key, fname, p, cols, df_name, df, missing\n",
    "del expected_cols, INDEX_VERSION_SUFFIX, GRAPH_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c016e5c-5929-43c0-90a1-c188d8d9b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up INDEXES path variables that have been loaded into dataframes\n",
    "del INDEXES_PATH, INDEXES_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198dba64-e9e5-4239-9b90-13b68d5965f1",
   "metadata": {},
   "source": [
    "## Phase P3: Optional supporting tables (labels + context)\n",
    "\n",
    "Optionally loads canonical index and vocabulary tables to improve readability and expand context in displays.\n",
    "\n",
    "Graph traversal uses nodes + edges as the primary structure. Supporting tables are for presentation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "426bc954-acce-43c1-921e-37bf8f709612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase P3 OK: optional vocabulary tables loaded (if present).\n",
      "- DF_VOCAB_ENTITIES:    176 rows\n",
      "- DF_VOCAB_ALIASES:      87 rows\n",
      "- DF_VOCAB_AUTHORS:       6 rows\n",
      "- DF_VOCAB_LOOKUP:      269 rows\n"
     ]
    }
   ],
   "source": [
    "# Phase P3: Load optional vocabulary tables (for lookup / entrypoints)\n",
    "LAST_PHASE_RUN = \"P3\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "warnings = []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Semantic column mappings (same as Index Query)\n",
    "# ------------------------------------------------------------------\n",
    "ENTITY_COLS = {\"entity_id\": [\"entity_id\", \"id\"], \"canonical\": [\"canonical\", \"canonical_name\", \"name\"]}\n",
    "ALIAS_COLS  = {\"entity_id\": [\"entity_id\", \"id\"], \"alias\": [\"alias\", \"alt\", \"alternate\"]}\n",
    "AUTHOR_ALIAS_COLS = {\n",
    "    \"author\": [\"author\", \"discord_name\", \"handle\"],\n",
    "    \"player_entity_id\": [\"player_entity_id\", \"player\", \"player_id\"],\n",
    "    \"ambig_char_id\": [\"ambig_char_id\", \"ambiguous_character\", \"ambig_character\"],\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Use descriptor-validated vocab paths (from Phase P1)\n",
    "# ------------------------------------------------------------------\n",
    "vocab_files = [\n",
    "    (\"entities\", VOCAB_ENTITIES_PATH, ENTITY_COLS),\n",
    "    (\"aliases\", VOCAB_ALIASES_PATH, ALIAS_COLS),\n",
    "    (\"author_aliases\", VOCAB_AUTHORS_PATH, AUTHOR_ALIAS_COLS),\n",
    "]\n",
    "\n",
    "# Published outputs (always defined)\n",
    "DF_VOCAB_ENTITIES = pd.DataFrame(columns=list(ENTITY_COLS.keys()))\n",
    "DF_VOCAB_ALIASES  = pd.DataFrame(columns=list(ALIAS_COLS.keys()))\n",
    "DF_VOCAB_AUTHORS  = pd.DataFrame(columns=list(AUTHOR_ALIAS_COLS.keys()))\n",
    "DF_VOCAB_LOOKUP   = pd.DataFrame(columns=[\"vocab_id\", \"vocab\", \"vocab_kind\", \"vocab_norm\"])\n",
    "DF_VOCAB_TO_NODE  = pd.DataFrame(columns=[\"vocab_id\", \"node_id\"])  # convenience, same value in v0\n",
    "\n",
    "def _load_and_normalize_csv(path_obj, col_map, key_label):\n",
    "    if not path_obj:\n",
    "        return pd.DataFrame(columns=list(col_map.keys()))\n",
    "\n",
    "    p = Path(path_obj)\n",
    "    if not p.exists():\n",
    "        warnings.append(f\"Optional vocab file not found: {p}\")\n",
    "        return pd.DataFrame(columns=list(col_map.keys()))\n",
    "\n",
    "    raw_df = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "\n",
    "    rename = {}\n",
    "    for semantic, options in col_map.items():\n",
    "        found = next((c for c in options if c in raw_df.columns), None)\n",
    "        if found:\n",
    "            rename[found] = semantic\n",
    "\n",
    "    if len(raw_df) > 0 and not rename:\n",
    "        warnings.append(\n",
    "            f\"[{key_label}] CSV has rows but none of the expected columns were found.\\n\"\n",
    "            f\"  CSV columns: {list(raw_df.columns)}\\n\"\n",
    "            f\"  Expected mapping: {col_map}\\n\"\n",
    "            f\"  File: {p}\"\n",
    "        )\n",
    "        return pd.DataFrame(columns=list(col_map.keys()))\n",
    "\n",
    "    out = raw_df.rename(columns=rename)\n",
    "    keep = [k for k in col_map.keys() if k in out.columns]\n",
    "    return out[keep]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load tables (optional)\n",
    "# ------------------------------------------------------------------\n",
    "for key, path_obj, col_map in vocab_files:\n",
    "    norm_df = _load_and_normalize_csv(path_obj, col_map, key)\n",
    "\n",
    "    if key == \"entities\":\n",
    "        DF_VOCAB_ENTITIES = norm_df\n",
    "    elif key == \"aliases\":\n",
    "        DF_VOCAB_ALIASES = norm_df\n",
    "    elif key == \"author_aliases\":\n",
    "        DF_VOCAB_AUTHORS = norm_df\n",
    "\n",
    "del vocab_files, key, path_obj, col_map, norm_df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build DF_VOCAB_LOOKUP (unified lookup for free-text -> node id)\n",
    "# NOTE: In v0, vocab_id is already the node_id for entities/players.\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "for _, r in DF_VOCAB_ENTITIES.iterrows():\n",
    "    vid = str(r.get(\"entity_id\", \"\")).strip()\n",
    "    v = str(r.get(\"canonical\", \"\")).strip()\n",
    "    if vid and v:\n",
    "        rows.append([vid, v, \"entity\"])\n",
    "\n",
    "if not DF_VOCAB_ALIASES.empty:\n",
    "    for _, r in DF_VOCAB_ALIASES.iterrows():\n",
    "        vid = str(r.get(\"entity_id\", \"\")).strip()\n",
    "        v = str(r.get(\"alias\", \"\")).strip()\n",
    "        if vid and v:\n",
    "            rows.append([vid, v, \"alias\"])\n",
    "\n",
    "if not DF_VOCAB_AUTHORS.empty:\n",
    "    for _, r in DF_VOCAB_AUTHORS.iterrows():\n",
    "        vid = str(r.get(\"player_entity_id\", \"\")).strip()\n",
    "        v = str(r.get(\"author\", \"\")).strip()\n",
    "        if vid and v:\n",
    "            rows.append([vid, v, \"author\"])\n",
    "\n",
    "DF_VOCAB_LOOKUP = pd.DataFrame(rows, columns=[\"vocab_id\", \"vocab\", \"vocab_kind\"])\n",
    "DF_VOCAB_LOOKUP[\"vocab_norm\"] = DF_VOCAB_LOOKUP[\"vocab\"].astype(str).str.strip().str.lower()\n",
    "DF_VOCAB_LOOKUP = (\n",
    "    DF_VOCAB_LOOKUP\n",
    "    .drop_duplicates(subset=[\"vocab_id\", \"vocab_norm\", \"vocab_kind\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Convenience mapping (explicit name helps later recipes)\n",
    "DF_VOCAB_TO_NODE = DF_VOCAB_LOOKUP.loc[:, [\"vocab_id\"]].drop_duplicates().rename(columns={\"vocab_id\": \"node_id\"})\n",
    "DF_VOCAB_TO_NODE[\"vocab_id\"] = DF_VOCAB_TO_NODE[\"node_id\"]\n",
    "\n",
    "del rows, r, vid, v, _\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Summary\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Phase P3 OK: optional vocabulary tables loaded (if present).\")\n",
    "print(f\"- DF_VOCAB_ENTITIES: {len(DF_VOCAB_ENTITIES):>6} rows\")\n",
    "print(f\"- DF_VOCAB_ALIASES:  {len(DF_VOCAB_ALIASES):>6} rows\")\n",
    "print(f\"- DF_VOCAB_AUTHORS:  {len(DF_VOCAB_AUTHORS):>6} rows\")\n",
    "print(f\"- DF_VOCAB_LOOKUP:   {len(DF_VOCAB_LOOKUP):>6} rows\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"- {w}\")\n",
    "\n",
    "# cleanup\n",
    "del pd, Path, warnings\n",
    "del ENTITY_COLS, ALIAS_COLS, AUTHOR_ALIAS_COLS\n",
    "del _load_and_normalize_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1c7df8-d639-4b40-9d44-df6bd54b5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: clean up VOCAB path variables\n",
    "del VOCAB_ENTITIES_PATH, VOCAB_ENTITIES_RELPATH\n",
    "del VOCAB_ALIASES_PATH, VOCAB_ALIASES_RELPATH\n",
    "del VOCAB_AUTHORS_PATH, VOCAB_AUTHORS_RELPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1fe07-05c9-459b-bae5-75d9154df870",
   "metadata": {},
   "source": [
    "# Graph Engine: Build the in-memory graph\n",
    "\n",
    "This section converts the loaded CSV artifacts into an in-memory graph object for traversal.\n",
    "\n",
    "- Input artifacts: `DF_GRAPH_NODES`, `DF_GRAPH_EDGES`\n",
    "- Output object: `WORLD_GRAPH` (a NetworkX graph with node and edge attributes)\n",
    "\n",
    "Everything below this point should query the graph, not the raw tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "985b848c-e18d-48c2-a0ab-60216ecc9c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized WORLD_GRAPH: MultiDiGraph\n",
      "Nodes loaded into WORLD_GRAPH\n",
      "- WORLD_GRAPH number_of_nodes(): 1,696\n",
      "Edges loaded into WORLD_GRAPH\n",
      "- WORLD_GRAPH number_of_edges(): 12,691\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Phase G: Build graph object (bridge between CSV and queries)\n",
    "# -------------------------------------------------------------------\n",
    "LAST_PHASE_RUN = \"G\"\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# WORLD_GRAPH is the in-memory graph object we will query.\n",
    "# MultiDiGraph allows multiple edges between the same nodes (needed for ambiguity later).\n",
    "WORLD_GRAPH = nx.MultiDiGraph()\n",
    "\n",
    "print(\"Initialized WORLD_GRAPH:\", type(WORLD_GRAPH).__name__)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# G.1: Add nodes to WORLD_GRAPH\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (nodes table):\n",
    "#   node_id + node_type + label  => \"This thing exists in the world graph\"\n",
    "#\n",
    "# Target grammar (graph object):\n",
    "#   WORLD_GRAPH.add_node(node_id, node_type=..., label=...)\n",
    "# -------------------------------------------------------------------\n",
    "for r in DF_GRAPH_NODES.itertuples(index=False):\n",
    "    WORLD_GRAPH.add_node(\n",
    "        str(r.node_id),\n",
    "        node_type=str(r.node_type),\n",
    "        label=str(r.label),\n",
    "    )\n",
    "\n",
    "print(\"Nodes loaded into WORLD_GRAPH\")\n",
    "print(f\"- WORLD_GRAPH number_of_nodes(): {WORLD_GRAPH.number_of_nodes():,}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# G.2: Add edges to WORLD_GRAPH\n",
    "# -------------------------------------------------------------------\n",
    "# Source grammar (edges table):\n",
    "#   subject + predicate + object (+ optional weight)\n",
    "#     => \"This relationship statement exists\"\n",
    "#\n",
    "# Target grammar (graph object):\n",
    "#   WORLD_GRAPH.add_edge(subject, object, predicate=..., weight=...)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for r in DF_GRAPH_EDGES.itertuples(index=False):\n",
    "    WORLD_GRAPH.add_edge(\n",
    "        str(r.subject),\n",
    "        str(r.object),\n",
    "        predicate=str(r.predicate),\n",
    "        weight=r.weight,   # whatever pandas loaded (NaN for blanks is fine for now)\n",
    "    )\n",
    "\n",
    "print(\"Edges loaded into WORLD_GRAPH\")\n",
    "print(f\"- WORLD_GRAPH number_of_edges(): {WORLD_GRAPH.number_of_edges():,}\")\n",
    "\n",
    "# clean up locals\n",
    "del r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50160940-630d-4a40-987d-ccaf5df9305b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47aa3b30-1547-4ba0-ac1f-1278f1e9ef97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Query recipes (edit and run)\n",
    "\n",
    "Each recipe answers one DM question by chaining statements into an evidence trail.\n",
    "\n",
    "You will usually only edit the parameter lines (ENTITY, PLAYER, FILE, etc.) and rerun the cell.\n",
    "Recipes are designed to be modified during use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d97937-7d5d-44b0-9ef4-6b0203f73969",
   "metadata": {},
   "source": [
    "## Q1: Where does entity X appear?\n",
    "\n",
    "Goal: produce evidence locations (files + chunks) for a named entity.\n",
    "\n",
    "Chain used (conceptually):\n",
    "file contains chunk; chunk mentions vocab; vocab refers_to entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50352a-6e53-4917-b8f1-c99e4d6f0ae1",
   "metadata": {},
   "source": [
    "## Q2: What else appears with entity X?\n",
    "\n",
    "Goal: find entities structurally associated with X via cooccurrence.\n",
    "\n",
    "This is a weighted relationship:\n",
    "entity cooccurs_with entity (weight = shared chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737228dc-e778-4945-aba1-3e6faf0b196d",
   "metadata": {},
   "source": [
    "## Q3: Where do X and Y overlap?\n",
    "\n",
    "Goal: show evidence chunks where both X and Y appear.\n",
    "\n",
    "This recipe is \"proof oriented\": it outputs overlap chunks and a show-chunk command for context review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905503d-bffb-420a-96e7-5047cde80f66",
   "metadata": {},
   "source": [
    "## Q4: How does X likely connect to Y?\n",
    "\n",
    "Goal: support “social proximity” style questions using multi-hop structure.\n",
    "\n",
    "This recipe explicitly calls out:\n",
    "- when multiple hops are required\n",
    "- when a predicate is read backwards (incoming edges)\n",
    "- when weights are used as strength signals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007aca13-1194-4b48-bcf0-1fc261bf12e7",
   "metadata": {},
   "source": [
    "## Q5: What does player Z usually write about?\n",
    "\n",
    "Goal: map from player -> character(s) -> evidence chunks and summarize recurring entities.\n",
    "\n",
    "This recipe teaches two things:\n",
    "- using declared mappings (plays)\n",
    "- chaining from authorship-related nodes into evidence trails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dfebf-5296-49ed-9330-14decb20bc6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notes and next steps\n",
    "\n",
    "This notebook is an analysis layer over existing graph artifacts.\n",
    "\n",
    "If results look wrong:\n",
    "- adjust vocabulary / indexing inputs\n",
    "- regenerate canonical indexes and graph artifacts\n",
    "- re-run this notebook\n",
    "\n",
    "No files are written here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iwtc-tools)",
   "language": "python",
   "name": "iwtc-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
