---
title: IWTC-Tools Progress Report - February 10, 2026
author: Charis Sophia
project: Iron Wolf Trading Company / IWTC-Tools
version: 0.5
repository: https://github.com/CharisSophia/iwtc-tools
---

# IWTC-Tools Progress Report – February 10, 2026

## Project Summary

This milestone captures the completion of **Phase 7: Vocabulary-based linking**, extending the pipeline from candidate discovery into deterministic, reproducible linkage between curated vocabulary and normalized source content.

Where earlier phases focused on *discovering* what might matter, Phase 7 establishes the first **authoritative indexing layer**: applying human-curated vocabulary to chunked narrative text in a controlled, non-inferential way.

The pipeline now spans:

**raw sources → normalized files → chunks → vocabulary → entity mentions**

This marks the transition from exploratory tooling into a stable indexing foundation suitable for attribution, aggregation, and downstream graph construction.

---

## 1. Relpath Discipline and Path Refactor

A significant refactor standardized how paths are handled throughout the notebook:

- All source files, chunks, and outputs now carry a stable `relpath` relative to `WORLD_ROOT`
- Absolute paths are retained only as internal references
- Relpath computation was moved upstream into descriptor validation and source expansion
- Downstream phases no longer recompute or infer relative paths

**Key learning:** Relpath handling is architectural, not cosmetic. Centralizing it simplified later phases and eliminated subtle inconsistencies.

---

## 2. PbP Header Author Extraction

The pipeline now explicitly recognizes **play-by-post (PbP) header authorship** as structured data:

- Phase 6 extracts PbP header authors from `pbp_hash` chunks
- Author presence is counted and tracked per chunk and per file
- Header authorship is treated as metadata, not narrative content

This enables later attribution without prematurely inferring character voice or intent.

**Key learning:** Headers are one of the few places where attribution is explicit and reliable; handling them separately was the correct design choice.

---

## 3. Vocabulary Registry Normalization

Vocabulary handling was substantially strengthened:

- CSV column normalization allows flexible, human-maintained schemas
- Each vocabulary file declares acceptable column aliases
- Extra columns are ignored safely
- CSVs that load but do not map any rows now emit explicit warnings

Supported vocabularies now include:

- Canonical entities
- Entity aliases
- Author-to-player mappings
- Player-to-character mappings (loaded but not yet applied)

**Key learning:** Human-edited vocabularies will drift. Tooling must be resilient rather than brittle.

---

## 4. Vocabulary Preparation

- Loads all authoritative vocabularies into dataframes
- Normalizes columns using declared schemas
- Produces a unified vocabulary table (`VOCAB_DF`)
- Compiles boundary-aware, longest-first regex patterns
- Emits clear diagnostics showing what loaded and from where

**Key learning:** Separating preparation from application made debugging and reasoning dramatically easier.

---

## 5. Entity Linking

- Matches canonical names and approved aliases only
- Uses boundary-aware, longest-first matching with overlap masking
- Preserves chunk, source, and positional context
- Produces a stable `ENTITY_MENTIONS_V0` table (5,200+ rows)

This phase answers a single, well-scoped question:

**“Where do known entities appear in the corpus?”**

**Key learning:** Resisting interpretive logic at this stage keeps the data reusable and trustworthy.

---

## 6. Attribution Scope Clarification

During design review, attribution scope was intentionally constrained:

- Current attribution is limited to **PbP authorship**
- The pipeline records *who authored a chunk*, not *which character spoke*
- Player–character mappings are loaded but not yet applied
- Zoom transcript attribution is deferred pending further investigation

This avoids conflating narrative voice with character identity before indexing behavior is fully understood.

**Key learning:** Attribution is layered; treating it as a single problem too early leads to overfitting.

---

## 7. Current Baseline State

At this checkpoint:

- `CHUNKS_V0` is stable and reusable
- Vocabulary discovery and curation are complete for v0
- Entity mentions are linked deterministically
- Authorship metadata is available for PbP content
- Notebook state is clean, restart-safe, and phase-tracked
- Outputs are reproducible, timestamped, and path-stable

Entity linking is now **functionally complete** for its intended scope.

---

## 8. Next Steps

Planned follow-on work includes:

1. **Aggregation Layers**
   - Per-entity summaries
   - Coverage and distribution metrics

2. **Graph Construction**
   - Co-occurrence edges
   - Temporal proximity
   - Source-based weighting

3. **Auto-transcript Re-evaluation**
   - Assess whether normalization can make them usable
   - Keep them isolated until quality improves

---

**Current status:**  
The project has moved from *discovery* to *indexing*.  
The tooling now produces authoritative references suitable for querying, aggregation, and graph-based exploration.