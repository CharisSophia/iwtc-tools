---
title: IWTC-Tools Progress Report - January 16, 2026
author: Charis Sophia
project: Iron Wolf Trading Company / IWTC-Tools
version: 0.4
repository: https://github.com/CharisSophia/iwtc-tools
---

# IWTC-Tools Progress Report - January 16, 2026

## [Project Summary]
This milestone captures the completion of the **v0 chunking, vocabulary discovery, and entity-linking pipeline**. The work builds directly on the raw source indexing baseline established on January 8, extending the workflow from normalized source material through structured entity references suitable for downstream indexing and graph construction.

This phase introduces the project’s first **interpretive pass** over narrative content, while maintaining strict boundaries between human-curated authority and machine-assisted discovery.

---

## 1. Unified v0 Chunking Pipeline
- Implemented a single, generalized chunking system across:
  - `pbp_transcripts`
  - `session_notes`
  - `planning_notes`
  - `auto_transcripts` (explicitly isolated for now)
- Preserved stable `chunk_id`, `source_id`, and line boundaries.
- Normalized chunk structure to a common schema (`CHUNKS_V0`).

**Key Learning:**  
Identifying header lines more generally produced good enough chunking and proved surprisingly useful when proposing vocabulary.

---

## 2. Exploratory Data Profiling
- Added exploratory statistics and charts to analyze:
  - Chunk sizes
  - Line density
  - Source-type variance
- Used profiling results to validate chunking heuristics rather than overfitting them.

**Key Learning:**  
Docx files lose formatting information on ingestion. Favoring markdown as a source format over docx.

---

## 3. Candidate Vocabulary Bootstrap
- Introduced a controlled vocabulary discovery pass over `CHUNKS_V0`.
- Extracted candidate proper nouns using conservative heuristics:
  - Title-case phrases
  - Acronyms
  - Multi-word spans with connector awareness
- Produced ranked candidate lists with supporting evidence snippets.

**Key Learning:**  
We had a surprising number of relationships CharA and CharB appear in the multiword candidates. Noted for later reference.

---

## 4. Descriptor & Path Validation Refactor
- Refactored descriptor validation logic to:
  - Generalize path resolution
  - Enforce relative-path discipline
  - Reduce notebook state leakage
- Cleaned up earlier ad-hoc variables in favor of a clear notebook-level contract.

**Key Learning:**  
Having more paths to validate helped break thinking out of prior structures and simplified overall.

---

## 5. Vocabulary Registry Integration
- Extended the world repository descriptor to reference:
  - Canonical entity registry
  - Alias mappings
  - Player-character associations
- Ensured tooling consumes vocabularies as **authoritative inputs**, not inferred artifacts.

**Key Learning:**  
Taxonomy was surprisingly tricky. Aliases helped think through what changes and what doesn't.

---

## 6. Entity Linking to Chunks
- Linked canonical entities and approved aliases to chunk occurrences.
- Produced a structured `ENTITY_MENTIONS_V0` table with:
  - Chunk context
  - Source metadata
  - Evidence snippets
- Explicitly excluded `auto_transcripts` from linking due to quality concerns.

**Key Learning:**  
Discovered one taxonomy decision hadn't made it into the aliases file.

---

## 7. Current Baseline State
- `CHUNKS_V0` is stable and reusable
- Candidate vocabulary discovery is operational
- Canonical entities and aliases are linked deterministically
- Outputs are reproducible and timestamped
- Notebook state is clean and restart-safe

This baseline is now considered **ready for aggregation and higher-order indexing**.

---

## 8. Next Steps

1. **Entity Aggregation**
   - Collapse mentions into per-entity summaries
   - Track frequency, distribution, and source coverage

2. **Alias Expansion Support**
   - Use unresolved candidates to suggest new aliases
   - Maintain human-in-the-loop curation

3. **Relationship & Graph Construction**
   - Layer co-occurrence and temporal proximity
   - Prepare data for graph queries and visualization

4. **Revisit auto_transcripts**
   - Explore normalization or filtering strategies
   - Decide whether to reintroduce them into linking

---

**Current Status:**  
The pipeline now spans raw sources → chunks → vocabulary → entity mentions.  
The project is positioned to move from *indexing* into *structural analysis*.

---
